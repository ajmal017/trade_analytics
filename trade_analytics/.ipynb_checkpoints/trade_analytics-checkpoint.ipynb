{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/cbook.py:136: MatplotlibDeprecationWarning: The finance module has been deprecated in mpl 2.0 and will be removed in mpl 2.2. Please use the module mpl_finance instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n",
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1401: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n",
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter, WeekdayLocator,\\\n",
    "    DayLocator, MONDAY,date2num,num2date,AutoDateLocator\n",
    "from matplotlib.finance import quotes_historical_yahoo_ohlc, candlestick_ohlc,candlestick2_ochl,volume_overlay3\n",
    "\n",
    "from stockapp import models as stkmd\n",
    "from dataapp import models as dtamd\n",
    "from dataapp import tasks as dtatks\n",
    "from dataapp import libs as dtalibs\n",
    "from featureapp import libs as ftlibs\n",
    "from featureapp import models as ftmd\n",
    "from stockapp import tasks as stktks\n",
    "from stockapp import libs as stklibs\n",
    "import featureapp.models as ftmd\n",
    "import featureapp.tasks as fttks\n",
    "import queryapp.models as qrymd\n",
    "import queryapp.tasks as qrytks\n",
    "\n",
    "import charts.chartservers.libs as chservlibs\n",
    "import charts.libs as chlibs\n",
    "\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "\n",
    "\n",
    "import featureapp as ftapp\n",
    "import utility as uty\n",
    "from utility import models as utymd\n",
    "import itertools as itt\n",
    "import multiprocessing as mp\n",
    "from django.db import connection,connections\n",
    "from django.db import reset_queries\n",
    "import time \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import inspect\n",
    "import imp\n",
    "import datetime\n",
    "from talib.abstract import *\n",
    "import utility.models as utmd\n",
    "import stockapp.libs as stklib\n",
    "from utility import codemanager as cdmng\n",
    "from utility import maintenance as mnt\n",
    "import os \n",
    "import json\n",
    "from django.contrib.auth.models import AnonymousUser\n",
    "import threading\n",
    "\n",
    "stk=stkmd.Stockmeta.objects.get(Symbol='TSLA')\n",
    "Fromdate=pd.datetime(2008,1,1)\n",
    "Todate=pd.datetime.today()\n",
    "Trange=pd.date_range(Fromdate,Todate)\n",
    "Trange=[T.date() for T in Trange if T.weekday()<=4]\n",
    "\n",
    "import json\n",
    "# fttks.computefeatuers(stk.id,Trange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entries=[\n",
    "    {'Symbol':'TSLA','TF':pd.datetime(2012,1,1).date(),'T0':pd.datetime(2011,1,1).date()  },\n",
    "    {'Symbol':'AAPL','TF':pd.datetime(2012,1,1).date(),'T0':pd.datetime(2011,1,1).date()  }\n",
    "]\n",
    "chservlibs.request_db_charts(entries,5003)\n",
    "# img=chlibs.CurrentByFutureChart_bydb(entries[0]['T0'],entries[0]['TF'],entries[0]['Symbol'],indicatorlist=(),pricecols=(),querycols=(),featcols=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Running Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"---------------- features-------------------\"\n",
    "featurecodes=ftmd.FeatureComputeCode.objects.all()\n",
    "computecode=featurecodes[0]\n",
    "computeclass=computecode.importcomputeclass()\n",
    "CF=computeclass(stk.id,Trange)\n",
    "CF.computeall(skipdone=True)\n",
    "# CF.saveall()\n",
    "print CF.getfeaturelist()\n",
    "\n",
    "CF.df=CF.addindicators(CF.df,[\n",
    "        {'name':'SMAstd','timeperiod':20,'colname':'SMAstd20'},\n",
    "        {'name':'EMAstd','timeperiod':8,'colname':'EMAstd8'},\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "querycodes=qrymd.QueryComputeCode.objects.all()\n",
    "computecode=querycodes[0]\n",
    "computeclass=computecode.importcomputeclass()\n",
    "CQ=computeclass(stk.id,Trange)\n",
    "CQ.computeall(skipdone=True)\n",
    "CQ.saveall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CQ.OutcomeCharts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CQ.getquerylist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CQ.chartfeatures(addpricecols=(),ip=5562,\n",
    "addfeatcols=[\n",
    "    ['CCI5','CCI50'],\n",
    "    ['PastPROFIT10days','PastLOSS10days'],['FutPROFIT10days','FutLOSS10days']\n",
    "],\n",
    "addquerycols=[\n",
    "    'CCICHERRIES',\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datascience import libs as dtsclibs\n",
    "import pandas as pd\n",
    "\n",
    "@dtsclibs.register_compfunc(RequiredImports=['import pandas as pd','from dataapp import libs as dtalibs'],overwrite_if_exists=False)\n",
    "def extractdataset(data_id,Symbol):\n",
    "    \"\"\"\n",
    "    @funcName : test\n",
    "    @input x : an int\n",
    "    @output df : pd.DataFrame, some random 2 by 2 \n",
    "    @description : takes an int and then returns a dummy dataframe. This is just for testing purposes\n",
    "    @Source : \n",
    "    def extractdataset(data_id,Symbol):\n",
    "        window=60\n",
    "        window_fut=30\n",
    "        Tfs=map(lambda x: ( (x.date()-pd.Dateoffset(window)).date(),x.date(), (x.date()+pd.Dateoffset(window_fut)).date() ),\n",
    "                pd.date_range(start=pd.datetime(2010,1,1),end=pd.datetime.today(),freq='W-MON') )\n",
    "\n",
    "        N=len(Tfs)\n",
    "        dfinstants=pd.DataFrame({'T0':map(lambda x: x[0],Tfs),'TF':map(lambda x: x[1],Tfs),'Symbol':[Symbol]*N})\n",
    "        X,X_Meta=dtalibs.Getbatchdata(dfinstants)\n",
    "\n",
    "        dfinstants=pd.DataFrame({'T0':map(lambda x: x[1],Tfs),'TF':map(lambda x: x[2],Tfs),'Symbol':[Symbol]*N})\n",
    "        Y,Y_Meta=dtalibs.Getbatchdata(dfinstants)\n",
    "\n",
    "        shard=dtscmd.DataShard(Data__id=data_id)\n",
    "        shard.Info['X_Meta']=X_Meta\n",
    "        shard.Info['Y_Meta']=Y_Meta\n",
    "        shard.save()\n",
    "\n",
    "        np.savez_compressed(shard.shardpath(),X=X,Y=Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    window=60\n",
    "    window_fut=30\n",
    "    Tfs=map(lambda x: ( (x.date()-pd.Dateoffset(window)).date(),x.date(), (x.date()+pd.Dateoffset(window_fut)).date() ),\n",
    "            pd.date_range(start=pd.datetime(2010,1,1),end=pd.datetime.today(),freq='W-MON') )\n",
    "    \n",
    "    N=len(Tfs)\n",
    "    dfinstants=pd.DataFrame({'T0':map(lambda x: x[0],Tfs),'TF':map(lambda x: x[1],Tfs),'Symbol':[Symbol]*N})\n",
    "    X,X_Meta=dtalibs.Getbatchdata(dfinstants)\n",
    "    \n",
    "    dfinstants=pd.DataFrame({'T0':map(lambda x: x[1],Tfs),'TF':map(lambda x: x[2],Tfs),'Symbol':[Symbol]*N})\n",
    "    Y,Y_Meta=dtalibs.Getbatchdata(dfinstants)\n",
    "    \n",
    "    shard=dtscmd.DataShard(Data__id=data_id)\n",
    "    shard.Info['X_Meta']=X_Meta\n",
    "    shard.Info['Y_Meta']=Y_Meta\n",
    "    shard.save()\n",
    "    \n",
    "    np.savez_compressed(shard.shardpath(),X=X,Y=Y)\n",
    "\n",
    "extractdataset.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating initial Stock price dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "projid,dataid=dtsclibs.register_dataset(project_Name=\"PredictReturn\",project_Info={'description': \"Data taken on every Monday. 360 days back and 60 days forward\"},\n",
    "                                        Datatype='RawProcessed',GroupName=\"AllStocks\",tag=\"1\",\n",
    "                                        data_format='npz',Modeltype='Regression',\n",
    "                                        TransformedFromDataId=None,TransFuncId=None, use_project_ifexists=True)\n",
    "\n",
    "dtsctks.CreateStockData_2(360,60,dataid,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print dtscmd.Data.objects.filter(Project__id=7,GroupName=\"AllStocks\",tag=\"1\",Datatype='RawProcessed',\n",
    "                           Dataformat='npz',Modeltype='Regression')\n",
    "data = dtscmd.Data.objects.get(Project__id=7,GroupName=\"AllStocks\",tag=\"1\",Datatype='RawProcessed',\n",
    "                           Dataformat='npz',Modeltype='Regression')\n",
    "\n",
    "data.datapath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtscmd.DataShard.objects.filter(Data=data).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtscmd.DataShard.objects.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions on shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over writing previous function\n",
      "saving function :  CleanData\n",
      "function id =  20\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "@dtsclibs.register_compfunc(Group='Cleanup',overwrite_if_exists=True)\n",
    "def CleanData(shardId):\n",
    "    \"\"\"\n",
    "    @Description: remove shards with many Nan samples. Remove samples with many Nans\n",
    "    @Source\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import datascience.models as dtscmd\n",
    "    \n",
    "    \n",
    "    \n",
    "    shard=dtscmd.DataShard.objects.get(id=shardId)\n",
    "    X,Y,Meta=shard.getdata()\n",
    "    \n",
    "    # remove samples with lot of nans\n",
    "    Nsamples=X.shape[0]\n",
    "    Tsteps=X.shape[1]\n",
    "    Nfeat=X.shape[2]\n",
    "    volumecols=[4,11,12]\n",
    "    colsX=list( Meta['MetaX']['columns'] )\n",
    "    colsY=list( Meta['MetaY']['columns'] )\n",
    "    delsample=[]\n",
    "    for i in range(Nsamples):\n",
    "        dfX=pd.DataFrame(X[i,:,:],columns=colsX)\n",
    "        dfY=pd.DataFrame(X[i,:,:],columns=colsY)\n",
    "        if np.sum(pd.isnull(dfX['Close'].values).astype(float))/len(dfX)>=0.5:\n",
    "            delsample.append(i)\n",
    "    \n",
    "    X=np.delete(X,delsample,axis=0)\n",
    "    Y=np.delete(Y,delsample,axis=0)\n",
    "    \n",
    "    # if not more samples then delete the shard\n",
    "    if X.shape[0]/(Nsamples*1.0) <0.4:\n",
    "        print \"----------------------\"\n",
    "        print str(shardId)+\"   \"+ str(X.shape[0]/(Nsamples*1.0))\n",
    "        print \"----------------------\"\n",
    "        print \"delete shaord id = \"+str(shard.id)\n",
    "        shard.delete()\n",
    "    elif X.shape[0]!=Nsamples:\n",
    "        shard.savedata(X=X,Y=Y,Meta=Meta)\n",
    "    else:\n",
    "        pass\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import datascience.models as dtscmd\n",
    "    \n",
    "    \n",
    "    \n",
    "    shard=dtscmd.DataShard.objects.get(id=shardId)\n",
    "    X,Y,Meta=shard.getdata()\n",
    "    \n",
    "    # remove samples with lot of nans\n",
    "    Nsamples=X.shape[0]\n",
    "    Tsteps=X.shape[1]\n",
    "    Nfeat=X.shape[2]\n",
    "    volumecols=[4,11,12]\n",
    "    colsX=list( Meta['MetaX']['columns'] )\n",
    "    colsY=list( Meta['MetaY']['columns'] )\n",
    "    delsample=[]\n",
    "    for i in range(Nsamples):\n",
    "        dfX=pd.DataFrame(X[i,:,:],columns=colsX)\n",
    "        dfY=pd.DataFrame(X[i,:,:],columns=colsY)\n",
    "        if np.sum(pd.isnull(dfX['Close'].values).astype(float))/len(dfX)>=0.5:\n",
    "            delsample.append(i)\n",
    "    \n",
    "    X=np.delete(X,delsample,axis=0)\n",
    "    Y=np.delete(Y,delsample,axis=0)\n",
    "    \n",
    "    # if not more samples then delete the shard\n",
    "    if X.shape[0]/(Nsamples*1.0) <0.4:\n",
    "        print \"----------------------\"\n",
    "        print str(shardId)+\"   \"+ str(X.shape[0]/(Nsamples*1.0))\n",
    "        print \"----------------------\"\n",
    "        print \"delete shaord id = \"+str(shard.id)\n",
    "        shard.delete()\n",
    "    elif X.shape[0]!=Nsamples:\n",
    "        shard.savedata(X=X,Y=Y,Meta=Meta)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "# dtsctks.applyfunc2data(CleanData.id,6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6548"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtscmd.DataShard.objects.filter(Data__id=6).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Creating derived DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over writing previous function\n",
      "saving function :  StandardizeData_1\n",
      "function id =  21\n",
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "saving transfoermer function to this dataset\n",
      "('project id', 'data id')  :  (7, 9)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "\n",
    "@dtsclibs.register_compfunc(Group='Transformer',overwrite_if_exists=True)\n",
    "def StandardizeData_1(X,Y,Meta):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    \"\"\"\n",
    "    A transformer function has to take X,Y,Meta and return another modified X,Y,Meta\n",
    "    1. Normalize all data as \n",
    "        1. Volume to 0-1\n",
    "        2. Prices --> also 0-1\n",
    "    2. For output Y data:\n",
    "        1. Take only Close\n",
    "        2. Pull out best returns in first 5 days, 10 days, 30 days, 60 days, 90 days\n",
    "        3. Pull out worst loss in first 5 days, 10 days, 30 days, 60 days, 90 days\n",
    "    \n",
    "    @Source:\n",
    "    # next normalize the volume to 0-1\n",
    "    Nsamples=X.shape[0]\n",
    "    Tsteps=X.shape[1]\n",
    "    Nfeat=X.shape[2]\n",
    "    volumecols=['Volume','VolSMA10','VolSMA20']\n",
    "    pricecols=['Close','Open','High','Low','SMA10','SMA20','SMA50','SMA100','SMA200','EMA8','EMA20']\n",
    "    colsX=list( Meta['MetaX']['columns'] )\n",
    "    colsY=list( Meta['MetaY']['columns'] )\n",
    "    FinalXcols=pricecols+volumecols\n",
    "    FinalYcols=['FutProfit5days','FutProfit10days','FutProfit30days','FutProfit60days','FutProfit90days']+['FutLoss5days',\n",
    "                                    'FutLoss10days','FutLoss30days','FutLoss60days','FutLoss90days']\n",
    "    \n",
    "    Xn=None\n",
    "    Yn=None\n",
    "    Metan=None\n",
    "    for i in range(Nsamples):\n",
    "        dfX=pd.DataFrame(X[i,:,:],columns=colsX)\n",
    "        dfY=pd.DataFrame(Y[i,:,:],columns=colsY)\n",
    "        \n",
    "        \n",
    "        # clean up Y\n",
    "        dfY.drop('Symbol',axis=1,inplace=True)\n",
    "        \n",
    "        dfY.drop(volumecols,axis=1,inplace=True)\n",
    "        dfY.drop([cc for cc in pricecols if cc!='Close'],axis=1,inplace=True)\n",
    "        \n",
    "        Ydict={}\n",
    "        dfY['ZeroPerf']=0\n",
    "    \n",
    "#         dfY['FutProfit5days']=-100*self.df['Close'].diff(periods=-5)/self.df['Close']\n",
    "        dfY['Returns']=100*(dfY['Close']-dfY['Close'].iloc[0])/dfY['Close'].iloc[0]\n",
    "        \n",
    "        Ydict['FutProfit5days']=dfY[['Returns','ZeroPerf']].iloc[0:5].max(axis=1).round().max()\n",
    "        Ydict['FutProfit10days']=dfY[['Returns','ZeroPerf']].iloc[0:10].max(axis=1).round().max()\n",
    "        Ydict['FutProfit30days']=dfY[['Returns','ZeroPerf']].iloc[0:30].max(axis=1).round().max()\n",
    "        Ydict['FutProfit60days']=dfY[['Returns','ZeroPerf']].iloc[0:60].max(axis=1).round().max()\n",
    "        Ydict['FutProfit90days']=dfY[['Returns','ZeroPerf']].iloc[0:90].max(axis=1).round().max()\n",
    "        \n",
    "        Ydict['FutLoss5days']=dfY[['Returns','ZeroPerf']].iloc[0:5].min(axis=1).round().min()\n",
    "        Ydict['FutLoss10days']=dfY[['Returns','ZeroPerf']].iloc[0:10].min(axis=1).round().min()\n",
    "        Ydict['FutLoss30days']=dfY[['Returns','ZeroPerf']].iloc[0:30].min(axis=1).round().min()\n",
    "        Ydict['FutLoss60days']=dfY[['Returns','ZeroPerf']].iloc[0:60].min(axis=1).round().min()\n",
    "        Ydict['FutLoss90days']=dfY[['Returns','ZeroPerf']].iloc[0:90].min(axis=1).round().min()\n",
    "        \n",
    "        # clean up X\n",
    "        dfX.drop('Symbol',axis=1,inplace=True)\n",
    "        \n",
    "        mxvol=dfX['Volume'].max()\n",
    "        dfX['Volume']=dfX['Volume']/mxvol\n",
    "        dfX['VolSMA10']=dfX['VolSMA10']/mxvol\n",
    "        dfX['VolSMA20']=dfX['VolSMA20']/mxvol\n",
    "        \n",
    "        mxHigh=dfX['High'].max()\n",
    "        mnLow=dfX['Low'].min()\n",
    "        for cc in pricecols:\n",
    "            dfX[cc]=(dfX[cc]-mnLow)/mxHigh\n",
    "        \n",
    "        XX=np.expand_dims( dfX[FinalXcols].astype(float).values   ,axis=0     )\n",
    "        YY=np.expand_dims( np.array([int(Ydict[key]) for key in FinalYcols]),axis=0 )\n",
    "        if Xn is None:\n",
    "            Xn=XX\n",
    "            Yn=YY\n",
    "        else:\n",
    "            Xn=np.vstack((Xn,XX))\n",
    "            Yn=np.vstack((Yn,YY))\n",
    "    \n",
    "    Metan=Meta\n",
    "    Metan['MetaX']['columns']=FinalXcols\n",
    "    Metan['MetaY']['columns']=FinalYcols\n",
    "    \n",
    "    return Xn,Yn,Metan\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # next normalize the volume to 0-1\n",
    "    Nsamples=X.shape[0]\n",
    "    Tsteps=X.shape[1]\n",
    "    Nfeat=X.shape[2]\n",
    "    volumecols=['Volume','VolSMA10','VolSMA20']\n",
    "    pricecols=['Close','Open','High','Low','SMA10','SMA20','SMA50','SMA100','SMA200','EMA8','EMA20']\n",
    "    colsX=list( Meta['MetaX']['columns'] )\n",
    "    colsY=list( Meta['MetaY']['columns'] )\n",
    "    FinalXcols=pricecols+volumecols\n",
    "    FinalYcols=['FutProfit5days','FutProfit10days','FutProfit30days','FutProfit60days','FutProfit90days']+['FutLoss5days',\n",
    "                                    'FutLoss10days','FutLoss30days','FutLoss60days','FutLoss90days']\n",
    "    \n",
    "    Xn=None\n",
    "    Yn=None\n",
    "    Metan=None\n",
    "    for i in range(Nsamples):\n",
    "        dfX=pd.DataFrame(X[i,:,:],columns=colsX)\n",
    "        dfY=pd.DataFrame(Y[i,:,:],columns=colsY)\n",
    "        \n",
    "        \n",
    "        # clean up Y\n",
    "        dfY.drop('Symbol',axis=1,inplace=True)\n",
    "        \n",
    "        dfY.drop(volumecols,axis=1,inplace=True)\n",
    "        dfY.drop([cc for cc in pricecols if cc!='Close'],axis=1,inplace=True)\n",
    "        \n",
    "        Ydict={}\n",
    "        dfY['ZeroPerf']=0\n",
    "    \n",
    "#         dfY['FutProfit5days']=-100*self.df['Close'].diff(periods=-5)/self.df['Close']\n",
    "        dfY['Returns']=100*(dfY['Close']-dfY['Close'].iloc[0])/dfY['Close'].iloc[0]\n",
    "        \n",
    "        Ydict['FutProfit5days']=dfY[['Returns','ZeroPerf']].iloc[0:5].max(axis=1).round().max()\n",
    "        Ydict['FutProfit10days']=dfY[['Returns','ZeroPerf']].iloc[0:10].max(axis=1).round().max()\n",
    "        Ydict['FutProfit30days']=dfY[['Returns','ZeroPerf']].iloc[0:30].max(axis=1).round().max()\n",
    "        Ydict['FutProfit60days']=dfY[['Returns','ZeroPerf']].iloc[0:60].max(axis=1).round().max()\n",
    "        Ydict['FutProfit90days']=dfY[['Returns','ZeroPerf']].iloc[0:90].max(axis=1).round().max()\n",
    "        \n",
    "        Ydict['FutLoss5days']=dfY[['Returns','ZeroPerf']].iloc[0:5].min(axis=1).round().min()\n",
    "        Ydict['FutLoss10days']=dfY[['Returns','ZeroPerf']].iloc[0:10].min(axis=1).round().min()\n",
    "        Ydict['FutLoss30days']=dfY[['Returns','ZeroPerf']].iloc[0:30].min(axis=1).round().min()\n",
    "        Ydict['FutLoss60days']=dfY[['Returns','ZeroPerf']].iloc[0:60].min(axis=1).round().min()\n",
    "        Ydict['FutLoss90days']=dfY[['Returns','ZeroPerf']].iloc[0:90].min(axis=1).round().min()\n",
    "        \n",
    "        # clean up X\n",
    "        dfX.drop('Symbol',axis=1,inplace=True)\n",
    "        \n",
    "        mxvol=dfX['Volume'].max()\n",
    "        dfX['Volume']=dfX['Volume']/mxvol\n",
    "        dfX['VolSMA10']=dfX['VolSMA10']/mxvol\n",
    "        dfX['VolSMA20']=dfX['VolSMA20']/mxvol\n",
    "        \n",
    "        mxHigh=dfX['High'].max()\n",
    "        mnLow=dfX['Low'].min()\n",
    "        for cc in pricecols:\n",
    "            dfX[cc]=(dfX[cc]-mnLow)/mxHigh\n",
    "        \n",
    "        XX=np.expand_dims( dfX[FinalXcols].astype(float).values   ,axis=0     )\n",
    "        YY=np.expand_dims( np.array([int(Ydict[key]) for key in FinalYcols]),axis=0 )\n",
    "        if Xn is None:\n",
    "            Xn=XX\n",
    "            Yn=YY\n",
    "        else:\n",
    "            Xn=np.vstack((Xn,XX))\n",
    "            Yn=np.vstack((Yn,YY))\n",
    "    \n",
    "    Metan=Meta\n",
    "    Metan['MetaX']['columns']=FinalXcols\n",
    "    Metan['MetaY']['columns']=FinalYcols\n",
    "    \n",
    "    return Xn,Yn,Metan\n",
    "\n",
    "\n",
    "projectid,dataid=dtsclibs.register_dataset(tag='NormalizedDerivedFromId6',TransformedFromDataId=6,TransFuncId=StandardizeData_1.id )\n",
    "\n",
    "dtsctks.Perform_TransformData(dataid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "dataId1=7\n",
    "data1=dtscmd.Data.objects.get(id=dataId1)\n",
    "data0=dtscmd.Data.objects.get(id=data1.ParentData.id)\n",
    "print data0.id\n",
    "# shard0Ids=data0.objects.all().values_list('id',flat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtscmd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-455bf37fd8f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdtscmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataShard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData__id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dtscmd' is not defined"
     ]
    }
   ],
   "source": [
    "dtscmd.DataShard.objects.filter(Data__id=9).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "DoesNotExist",
     "evalue": "Data matching query does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDoesNotExist\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-18afbb947f12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdtscmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/django/db/models/manager.pyc\u001b[0m in \u001b[0;36mmanager_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcreate_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mmanager_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_queryset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mmanager_method\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mmanager_method\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/django/db/models/query.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m             raise self.model.DoesNotExist(\n\u001b[1;32m    379\u001b[0m                 \u001b[0;34m\"%s matching query does not exist.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m             )\n\u001b[1;32m    382\u001b[0m         raise self.model.MultipleObjectsReturned(\n",
      "\u001b[0;31mDoesNotExist\u001b[0m: Data matching query does not exist."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shard=dtscmd.DataShard.objects.get(Data__id=7)\n",
    "X,Y,Meta=shard.getdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   1, ...,  -9, -14, -14],\n",
       "       [  4,   6,   6, ..., -10, -10, -10],\n",
       "       [  1,   1,   1, ..., -14, -14, -14],\n",
       "       ..., \n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Close', u'Open', u'High', u'Low', u'Volume', u'Symbol', u'SMA10',\n",
       "       u'SMA20', u'SMA50', u'SMA100', u'SMA200', u'VolSMA10', u'VolSMA20',\n",
       "       u'EMA8', u'EMA20'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Meta['MetaX']['columns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [1, 2, 3]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "A=np.array([1,2,3])\n",
    "B=np.expand_dims(A,axis=0)\n",
    "np.vstack((B,B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<QuerySet [<Project: PredictReturn>]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtscmd.Project.objects.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
