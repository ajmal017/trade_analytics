{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/cbook.py:136: MatplotlibDeprecationWarning: The finance module has been deprecated in mpl 2.0 and will be removed in mpl 2.2. Please use the module mpl_finance instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n",
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1401: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n",
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter, WeekdayLocator,\\\n",
    "    DayLocator, MONDAY,date2num,num2date,AutoDateLocator\n",
    "from matplotlib.finance import quotes_historical_yahoo_ohlc, candlestick_ohlc,candlestick2_ochl,volume_overlay3\n",
    "\n",
    "from stockapp import models as stkmd\n",
    "from dataapp import models as dtamd\n",
    "from dataapp import tasks as dtatks\n",
    "from dataapp import libs as dtalibs\n",
    "from featureapp import libs as ftlibs\n",
    "from featureapp import models as ftmd\n",
    "from stockapp import tasks as stktks\n",
    "from stockapp import libs as stklibs\n",
    "import featureapp.models as ftmd\n",
    "import featureapp.tasks as fttks\n",
    "import queryapp.models as qrymd\n",
    "import queryapp.tasks as qrytks\n",
    "\n",
    "import charts.chartservers.libs as chservlibs\n",
    "import charts.libs as chlibs\n",
    "\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "\n",
    "\n",
    "import featureapp as ftapp\n",
    "import utility as uty\n",
    "from utility import models as utymd\n",
    "import itertools as itt\n",
    "import multiprocessing as mp\n",
    "from django.db import connection,connections\n",
    "from django.db import reset_queries\n",
    "import time \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import inspect\n",
    "import imp\n",
    "import datetime\n",
    "from talib.abstract import *\n",
    "import utility.models as utmd\n",
    "import stockapp.libs as stklib\n",
    "from utility import codemanager as cdmng\n",
    "from utility import maintenance as mnt\n",
    "import os \n",
    "import json\n",
    "from django.contrib.auth.models import AnonymousUser\n",
    "import threading\n",
    "\n",
    "stk=stkmd.Stockmeta.objects.get(Symbol='TSLA')\n",
    "Fromdate=pd.datetime(2008,1,1)\n",
    "Todate=pd.datetime.today()\n",
    "Trange=pd.date_range(Fromdate,Todate)\n",
    "Trange=[T.date() for T in Trange if T.weekday()<=4]\n",
    "\n",
    "import json\n",
    "# fttks.computefeatuers(stk.id,Trange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entries=[\n",
    "    {'Symbol':'TSLA','TF':pd.datetime(2012,1,1).date(),'T0':pd.datetime(2011,1,1).date()  },\n",
    "    {'Symbol':'AAPL','TF':pd.datetime(2012,1,1).date(),'T0':pd.datetime(2011,1,1).date()  }\n",
    "]\n",
    "chservlibs.request_db_charts(entries,5003)\n",
    "# img=chlibs.CurrentByFutureChart_bydb(entries[0]['T0'],entries[0]['TF'],entries[0]['Symbol'],indicatorlist=(),pricecols=(),querycols=(),featcols=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Running Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"---------------- features-------------------\"\n",
    "featurecodes=ftmd.FeatureComputeCode.objects.all()\n",
    "computecode=featurecodes[0]\n",
    "computeclass=computecode.importcomputeclass()\n",
    "CF=computeclass(stk.id,Trange)\n",
    "CF.computeall(skipdone=True)\n",
    "# CF.saveall()\n",
    "print CF.getfeaturelist()\n",
    "\n",
    "CF.df=CF.addindicators(CF.df,[\n",
    "        {'name':'SMAstd','timeperiod':20,'colname':'SMAstd20'},\n",
    "        {'name':'EMAstd','timeperiod':8,'colname':'EMAstd8'},\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "querycodes=qrymd.QueryComputeCode.objects.all()\n",
    "computecode=querycodes[0]\n",
    "computeclass=computecode.importcomputeclass()\n",
    "CQ=computeclass(stk.id,Trange)\n",
    "CQ.computeall(skipdone=True)\n",
    "CQ.saveall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CQ.OutcomeCharts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CQ.getquerylist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CQ.chartfeatures(addpricecols=(),ip=5562,\n",
    "addfeatcols=[\n",
    "    ['CCI5','CCI50'],\n",
    "    ['PastPROFIT10days','PastLOSS10days'],['FutPROFIT10days','FutLOSS10days']\n",
    "],\n",
    "addquerycols=[\n",
    "    'CCICHERRIES',\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datascience import libs as dtsclibs\n",
    "import pandas as pd\n",
    "\n",
    "@dtsclibs.register_compfunc(RequiredImports=['import pandas as pd','from dataapp import libs as dtalibs'],overwrite_if_exists=False)\n",
    "def extractdataset(data_id,Symbol):\n",
    "    \"\"\"\n",
    "    @funcName : test\n",
    "    @input x : an int\n",
    "    @output df : pd.DataFrame, some random 2 by 2 \n",
    "    @description : takes an int and then returns a dummy dataframe. This is just for testing purposes\n",
    "    @Source : \n",
    "    def extractdataset(data_id,Symbol):\n",
    "        window=60\n",
    "        window_fut=30\n",
    "        Tfs=map(lambda x: ( (x.date()-pd.Dateoffset(window)).date(),x.date(), (x.date()+pd.Dateoffset(window_fut)).date() ),\n",
    "                pd.date_range(start=pd.datetime(2010,1,1),end=pd.datetime.today(),freq='W-MON') )\n",
    "\n",
    "        N=len(Tfs)\n",
    "        dfinstants=pd.DataFrame({'T0':map(lambda x: x[0],Tfs),'TF':map(lambda x: x[1],Tfs),'Symbol':[Symbol]*N})\n",
    "        X,X_Meta=dtalibs.Getbatchdata(dfinstants)\n",
    "\n",
    "        dfinstants=pd.DataFrame({'T0':map(lambda x: x[1],Tfs),'TF':map(lambda x: x[2],Tfs),'Symbol':[Symbol]*N})\n",
    "        Y,Y_Meta=dtalibs.Getbatchdata(dfinstants)\n",
    "\n",
    "        shard=dtscmd.DataShard(Data__id=data_id)\n",
    "        shard.Info['X_Meta']=X_Meta\n",
    "        shard.Info['Y_Meta']=Y_Meta\n",
    "        shard.save()\n",
    "\n",
    "        np.savez_compressed(shard.shardpath(),X=X,Y=Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    window=60\n",
    "    window_fut=30\n",
    "    Tfs=map(lambda x: ( (x.date()-pd.Dateoffset(window)).date(),x.date(), (x.date()+pd.Dateoffset(window_fut)).date() ),\n",
    "            pd.date_range(start=pd.datetime(2010,1,1),end=pd.datetime.today(),freq='W-MON') )\n",
    "    \n",
    "    N=len(Tfs)\n",
    "    dfinstants=pd.DataFrame({'T0':map(lambda x: x[0],Tfs),'TF':map(lambda x: x[1],Tfs),'Symbol':[Symbol]*N})\n",
    "    X,X_Meta=dtalibs.Getbatchdata(dfinstants)\n",
    "    \n",
    "    dfinstants=pd.DataFrame({'T0':map(lambda x: x[1],Tfs),'TF':map(lambda x: x[2],Tfs),'Symbol':[Symbol]*N})\n",
    "    Y,Y_Meta=dtalibs.Getbatchdata(dfinstants)\n",
    "    \n",
    "    shard=dtscmd.DataShard(Data__id=data_id)\n",
    "    shard.Info['X_Meta']=X_Meta\n",
    "    shard.Info['Y_Meta']=Y_Meta\n",
    "    shard.save()\n",
    "    \n",
    "    np.savez_compressed(shard.shardpath(),X=X,Y=Y)\n",
    "\n",
    "extractdataset.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating initial Stock price dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "projid,dataid=dtsclibs.register_dataset(project_Name=\"PredictReturn\",project_Info={'description': \"Data taken on every Monday. 360 days back and 60 days forward\"},\n",
    "                                        Datatype='RawProcessed',GroupName=\"AllStocks\",tag=\"1\",\n",
    "                                        data_format='npz',Modeltype='Regression',\n",
    "                                        TransformedFromDataId=None,TransFuncId=None, use_project_ifexists=True)\n",
    "\n",
    "dtsctks.CreateStockData_2(360,60,dataid,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print dtscmd.Data.objects.filter(Project__id=7,GroupName=\"AllStocks\",tag=\"1\",Datatype='RawProcessed',\n",
    "                           Dataformat='npz',Modeltype='Regression')\n",
    "data = dtscmd.Data.objects.get(Project__id=7,GroupName=\"AllStocks\",tag=\"1\",Datatype='RawProcessed',\n",
    "                           Dataformat='npz',Modeltype='Regression')\n",
    "\n",
    "data.datapath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtscmd.DataShard.objects.filter(Data=data).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtscmd.DataShard.objects.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions on shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over writing previous function\n",
      "saving function :  CleanData\n",
      "function id =  20\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "@dtsclibs.register_compfunc(Group='Cleanup',overwrite_if_exists=True)\n",
    "def CleanData(shardId):\n",
    "    \"\"\"\n",
    "    @Description: remove shards with many Nan samples. Remove samples with many Nans\n",
    "    @Source\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import datascience.models as dtscmd\n",
    "    \n",
    "    \n",
    "    \n",
    "    shard=dtscmd.DataShard.objects.get(id=shardId)\n",
    "    X,Y,Meta=shard.getdata()\n",
    "    \n",
    "    # remove samples with lot of nans\n",
    "    Nsamples=X.shape[0]\n",
    "    Tsteps=X.shape[1]\n",
    "    Nfeat=X.shape[2]\n",
    "    volumecols=[4,11,12]\n",
    "    colsX=list( Meta['MetaX']['columns'] )\n",
    "    colsY=list( Meta['MetaY']['columns'] )\n",
    "    delsample=[]\n",
    "    for i in range(Nsamples):\n",
    "        dfX=pd.DataFrame(X[i,:,:],columns=colsX)\n",
    "        dfY=pd.DataFrame(X[i,:,:],columns=colsY)\n",
    "        if np.sum(pd.isnull(dfX['Close'].values).astype(float))/len(dfX)>=0.5:\n",
    "            delsample.append(i)\n",
    "    \n",
    "    X=np.delete(X,delsample,axis=0)\n",
    "    Y=np.delete(Y,delsample,axis=0)\n",
    "    \n",
    "    # if not more samples then delete the shard\n",
    "    if X.shape[0]/(Nsamples*1.0) <0.4:\n",
    "        print \"----------------------\"\n",
    "        print str(shardId)+\"   \"+ str(X.shape[0]/(Nsamples*1.0))\n",
    "        print \"----------------------\"\n",
    "        print \"delete shaord id = \"+str(shard.id)\n",
    "        shard.delete()\n",
    "    elif X.shape[0]!=Nsamples:\n",
    "        shard.savedata(X=X,Y=Y,Meta=Meta)\n",
    "    else:\n",
    "        pass\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import datascience.models as dtscmd\n",
    "    \n",
    "    \n",
    "    \n",
    "    shard=dtscmd.DataShard.objects.get(id=shardId)\n",
    "    X,Y,Meta=shard.getdata()\n",
    "    \n",
    "    # remove samples with lot of nans\n",
    "    Nsamples=X.shape[0]\n",
    "    Tsteps=X.shape[1]\n",
    "    Nfeat=X.shape[2]\n",
    "    volumecols=[4,11,12]\n",
    "    colsX=list( Meta['MetaX']['columns'] )\n",
    "    colsY=list( Meta['MetaY']['columns'] )\n",
    "    delsample=[]\n",
    "    for i in range(Nsamples):\n",
    "        dfX=pd.DataFrame(X[i,:,:],columns=colsX)\n",
    "        dfY=pd.DataFrame(X[i,:,:],columns=colsY)\n",
    "        if np.sum(pd.isnull(dfX['Close'].values).astype(float))/len(dfX)>=0.5:\n",
    "            delsample.append(i)\n",
    "    \n",
    "    X=np.delete(X,delsample,axis=0)\n",
    "    Y=np.delete(Y,delsample,axis=0)\n",
    "    \n",
    "    # if not more samples then delete the shard\n",
    "    if X.shape[0]/(Nsamples*1.0) <0.4:\n",
    "        print \"----------------------\"\n",
    "        print str(shardId)+\"   \"+ str(X.shape[0]/(Nsamples*1.0))\n",
    "        print \"----------------------\"\n",
    "        print \"delete shaord id = \"+str(shard.id)\n",
    "        shard.delete()\n",
    "    elif X.shape[0]!=Nsamples:\n",
    "        shard.savedata(X=X,Y=Y,Meta=Meta)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "# dtsctks.applyfunc2data(CleanData.id,6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6548"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtscmd.DataShard.objects.filter(Data__id=6).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Creating derived DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over writing previous function\n",
      "saving function :  StandardizeData_1\n",
      "function id =  21\n",
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "saving transfoermer function to this dataset\n",
      "('project id', 'data id')  :  (7, 9)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "\n",
    "@dtsclibs.register_compfunc(Group='Transformer',overwrite_if_exists=True)\n",
    "def StandardizeData_1(X,Y,Meta):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    \"\"\"\n",
    "    A transformer function has to take X,Y,Meta and return another modified X,Y,Meta\n",
    "    1. Normalize all data as \n",
    "        1. Volume to 0-1\n",
    "        2. Prices --> also 0-1\n",
    "    2. For output Y data:\n",
    "        1. Take only Close\n",
    "        2. Pull out best returns in first 5 days, 10 days, 30 days, 60 days, 90 days\n",
    "        3. Pull out worst loss in first 5 days, 10 days, 30 days, 60 days, 90 days\n",
    "    \n",
    "    @Source:\n",
    "    # next normalize the volume to 0-1\n",
    "    Nsamples=X.shape[0]\n",
    "    Tsteps=X.shape[1]\n",
    "    Nfeat=X.shape[2]\n",
    "    volumecols=['Volume','VolSMA10','VolSMA20']\n",
    "    pricecols=['Close','Open','High','Low','SMA10','SMA20','SMA50','SMA100','SMA200','EMA8','EMA20']\n",
    "    colsX=list( Meta['MetaX']['columns'] )\n",
    "    colsY=list( Meta['MetaY']['columns'] )\n",
    "    FinalXcols=pricecols+volumecols\n",
    "    FinalYcols=['FutProfit5days','FutProfit10days','FutProfit30days','FutProfit60days','FutProfit90days']+['FutLoss5days',\n",
    "                                    'FutLoss10days','FutLoss30days','FutLoss60days','FutLoss90days']\n",
    "    \n",
    "    Xn=None\n",
    "    Yn=None\n",
    "    Metan=None\n",
    "    for i in range(Nsamples):\n",
    "        dfX=pd.DataFrame(X[i,:,:],columns=colsX)\n",
    "        dfY=pd.DataFrame(Y[i,:,:],columns=colsY)\n",
    "        \n",
    "        \n",
    "        # clean up Y\n",
    "        dfY.drop('Symbol',axis=1,inplace=True)\n",
    "        \n",
    "        dfY.drop(volumecols,axis=1,inplace=True)\n",
    "        dfY.drop([cc for cc in pricecols if cc!='Close'],axis=1,inplace=True)\n",
    "        \n",
    "        Ydict={}\n",
    "        dfY['ZeroPerf']=0\n",
    "    \n",
    "#         dfY['FutProfit5days']=-100*self.df['Close'].diff(periods=-5)/self.df['Close']\n",
    "        dfY['Returns']=100*(dfY['Close']-dfY['Close'].iloc[0])/dfY['Close'].iloc[0]\n",
    "        \n",
    "        Ydict['FutProfit5days']=dfY[['Returns','ZeroPerf']].iloc[0:5].max(axis=1).round().max()\n",
    "        Ydict['FutProfit10days']=dfY[['Returns','ZeroPerf']].iloc[0:10].max(axis=1).round().max()\n",
    "        Ydict['FutProfit30days']=dfY[['Returns','ZeroPerf']].iloc[0:30].max(axis=1).round().max()\n",
    "        Ydict['FutProfit60days']=dfY[['Returns','ZeroPerf']].iloc[0:60].max(axis=1).round().max()\n",
    "        Ydict['FutProfit90days']=dfY[['Returns','ZeroPerf']].iloc[0:90].max(axis=1).round().max()\n",
    "        \n",
    "        Ydict['FutLoss5days']=dfY[['Returns','ZeroPerf']].iloc[0:5].min(axis=1).round().min()\n",
    "        Ydict['FutLoss10days']=dfY[['Returns','ZeroPerf']].iloc[0:10].min(axis=1).round().min()\n",
    "        Ydict['FutLoss30days']=dfY[['Returns','ZeroPerf']].iloc[0:30].min(axis=1).round().min()\n",
    "        Ydict['FutLoss60days']=dfY[['Returns','ZeroPerf']].iloc[0:60].min(axis=1).round().min()\n",
    "        Ydict['FutLoss90days']=dfY[['Returns','ZeroPerf']].iloc[0:90].min(axis=1).round().min()\n",
    "        \n",
    "        # clean up X\n",
    "        dfX.drop('Symbol',axis=1,inplace=True)\n",
    "        \n",
    "        mxvol=dfX['Volume'].max()\n",
    "        dfX['Volume']=dfX['Volume']/mxvol\n",
    "        dfX['VolSMA10']=dfX['VolSMA10']/mxvol\n",
    "        dfX['VolSMA20']=dfX['VolSMA20']/mxvol\n",
    "        \n",
    "        mxHigh=dfX['High'].max()\n",
    "        mnLow=dfX['Low'].min()\n",
    "        for cc in pricecols:\n",
    "            dfX[cc]=(dfX[cc]-mnLow)/mxHigh\n",
    "        \n",
    "        XX=np.expand_dims( dfX[FinalXcols].astype(float).values   ,axis=0     )\n",
    "        YY=np.expand_dims( np.array([int(Ydict[key]) for key in FinalYcols]),axis=0 )\n",
    "        if Xn is None:\n",
    "            Xn=XX\n",
    "            Yn=YY\n",
    "        else:\n",
    "            Xn=np.vstack((Xn,XX))\n",
    "            Yn=np.vstack((Yn,YY))\n",
    "    \n",
    "    Metan=Meta\n",
    "    Metan['MetaX']['columns']=FinalXcols\n",
    "    Metan['MetaY']['columns']=FinalYcols\n",
    "    \n",
    "    return Xn,Yn,Metan\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # next normalize the volume to 0-1\n",
    "    Nsamples=X.shape[0]\n",
    "    Tsteps=X.shape[1]\n",
    "    Nfeat=X.shape[2]\n",
    "    volumecols=['Volume','VolSMA10','VolSMA20']\n",
    "    pricecols=['Close','Open','High','Low','SMA10','SMA20','SMA50','SMA100','SMA200','EMA8','EMA20']\n",
    "    colsX=list( Meta['MetaX']['columns'] )\n",
    "    colsY=list( Meta['MetaY']['columns'] )\n",
    "    FinalXcols=pricecols+volumecols\n",
    "    FinalYcols=['FutProfit5days','FutProfit10days','FutProfit30days','FutProfit60days','FutProfit90days']+['FutLoss5days',\n",
    "                                    'FutLoss10days','FutLoss30days','FutLoss60days','FutLoss90days']\n",
    "    \n",
    "    Xn=None\n",
    "    Yn=None\n",
    "    Metan=None\n",
    "    for i in range(Nsamples):\n",
    "        dfX=pd.DataFrame(X[i,:,:],columns=colsX)\n",
    "        dfY=pd.DataFrame(Y[i,:,:],columns=colsY)\n",
    "        \n",
    "        \n",
    "        # clean up Y\n",
    "        dfY.drop('Symbol',axis=1,inplace=True)\n",
    "        \n",
    "        dfY.drop(volumecols,axis=1,inplace=True)\n",
    "        dfY.drop([cc for cc in pricecols if cc!='Close'],axis=1,inplace=True)\n",
    "        \n",
    "        Ydict={}\n",
    "        dfY['ZeroPerf']=0\n",
    "    \n",
    "#         dfY['FutProfit5days']=-100*self.df['Close'].diff(periods=-5)/self.df['Close']\n",
    "        dfY['Returns']=100*(dfY['Close']-dfY['Close'].iloc[0])/dfY['Close'].iloc[0]\n",
    "        \n",
    "        Ydict['FutProfit5days']=dfY[['Returns','ZeroPerf']].iloc[0:5].max(axis=1).round().max()\n",
    "        Ydict['FutProfit10days']=dfY[['Returns','ZeroPerf']].iloc[0:10].max(axis=1).round().max()\n",
    "        Ydict['FutProfit30days']=dfY[['Returns','ZeroPerf']].iloc[0:30].max(axis=1).round().max()\n",
    "        Ydict['FutProfit60days']=dfY[['Returns','ZeroPerf']].iloc[0:60].max(axis=1).round().max()\n",
    "        Ydict['FutProfit90days']=dfY[['Returns','ZeroPerf']].iloc[0:90].max(axis=1).round().max()\n",
    "        \n",
    "        Ydict['FutLoss5days']=dfY[['Returns','ZeroPerf']].iloc[0:5].min(axis=1).round().min()\n",
    "        Ydict['FutLoss10days']=dfY[['Returns','ZeroPerf']].iloc[0:10].min(axis=1).round().min()\n",
    "        Ydict['FutLoss30days']=dfY[['Returns','ZeroPerf']].iloc[0:30].min(axis=1).round().min()\n",
    "        Ydict['FutLoss60days']=dfY[['Returns','ZeroPerf']].iloc[0:60].min(axis=1).round().min()\n",
    "        Ydict['FutLoss90days']=dfY[['Returns','ZeroPerf']].iloc[0:90].min(axis=1).round().min()\n",
    "        \n",
    "        # clean up X\n",
    "        dfX.drop('Symbol',axis=1,inplace=True)\n",
    "        \n",
    "        mxvol=dfX['Volume'].max()\n",
    "        dfX['Volume']=dfX['Volume']/mxvol\n",
    "        dfX['VolSMA10']=dfX['VolSMA10']/mxvol\n",
    "        dfX['VolSMA20']=dfX['VolSMA20']/mxvol\n",
    "        \n",
    "        mxHigh=dfX['High'].max()\n",
    "        mnLow=dfX['Low'].min()\n",
    "        for cc in pricecols:\n",
    "            dfX[cc]=(dfX[cc]-mnLow)/mxHigh\n",
    "        \n",
    "        XX=np.expand_dims( dfX[FinalXcols].astype(float).values   ,axis=0     )\n",
    "        YY=np.expand_dims( np.array([int(Ydict[key]) for key in FinalYcols]),axis=0 )\n",
    "        if Xn is None:\n",
    "            Xn=XX\n",
    "            Yn=YY\n",
    "        else:\n",
    "            Xn=np.vstack((Xn,XX))\n",
    "            Yn=np.vstack((Yn,YY))\n",
    "    \n",
    "    Metan=Meta\n",
    "    Metan['MetaX']['columns']=FinalXcols\n",
    "    Metan['MetaY']['columns']=FinalYcols\n",
    "    \n",
    "    return Xn,Yn,Metan\n",
    "\n",
    "\n",
    "projectid,dataid=dtsclibs.register_dataset(tag='NormalizedDerivedFromId6',TransformedFromDataId=6,TransFuncId=StandardizeData_1.id )\n",
    "\n",
    "dtsctks.Perform_TransformData(dataid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over writing previous function\n",
      "saving function :  StandardizeData_1\n",
      "function id =  21\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_CloseReturnVolume01\n",
      "function id =  22\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_Close01Volume01\n",
      "function id =  23\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_CloseSMAVolSMA10\n",
      "function id =  24\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_Close01Volume01_X30_Y5\n",
      "function id =  25\n",
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "updating data info\n",
      "saving transfoermer function to this dataset\n",
      "('project id', 'data id')  :  (7, 10)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "import dataapp.stockdata_transformers as stktrfrmr\n",
    "\n",
    "funcid=stktrfrmr.StandardizeData_Close01Volume01_X30_Y5.id\n",
    "DataInfo={'description':'All stocks data, close price is made into 0-1, Volume is made into 0-1, Y is profit/(profit+loss) for next 5 days'}\n",
    "projectid,dataid=dtsclibs.register_dataset(GroupName='AllStocksCloseVol',DataInfo=DataInfo,tag='NormalizedDerived30days_5daysFromId6',TransformedFromDataId=6,TransFuncId=funcid )\n",
    "\n",
    "dtsctks.Perform_TransformData(dataid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "updating data info\n",
      "Deleting the existing shards for this data\n",
      "saving transfoermer function to this dataset\n",
      "('project id', 'data id')  :  (7, 42)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "import datascience.DataCodes.AnonymousUser as stktrfrmr\n",
    "\n",
    "funcid=stktrfrmr.StandardizeData_Close01Volume01_X30_Y5prfbylss_interpcleaned_flatout.id\n",
    "DataInfo={'description':'FLATTENED!!!  All stocks data, close price is made into 0-1, Volume is made into 0-1, Y is profit/(profit+loss) for next 5 days'}\n",
    "projectid,dataid=dtsclibs.register_dataset(GroupName='AllStocksCloseVol_FLATTENED',DataInfo=DataInfo,tag='NormalizedDerived30days_5daysFromId6',TransformedFromDataId=6,TransFuncId=funcid,DeleteShards=True )\n",
    "\n",
    "dtsctks.Perform_TransformData(dataid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Shard Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over writing previous function\n",
      "saving function :  DataShardMeta_1\n",
      "function id =  29\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_1\n",
      "function id =  21\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_CloseReturnVolume01\n",
      "function id =  22\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_Close01Volume01\n",
      "function id =  23\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_CloseSMAVolSMA10\n",
      "function id =  24\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_Close01Volume01_X30_Y5prfbylss_interpcleaned\n",
      "function id =  27\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_Close01Volume01_X30_Y5prfbylss_interpcleaned_flatout\n",
      "function id =  28\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "import datascience.DataCodes.AnonymousUser as stktrfrmr\n",
    "\n",
    "funcId=stktrfrmr.DataShardMeta_1.id\n",
    "dataId=42\n",
    "M=dtsctks.applyfunc2data(funcId,dataId,wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, (394, 60), (394, 1)),\n",
       " (0, 0, (394, 60), (394, 1)),\n",
       " (0, 0, (394, 60), (394, 1)),\n",
       " (0, 0, (394, 60), (394, 1)),\n",
       " (0, 0, (394, 60), (394, 1)),\n",
       " (0, 0, (394, 60), (394, 1)),\n",
       " (0, 0, (394, 60), (394, 1)),\n",
       " (0, 0, (394, 60), (394, 1)),\n",
       " (0, 0, (394, 60), (394, 1)),\n",
       " (0, 0, (394, 60), (394, 1))]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.values()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 2315765, 2315765]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x,y:[x[0]+y[0],x[1]+y[1],x[2]+y[2][0],x[3]+y[3][0]],M.values(),[0,0,0,0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Overview of Projects and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Data ----------------------\n",
      "Project  ::  ProjectName= PredictReturn  ProjectId 7\n",
      "\tid= 6  ParentId= None\n",
      "\tGroupName= AllStocks  tag= 1\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= RawProcessed  Dataformat= npz\n",
      "\t#shards= 6548\n",
      "\t ------------------\n",
      "\tid= 42  ParentId= 6\n",
      "\tGroupName= AllStocksCloseVol_FLATTENED  tag= NormalizedDerived30days_5daysFromId6\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= RawProcessed  Dataformat= npz\n",
      "\t#shards= 6540\n",
      "\t ------------------\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "import datascience.stockdata_transformers as stktrfrmr\n",
    "\n",
    "\n",
    "    \n",
    "print \"------------ Data ----------------------\"\n",
    "for proj in dtscmd.Project.objects.all():\n",
    "    print \"Project  :: \",\"ProjectName=\",proj.Name,\" ProjectId\",proj.id\n",
    "    for data in dtscmd.Data.objects.filter(Project=proj).order_by(\"id\"): #\n",
    "        print \"\\t\",\"id=\",data.id,\" ParentId=\",data.ParentData.id if data.ParentData is not None else None \n",
    "        print \"\\t\",\"GroupName=\",data.GroupName,\" tag=\",data.tag\n",
    "        print \"\\t\",\"Modeltype=\",data.Modeltype,\" DataStructure=\",data.DataStructure\n",
    "        print \"\\t\",\"Datatype=\",data.Datatype,\" Dataformat=\",data.Dataformat\n",
    "        print \"\\t\",\"#shards=\",dtscmd.DataShard.objects.filter(Data=data).count()\n",
    "        print \"\\t ------------------\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shard=dtscmd.DataShard.objects.filter(id=308321).last()\n",
    "X,Y,Meta=shard.getdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=np.ones((2,3,4))\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create Train and Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datascience.ML.MLlibs as MLlibs\n",
    "MLlibs.get_train_test_from_RawProcessed(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
