{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/django/db/models/base.py:324: RuntimeWarning: Model 'datascience.label' was already registered. Reloading models is not advised as it can lead to inconsistencies, most notably with related models.\n",
      "  new_class._meta.apps.register_model(new_class._meta.app_label, new_class)\n",
      "/usr/local/lib/python2.7/dist-packages/django/db/models/base.py:324: RuntimeWarning: Model 'datascience.computefunc' was already registered. Reloading models is not advised as it can lead to inconsistencies, most notably with related models.\n",
      "  new_class._meta.apps.register_model(new_class._meta.app_label, new_class)\n",
      "/usr/local/lib/python2.7/dist-packages/django/db/models/base.py:324: RuntimeWarning: Model 'datascience.project' was already registered. Reloading models is not advised as it can lead to inconsistencies, most notably with related models.\n",
      "  new_class._meta.apps.register_model(new_class._meta.app_label, new_class)\n",
      "/usr/local/lib/python2.7/dist-packages/django/db/models/base.py:324: RuntimeWarning: Model 'datascience.data' was already registered. Reloading models is not advised as it can lead to inconsistencies, most notably with related models.\n",
      "  new_class._meta.apps.register_model(new_class._meta.app_label, new_class)\n",
      "/usr/local/lib/python2.7/dist-packages/django/db/models/base.py:324: RuntimeWarning: Model 'datascience.mlmodels' was already registered. Reloading models is not advised as it can lead to inconsistencies, most notably with related models.\n",
      "  new_class._meta.apps.register_model(new_class._meta.app_label, new_class)\n",
      "/usr/local/lib/python2.7/dist-packages/django/db/models/base.py:324: RuntimeWarning: Model 'datascience.modelmetrics' was already registered. Reloading models is not advised as it can lead to inconsistencies, most notably with related models.\n",
      "  new_class._meta.apps.register_model(new_class._meta.app_label, new_class)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named talib",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-73e8e2019a8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstockapp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstkmd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataapp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdtamd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataapp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtasks\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdtatks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataapp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdtalibs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfeatureapp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mftlibs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/venkat/n.adurthi@gmail.com/repos/trade_analytics/trade_analytics/dataapp/tasks.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutility\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelcomputations\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutpc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mitt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdataapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdtalibs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# make the function shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/venkat/n.adurthi@gmail.com/repos/trade_analytics/trade_analytics/dataapp/libs.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtalib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabstract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutility\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmaintenance\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmnt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named talib"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter, WeekdayLocator,\\\n",
    "    DayLocator, MONDAY,date2num,num2date,AutoDateLocator\n",
    "from matplotlib.finance import quotes_historical_yahoo_ohlc, candlestick_ohlc,candlestick2_ochl,volume_overlay3\n",
    "\n",
    "from stockapp import models as stkmd\n",
    "from dataapp import models as dtamd\n",
    "from dataapp import tasks as dtatks\n",
    "from dataapp import libs as dtalibs\n",
    "from featureapp import libs as ftlibs\n",
    "from featureapp import models as ftmd\n",
    "from stockapp import tasks as stktks\n",
    "from stockapp import libs as stklibs\n",
    "import featureapp.models as ftmd\n",
    "import featureapp.tasks as fttks\n",
    "import queryapp.models as qrymd\n",
    "import queryapp.tasks as qrytks\n",
    "\n",
    "import charts.chartservers.libs as chservlibs\n",
    "import charts.libs as chlibs\n",
    "\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "\n",
    "\n",
    "import featureapp as ftapp\n",
    "import utility as uty\n",
    "from utility import models as utymd\n",
    "import itertools as itt\n",
    "import multiprocessing as mp\n",
    "from django.db import connection,connections\n",
    "from django.db import reset_queries\n",
    "import time \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import inspect\n",
    "import imp\n",
    "import datetime\n",
    "from talib.abstract import *\n",
    "import utility.models as utmd\n",
    "import stockapp.libs as stklib\n",
    "from utility import codemanager as cdmng\n",
    "from utility import maintenance as mnt\n",
    "import os \n",
    "import json\n",
    "from django.contrib.auth.models import AnonymousUser\n",
    "import threading\n",
    "\n",
    "stk=stkmd.Stockmeta.objects.get(Symbol='TSLA')\n",
    "Fromdate=pd.datetime(2008,1,1)\n",
    "Todate=pd.datetime.today()\n",
    "Trange=pd.date_range(Fromdate,Todate)\n",
    "Trange=[T.date() for T in Trange if T.weekday()<=4]\n",
    "\n",
    "import json\n",
    "# fttks.computefeatuers(stk.id,Trange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to charting server\n",
      "received dataframe\n"
     ]
    }
   ],
   "source": [
    "entries=[\n",
    "    {'Symbol':'TSLA','TF':pd.datetime(2012,1,1).date(),'T0':pd.datetime(2011,1,1).date()  },\n",
    "    {'Symbol':'AAPL','TF':pd.datetime(2012,1,1).date(),'T0':pd.datetime(2011,1,1).date()  }\n",
    "]\n",
    "chservlibs.request_db_charts(entries,5003)\n",
    "# img=chlibs.CurrentByFutureChart_bydb(entries[0]['T0'],entries[0]['TF'],entries[0]['Symbol'],indicatorlist=(),pricecols=(),querycols=(),featcols=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from featureapp import libs as ftlibs\n",
    "import json,time\n",
    "\n",
    "store=pd.HDFStore(\"traindata.h5\")\n",
    "i=0\n",
    "for stk,df in ftlibs.GetFeature_iterator():\n",
    "    if 'EMALowPoly2win4Fit' in df.columns:\n",
    "        df['EMALowPoly2win4Fit']=df['EMALowPoly2win4Fit'].apply(lambda x: json.dumps(x))\n",
    "    if 'SMALowPoly2win4Fit' in df.columns:\n",
    "        df['SMALowPoly2win4Fit']=df['SMALowPoly2win4Fit'].apply(lambda x: json.dumps(x))\n",
    "    store.append('table',df,format='table',append=True,min_itemsize={'Symbol':15,'EMALowPoly2win4Fit':1000,'SMALowPoly2win4Fit':1000})\n",
    "    print \"iter = \",i,'\\r',\n",
    "    time.sleep(0.5)\n",
    "    i=i+1\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from featureapp import libs as ftlibs\n",
    "import json,time\n",
    "\n",
    "for stk,df in ftlibs.GetFeature_iterator():\n",
    "    if 'EMALowPoly2win4Fit' in df.columns:\n",
    "        df['EMALowPoly2win4Fit']=df['EMALowPoly2win4Fit'].apply(lambda x: json.dumps(x))\n",
    "    if 'SMALowPoly2win4Fit' in df.columns:\n",
    "        df['SMALowPoly2win4Fit']=df['SMALowPoly2win4Fit'].apply(lambda x: json.dumps(x))\n",
    "    if 'CCI50' not in df.columns or 'CCI5' not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    if os.path.isfile('training/train_'+stk.Symbol+'.npz'):\n",
    "        print \"file is there\"\n",
    "        continue\n",
    "    dp=df[ ((df['CCI50']-df['CCI5'])>=180) & (df['CCI50']>100) ]\n",
    "    if not dp.empty:\n",
    "        Xtrain=None\n",
    "        Ytrain=None\n",
    "        Tdone=[]\n",
    "        for Tind in dp.index:\n",
    "            for T in df[Tind:(Tind+pd.DateOffset(30)).date()].index:\n",
    "                if T in Tdone:\n",
    "                    continue\n",
    "                    \n",
    "                ds=dtalibs.GetStockData([stk.id],Fromdate= (T-pd.DateOffset(360)).date(),Todate=T)\n",
    "                x=ds[['Close','Open','High','Low','Volume']].values\n",
    "                x=np.vstack((np.zeros((360-x.shape[0],5)),x))\n",
    "                x = np.expand_dims(x, axis=0)\n",
    "                y=df.loc[T,['FutLOSS10days','FutLOSS30days','FutLOSS90days','FutPROFIT10days','FutPROFIT30days','FutPROFIT90days']].values\n",
    "                if Xtrain == None:\n",
    "                    Xtrain=x\n",
    "                    Ytrain=y\n",
    "                else:\n",
    "                    Xtrain=np.vstack( (Xtrain,x) )            \n",
    "                    Ytrain=np.vstack( (Ytrain,y) )\n",
    "                Tdone.append(T)\n",
    "                \n",
    "        \n",
    "        np.savez('training/train_'+stk.Symbol,Xtrain=Xtrain,Ytrain=Ytrain)\n",
    "        print \"Saved \"+stk.Symbol,\" \",Xtrain.shape, Ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=ds.values\n",
    "print x.shape\n",
    "x=np.vstack((np.zeros((360-x.shape[0],6)),x))\n",
    "print x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[Tind:(Tind+pd.DateOffset(30)).date()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tind=df.index[1000]\n",
    "dd=dtalibs.GetStockData([stk.id],Fromdate= (Tind-pd.DateOffset(360)).date(),Todate=Tind)\n",
    "dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=dd.iloc[0].values\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "featurecodes=ftmd.FeatureComputeCode.objects.all()\n",
    "for computecode in featurecodes:\n",
    "    computeclass=computecode.importcomputeclass()\n",
    "    CC=computeclass(6264,Trange)\n",
    "    CC.computeall(skipdone=True)\n",
    "    CC.saveall()\n",
    "#     del CC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print CC.df.shape\n",
    "print CC.df.index[2]\n",
    "CC.df['EMALowPoly2win4Fit'].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Trange=[T.date() for T in pd.date_range(pd.datetime(2002,1,1),pd.datetime.today()) if T.weekday()<=4]\n",
    "ftmd.FeaturesData.objects.filter(Symbol__id=6264,T__in=Trange).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Trange[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=ftlibs.GetFeature(Symbolids=[6264])\n",
    "print df.shape\n",
    "print df.index[2]\n",
    "df['EMALowPoly2win4Fit'].iloc[2]\n",
    "# df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[['EMA8','EMAstd8']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['FutPROFIT30days']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CC.saveall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replaceNaN2None(x):\n",
    "    print x\n",
    "    if type(x)==list:\n",
    "        for i in range(len(x)):\n",
    "            x[i]=replaceNaN2None(x[i])\n",
    "\n",
    "        return x\n",
    "    elif type(x)==tuple:\n",
    "        a=[]\n",
    "        for i in range(len(x)):\n",
    "            a.append(replaceNaN2None(x[i]))\n",
    "        x=tuple(a)\n",
    "        return x\n",
    "\n",
    "    elif type(x)==dict:\n",
    "        for k,v in x.items():\n",
    "            x[k]=replaceNaN2None(v)\n",
    "        return x\n",
    "    else:\n",
    "        if pd.isnull(x):\n",
    "            return None\t\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "D={'a':[1,2,np.nan],'b':{'c':None,'d':['a',np.nan,'gg',None]}}\n",
    "E=replaceNaN2None(D)\n",
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger=logging.getLogger('debug')\n",
    "logger.info(\"timing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stk.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Running Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"---------------- features-------------------\"\n",
    "featurecodes=ftmd.FeatureComputeCode.objects.all()\n",
    "computecode=featurecodes[0]\n",
    "computeclass=computecode.importcomputeclass()\n",
    "CF=computeclass(stk.id,Trange)\n",
    "CF.computeall(skipdone=True)\n",
    "# CF.saveall()\n",
    "print CF.getfeaturelist()\n",
    "\n",
    "CF.df=CF.addindicators(CF.df,[\n",
    "        {'name':'SMAstd','timeperiod':20,'colname':'SMAstd20'},\n",
    "        {'name':'EMAstd','timeperiod':8,'colname':'EMAstd8'},\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print \"---------------- Queries-------------------\"\n",
    "querycodes=qrymd.QueryComputeCode.objects.all()\n",
    "computecode=querycodes[0]\n",
    "computeclass=computecode.importcomputeclass()\n",
    "CQ=computeclass(stk.id,Trange)\n",
    "CQ.df=CF.df.copy()\n",
    "CQ.computeall(skipdone=True)\n",
    "# CQ.saveall()\n",
    "print CQ.OutcomeCharts()\n",
    "\n",
    "CQ.OutcomeCharts(perf=['FutPROFIT10days','FutLOSS10days','FutPROFIT30days','FutLOSS30days'])\n",
    "\n",
    "# CQ.chartfeatures(addpricecols=(),addfeatcols=[\n",
    "#     ['CCI5','CCI50'],\n",
    "#     ['PastPROFIT10days','PastLOSS10days','PastPROFIT30days','PastLOSS30days'],\n",
    "# #     [],\n",
    "# #     ['FutPROFIT10days','FutLOSS10days']\n",
    "# ],\n",
    "#  addquerycols=[\n",
    "#     'CCICHERRIES',\n",
    "# ],\n",
    "# ip=5562)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CQ.chartfeatures(addpricecols=(),addfeatcols=[\n",
    "# #     ['CCI5','CCI50'],\n",
    "#     ['PastPROFIT10days','PastLOSS10days','PastPROFIT30days','PastLOSS30days'],\n",
    "# #     [],\n",
    "# #     ['FutPROFIT10days','FutLOSS10days']\n",
    "# ],\n",
    "#  addquerycols=[\n",
    "#     'CCICHERRIES','SMA20BOUNCE1','SMA20BOUNCE2','SMA20BOUNCE3'\n",
    "# ],\n",
    "# ip=5562)\n",
    "\n",
    "\n",
    "CQ.chartfeatures(addpricecols=(),addfeatcols=[\n",
    "#     ['CCI5','CCI50'],\n",
    "    ['PastPROFIT10days','PastLOSS10days','PastPROFIT30days','PastLOSS30days'],\n",
    "#     [],\n",
    "#     ['FutPROFIT10days','FutLOSS10days']\n",
    "],\n",
    " addquerycols=[\n",
    "    'CHERRYSMABOUNCE',\n",
    "],\n",
    "ip=5562)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import simplejson\n",
    "def applyrollingfunc(df,newcolname,func,window,edge='right'):\n",
    "    df[newcolname]=np.nan\n",
    "    for i in range(1,len(df)-1):\n",
    "        if edge=='right':\n",
    "            dw=df.loc[ df.index[i-window:i],:  ]\n",
    "        elif edge=='left':\n",
    "            dw=df.loc[ df.index[i:i+window],:  ]\n",
    "        else:\n",
    "            dw=df.loc[ df.index[max(i-int(window/2),0):i+int(window/2)],:  ]\n",
    "        df.loc[df.index[i],newcolname]=func(dw)\n",
    "        \n",
    "    return df[newcolname]\n",
    "\n",
    "def getsignal(dw):\n",
    "    n=int( len(dw)/2.0 )\n",
    "    err=(dw['Low']-dw['SMA20']).values\n",
    "    p=np.polyfit(np.arange(len(err)),err,2)\n",
    "    mid=dw.index[n]\n",
    "#     print p,mid\n",
    "    if p[0]>0 and abs(min(err))<=0.5*dw['SMAstd20'].mean():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def getsignal(dw):\n",
    "    n=int( len(dw)/2.0 )\n",
    "    err=(dw['Low']-dw['SMA20']).values\n",
    "    p=np.polyfit(np.arange(len(err)),err,2)\n",
    "    s=[]\n",
    "    for a in p:\n",
    "        if np.isnan(a):\n",
    "            s.append(None)\n",
    "        else:\n",
    "            s.append(a)\n",
    "    return json.dumps( {'poly':s,'SMAstd20_mean':dw['SMAstd20'].mean(),'SMAstd20_max':dw['SMAstd20'].max(),'SMAstd20_min':dw['SMAstd20'].min()})\n",
    "\n",
    "def getbouncefeatures(dw):\n",
    "    n=int( len(dw)/2.0 )\n",
    "    err=(dw['Low']-dw['SMA20']).values\n",
    "    p=np.polyfit(np.arange(len(err)),err,2)\n",
    "    p=map(lambda x : None if np.isnan(x) else x,p)\n",
    "    return json.dumps({'poly':p,\n",
    "                        'err_mean':np.mean(err),'err_max':max(err),'err_min':min(err),\n",
    "                        'SMAstd20_mean':dw['SMAstd20'].mean(),'SMAstd20_max':dw['SMAstd20'].max(),'SMAstd20_min':dw['SMAstd20'].min()\n",
    "                        })\n",
    "\n",
    "CF.df['SMALowPoly2win4Fit']=applyrollingfunc(CF.df,'SMALowPoly2win4Fit',getbouncefeatures,4,edge='center')\n",
    "\n",
    "# CF.df=applyrollingfunc(CF.df,'mysignal',getsignal,4,edge='center')\n",
    "# print CF.df.shape,CF.df[CF.df['mysignal']==1].shape\n",
    "# CQ.chartfeatures(addpricecols=(),addfeatcols=[\n",
    "# #     ['CCI5','CCI50'],\n",
    "#     ['PastPROFIT10days','PastLOSS10days','PastPROFIT30days','PastLOSS30days'],\n",
    "# #     [],\n",
    "# #     ['FutPROFIT10days','FutLOSS10days']\n",
    "# ],\n",
    "#  addquerycols=[\n",
    "#     'mysignal','CCICHERRIES'\n",
    "# ],\n",
    "# ip=5562)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CF.df['SMALowPoly2win4Fit'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "querycodes=qrymd.QueryComputeCode.objects.all()\n",
    "computecode=querycodes[0]\n",
    "computeclass=computecode.importcomputeclass()\n",
    "CQ=computeclass(stk.id,Trange)\n",
    "CQ.computeall(skipdone=True)\n",
    "CQ.saveall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CQ.OutcomeCharts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CQ.getquerylist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CQ.chartfeatures(addpricecols=(),ip=5562,\n",
    "addfeatcols=[\n",
    "    ['CCI5','CCI50'],\n",
    "    ['PastPROFIT10days','PastLOSS10days'],['FutPROFIT10days','FutLOSS10days']\n",
    "],\n",
    "addquerycols=[\n",
    "    'CCICHERRIES',\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def applyrollingfunc(df,newcolname,func,window,edge='right'):\n",
    "    df[newcolname]=np.nan\n",
    "    for i in range(1,len(df)-1):\n",
    "        if edge=='right':\n",
    "            dw=df.loc[ df.index[i-window:i],:  ]\n",
    "        elif edge=='left':\n",
    "            dw=df.loc[ df.index[i:i+window],:  ]\n",
    "        else:\n",
    "            dw=df.loc[ df.index[max(i-int(window/2),0):i+int(window/2)],:  ]\n",
    "        df.loc[df.index[i],newcolname]=func(dw)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def getsignal(dw):\n",
    "    n=int( len(dw)/2.0 )\n",
    "    err=(dw['Low']-dw['SMA20']).values\n",
    "    p=np.polyfit(np.arange(len(err)),err,2)\n",
    "    mid=dw.index[n]\n",
    "    \n",
    "    if len(err[err>0])>len(err)/2 and p[0]>=0 and min(err)<=0.3*dw['SMAstd20'].max():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "CF.df=applyrollingfunc(CF.df,'mysignal',getsignal,10,edge='center')\n",
    "print CF.df.shape,CF.df[CF.df['mysignal']==1].shape\n",
    "CQ.chartfeatures(addpricecols=(),addfeatcols=[\n",
    "    ['CCI5','CCI50'],\n",
    "    ['PastPROFIT10days','PastLOSS10days','PastPROFIT30days','PastLOSS30days'],\n",
    "#     [],\n",
    "#     ['FutPROFIT10days','FutLOSS10days']\n",
    "],\n",
    " addquerycols=[\n",
    "    'CCICHERRIES','mysignal',\n",
    "],\n",
    "ip=5562)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window=10\n",
    "i=4\n",
    "# CF.df.index[[1,2,3,4]]\n",
    "range(max(i-int(window/2),0),i+int(window/2) )\n",
    "CF.df.loc[ CF.df.index[max(i-int(window/2),0):i+int(window/2)],:  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Add Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url='http://ec2-54-183-21-11.us-west-1.compute.amazonaws.com:8080/?symbols=TSLA,MSFT,AAPL&from=2012-01-01&to=2017-01-01'\n",
    "import urllib2\n",
    "response = urllib2.urlopen(url)\n",
    "html = response.read()\n",
    "D=json.loads(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D['query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(D['result'])\n",
    "# pd.DataFrame(D['result'])\n",
    "df=pd.DataFrame()\n",
    "for d in D['result']:\n",
    "    df=pd.concat([df,pd.DataFrame(d['quotes'])])\n",
    "df['date']=df['date'].apply(lambda x : pd.to_datetime(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Todate=pd.datetime(2017,1,1).date()\n",
    "Fromdate=pd.datetime(2014,8,1).date()\n",
    "stk=stkmd.Stockmeta.objects.get(Symbol='AAPL')\n",
    "df=dtalibs.GetStockData([stk.id],Fromdate,Todate,'concat')\n",
    "\n",
    "# uses close prices (default)\n",
    "df[['Open','High','Low','Close','Volume']]\n",
    "inputs1 = {\n",
    "    'open': df['Open'].values,\n",
    "    'high': df['High'].values,\n",
    "    'low': df['Low'].values,\n",
    "    'close': df['Close'].values,\n",
    "    'volume': df['Volume'].values\n",
    "}\n",
    "output = SMA(inputs1, timeperiod=25)\n",
    "df['sma20']=df['Close'].rolling(window=20).mean()\n",
    "df['smastd20']=df['Close'].rolling(window=20).std()\n",
    "df['ema20']=df['Close'].ewm(span=20).mean()\n",
    "df['emastd20']=df['Close'].ewm(span=20).std(bias=False)\n",
    "\n",
    "df['ema8']=df['Close'].ewm(span=8).mean()\n",
    "df['emastd8']=df['Close'].ewm(span=8).std(bias=False)\n",
    "\n",
    "\n",
    "df['sma50']=df['Close'].rolling(window=50).mean()\n",
    "df['smastd50']=df['Close'].rolling(window=50).std()\n",
    "df['ema50']=df['Close'].ewm(span=50).mean()\n",
    "df['emastd50']=df['Close'].ewm(span=50).std(bias=False)\n",
    "\n",
    "df['cci5'] = CCI(inputs1, timeperiod=5)\n",
    "df['cci50']= CCI(inputs1, timeperiod=50)\n",
    "df['hasCherries']=(df['cci50']-df['cci5'])>180\n",
    "\n",
    "def sequentialcluster(dp):\n",
    "    dp['date']=dp.index.copy()\n",
    "    dp.index=range(len(dp))\n",
    "    C=[]\n",
    "    c=[]\n",
    "    for i in dp.index[1:]:\n",
    "        if (dp.loc[i,'date']-dp.loc[i-1,'date']).days<=1:\n",
    "                c.append(dp.loc[i-1,'date'])\n",
    "        else:\n",
    "            if len(c)==0:\n",
    "                c=[dp.loc[i-1,'date']]\n",
    "            C.append(c)\n",
    "            c=[]\n",
    "    \n",
    "    C=[c[int(len(c)/2)] for c in C if len(c)>0]\n",
    "    dC=dp[dp['date'].isin(C)].copy()\n",
    "    dC.index=dC['date'].copy()\n",
    "    dC=dC[dC[\"Close\"]>=dC['sma20']].copy()\n",
    "    \n",
    "#     for ind in dC.index:\n",
    "    return dC\n",
    "        \n",
    "# C=sequentialcluster(dbsma20)\n",
    "\n",
    "\n",
    "dp=df[(df.index<Todate) & (df.index>Fromdate)].copy()\n",
    "\n",
    "dbsma20=dp[abs(dp['Close']-dp['ema20'])/dp['ema20']<= 0.6*dp['emastd20']/dp['ema20']].copy()\n",
    "# kmeans = KMeans(n_clusters=30, random_state=0).fit(dbsma20['date'].values.reshape(-1,1))\n",
    "# dateclusters=[pd.to_datetime(f) for f in kmeans.cluster_centers_.reshape(1,-1)[0]]\n",
    "\n",
    "dC=sequentialcluster(dbsma20)\n",
    "idx=[]\n",
    "print len(dC)\n",
    "for i in range(len(dC)):\n",
    "    for j in range(len(dp)):\n",
    "        if dp.index[j]==dC.index[i]:\n",
    "            break\n",
    "            \n",
    "    if (dp.loc[dp.index[j-2]:dp.index[j+3],'Close']-dp.loc[dp.index[j-2]:dp.index[j+3],'sma20']).min()>0:\n",
    "        idx.append(i)\n",
    "dC=dC.iloc[idx]\n",
    "\n",
    "fig,ax=plt.subplots(1,1,figsize=(15,7))\n",
    "ax.plot(dp.index,dp['Close'],linewidth=3,label='close')\n",
    "ax.plot(dp.index,dp['sma20'],'--',label='sma20')\n",
    "ax.plot(dp.index,dp['sma50'],'--',label='sma50')\n",
    "# ax.plot(dp.index,dp['sma20']+1*dp['smastd20'],'k--',label='+std20')\n",
    "# ax.plot(dp.index,dp['sma20']-1*dp['smastd20'],'k--',label='-std20')\n",
    "dc=dp[(dp['hasCherries']==True) & (dp['cci50']>100)].copy()\n",
    "ax.plot(dc.index,dc['Close'],'r',marker='o',linestyle='',label='cherry')\n",
    "ax.plot(dC.index,dC['Close'],'k',marker='*',markersize=10,linestyle='',label='bounce')\n",
    "\n",
    "\n",
    "ax.legend()\n",
    "fig,ax=plt.subplots(1,1,figsize=(15,2))\n",
    "ax.plot(dp.index,dp['Volume'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Todate=pd.datetime(2017,5,1).date()\n",
    "Fromdate=pd.datetime(2010,8,1).date()\n",
    "stk=stkmd.Stockmeta.objects.get(Symbol='TSLA')\n",
    "df=dtalibs.GetStockData([stk.id],Fromdate,Todate,'concat')\n",
    "\n",
    "# uses close prices (default)\n",
    "df[['Open','High','Low','Close','Volume']]\n",
    "inputs1 = {\n",
    "    'open': df['Open'].values,\n",
    "    'high': df['High'].values,\n",
    "    'low': df['Low'].values,\n",
    "    'close': df['Close'].values,\n",
    "    'volume': df['Volume'].values\n",
    "}\n",
    "output = SMA(inputs1, timeperiod=25)\n",
    "df['sma20']=df['Close'].rolling(window=20).mean()\n",
    "df['smastd20']=df['Close'].rolling(window=20).std()\n",
    "df['ema20']=df['Close'].ewm(span=20).mean()\n",
    "df['emastd20']=df['Close'].ewm(span=20).std(bias=False)\n",
    "\n",
    "df['ema8']=df['Close'].ewm(span=8).mean()\n",
    "df['emastd8']=df['Close'].ewm(span=8).std(bias=False)\n",
    "\n",
    "\n",
    "df['sma50']=df['Close'].rolling(window=50).mean()\n",
    "df['smastd50']=df['Close'].rolling(window=50).std()\n",
    "df['ema50']=df['Close'].ewm(span=50).mean()\n",
    "df['emastd50']=df['Close'].ewm(span=50).std(bias=False)\n",
    "\n",
    "df['cci5'] = CCI(inputs1, timeperiod=5)\n",
    "df['cci50']= CCI(inputs1, timeperiod=50)\n",
    "df['hasCherries']=(df['cci50']-df['cci5'])>180\n",
    "df['datenum']=date2num(df.index)\n",
    "\n",
    "def feature1(df,label=None):\n",
    "#     dp=df[(df['hasCherries']==True) & (df['cci50']>100)].copy()\n",
    "#     dp=df[df['cci50']>50].copy()\n",
    "    dp=df.copy()\n",
    "    dp['date']=dp.index.copy()\n",
    "    dp['bottom']=dp[['Open','Close']].min(axis=1)\n",
    "\n",
    "    dbsma20=dp[abs(dp['Low']-dp['ema20'])/dp['ema20']<= 0.2*dp['emastd20']/dp['ema20']].copy()\n",
    "#     dbsma20=dp[abs(dp['bottom']-dp['sma20'])/dp['sma20']<= 0.3*dp['smastd20']/dp['sma20']].copy()\n",
    "    \n",
    "#     dbsma20.index=range(len(dbsma20))\n",
    "#     C=[]\n",
    "#     c=[]\n",
    "#     for i in dbsma20.index[1:]:\n",
    "#         if (dbsma20.loc[i,'date']-dbsma20.loc[i-1,'date']).days<=1:\n",
    "#                 c.append(dbsma20.loc[i-1,'date'])\n",
    "#         else:\n",
    "#             if len(c)==0:\n",
    "#                 c=[dbsma20.loc[i-1,'date']]\n",
    "#             C.append(c)\n",
    "#             c=[]\n",
    "    \n",
    "#     C=[c[int(len(c)/2)] for c in C if len(c)>0]\n",
    "#     dC=dbsma20[dbsma20['date'].isin(C)].copy()\n",
    "    \n",
    "    \n",
    "    dC=dbsma20\n",
    "    \n",
    "    \n",
    "    dC.index=dC['date'].copy()\n",
    "\n",
    "    idx=[]\n",
    "    print len(dC)\n",
    "    for i in range(len(dC)):\n",
    "        for j in range(len(df)):\n",
    "            if df.index[j]==dC.index[i]:\n",
    "                break\n",
    "#         if (df.loc[df.index[j-3]:df.index[j],'Low']-df.loc[df.index[j-3]:df.index[j],'sma20']).min()>0:\n",
    "#             idx.append(i)\n",
    "#     dC=dC.iloc[idx]\n",
    "    \n",
    "    df['SMA20bounce']=False\n",
    "    for ind in dC.index:\n",
    "        df.loc[ind,'SMA20bounce']=True\n",
    "        ix=np.argwhere(df.index==ind)[0,0]\n",
    "#         if np.any(df.loc[df.index[ix-10:ix+10],'hasCherries'].values==True):\n",
    "#             df.loc[ind,'SMA20bounce']=True\n",
    "#         else:\n",
    "#             df.loc[ind,'SMA20bounce']=False\n",
    "            \n",
    "    return df\n",
    "\n",
    "def GetPerformance(df,label):\n",
    "    df['3monProf']=np.nan\n",
    "    df['6monProf']=np.nan\n",
    "    df['3monLoss']=np.nan\n",
    "    df['6monLoss']=np.nan\n",
    "    for ind in df[df[label]==True].index:\n",
    "        S=df.loc[ind:(ind+pd.DateOffset(90)).date(),'Close']-df.loc[ind,'Close']\n",
    "        df.loc[ind,'3monProf']=100*S[S>0].max()/df.loc[ind,'Close']\n",
    "        df.loc[ind,'3monLoss']=100*S[S<0].min()/df.loc[ind,'Close']\n",
    "        \n",
    "        S=df.loc[ind:(ind+pd.DateOffset(180)).date(),'Close']-df.loc[ind,'Close']\n",
    "        df.loc[ind,'6monProf']=100*S[S>0].max()/df.loc[ind,'Close']\n",
    "        df.loc[ind,'6monLoss']=100*S[S<0].min()/df.loc[ind,'Close']\n",
    "    return df\n",
    "\n",
    "def MakeChart(dF,T0,T,featcols):\n",
    "    df=dF[T0:T].copy()\n",
    "    autodate=AutoDateLocator()\n",
    "    mondays = WeekdayLocator(MONDAY)        # major ticks on the mondays\n",
    "    alldays = DayLocator()              # minor ticks on the days\n",
    "    weekFormatter = DateFormatter('%Y %b %d')  # e.g., Jan 12\n",
    "    dayFormatter = DateFormatter('%d')      # e.g., 12\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(30,10))\n",
    "    fig.subplots_adjust(bottom=0.2)\n",
    "    ax.xaxis.set_major_locator(autodate)\n",
    "    ax.xaxis.set_minor_locator(alldays)\n",
    "    ax.xaxis.set_major_formatter(weekFormatter)\n",
    "    #ax.xaxis.set_minor_formatter(dayFormatter)\n",
    "\n",
    "    quotes=[tuple(x) for x in df[['datenum','Open','High','Low','Close']].to_records(index=False)]\n",
    "    #plot_day_summary(ax, quotes, ticksize=3)\n",
    "    # candlestick2_ochl(ax, df['Open'], df['Close'], df['High'], df['Low'], width=4, colorup='k', colordown='r', alpha=0.75)\n",
    "    candlestick_ohlc(ax, quotes, width=0.6)\n",
    "    ax.plot(df['datenum'],df['sma20'],'--',label='sma20')\n",
    "    ax.plot(df['datenum'],df['sma50'],'--',label='sma50')\n",
    "    # ax.plot(dp.index,dp['sma20']+1*dp['smastd20'],'k--',label='+std20')\n",
    "    # ax.plot(dp.index,dp['sma20']-1*dp['smastd20'],'k--',label='-std20')\n",
    "#     dc=dp[(dp['hasCherries']==True) & (dp['cci50']>100)].copy()\n",
    "    for ff in  featcols:\n",
    "        dp=df[df[ff]==True]\n",
    "        ax.plot(dp['datenum'],dp['Close'],'r',marker='o',markersize=20,linestyle='',label=ff)\n",
    "#     ax.plot(dC['datenum'],dC['Close'],'k',marker='*',markersize=10,linestyle='',label='bounce')\n",
    "#     ax.arrow( dC['datenum'].iloc[0], dC['Close'].iloc[0]+1,0, 1, fc=\"k\", ec=\"k\",head_width=0.05, head_length=0.1 )\n",
    "\n",
    "    ax.xaxis_date()\n",
    "    ax.autoscale_view()\n",
    "    plt.setp(plt.gca().get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "\n",
    "    plt.show()\n",
    "    # mpld3.show()\n",
    "    # mpld3.display(fig)\n",
    "\n",
    "    \n",
    "\n",
    "df=feature1(df,label=None)\n",
    "MakeChart(df,pd.datetime(2010,1,1).date(),pd.datetime(2011,1,1).date(),['SMA20bounce'])\n",
    "MakeChart(df,pd.datetime(2011,1,1).date(),pd.datetime(2012,1,1).date(),['SMA20bounce'])\n",
    "MakeChart(df,pd.datetime(2012,1,1).date(),pd.datetime(2013,1,1).date(),['SMA20bounce'])\n",
    "MakeChart(df,pd.datetime(2013,1,1).date(),pd.datetime(2014,1,1).date(),['SMA20bounce'])\n",
    "MakeChart(df,pd.datetime(2014,1,1).date(),pd.datetime(2015,1,1).date(),['SMA20bounce'])\n",
    "MakeChart(df,pd.datetime(2015,1,1).date(),pd.datetime(2016,1,1).date(),['SMA20bounce'])\n",
    "MakeChart(df,pd.datetime(2016,1,1).date(),pd.datetime(2017,5,1).date(),['SMA20bounce'])\n",
    "\n",
    "\n",
    "df=GetPerformance(df,'SMA20bounce')\n",
    "plt.figure()\n",
    "df[['3monProf','3monLoss']].hist(figsize=(20,10),bins=20)\n",
    "plt.show()\n",
    "plt.figure()\n",
    "df[['6monProf','6monLoss']].hist(figsize=(20,10),bins=20)\n",
    "plt.show()\n",
    "\n",
    "color = dict(boxes='DarkGreen', whiskers='DarkOrange',medians='DarkBlue', caps='Gray')\n",
    "plt.figure()\n",
    "df[['3monProf','3monLoss','6monProf','6monLoss']].plot.box(color=color, sym='r+')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MakeChart(df,pricecols,querycols,featcols):\n",
    "    \"\"\"\n",
    "    pricecols are additional columns that have same range as close\n",
    "    querycols are true/false snapped to close price\n",
    "    featcols are additional that are plotted below\n",
    "    \n",
    "    For example:\n",
    "    pricecols=[{'colname':'sma20','plotargs':('g',),'plotkwargs':{'label':'sma20',}},\n",
    "          {'colname':'sma50','plotargs':('r',),'plotkwargs':{'label':'sma50',}}]\n",
    "    querycols=[{'colname':'hasCherries','plotargs':('y',),'plotkwargs':{'label':'hasCherries','marker':'o','markersize':15,'linestyle':''}}]\n",
    "    featcols=[ [{'colname':'cci5','plotargs':('r--',),'plotkwargs':{'label':'cci5',}}],\n",
    "               [ {'colname':'cci50','plotargs':('g',),'plotkwargs':{'label':'cci50',}}]\n",
    "             ]  \n",
    "    \"\"\"\n",
    "#     df=dF[T0:T].copy()\n",
    "#     dfperf=dF[T:(T+pd.DateOffset(360)).date()].copy()\n",
    "    \n",
    "    autodate=AutoDateLocator()\n",
    "    mondays = WeekdayLocator(MONDAY)        # major ticks on the mondays\n",
    "    alldays = DayLocator()              # minor ticks on the days\n",
    "    weekFormatter = DateFormatter('%Y %b %d')  # e.g., Jan 12\n",
    "    dayFormatter = DateFormatter('%d')      # e.g., 12\n",
    "    \n",
    "    Nsubplots=len(featcols)\n",
    "    fig, ax = plt.subplots(1,2,figsize=(15,7+2*Nsubplots))\n",
    "#     plt.figure(num=None, figsize=(30, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "#     fig.subplots_adjust(bottom=0.2)\n",
    "    ax1 = plt.subplot2grid((2+Nsubplots, 1), (0, 0), rowspan=2)\n",
    "    axft=[]\n",
    "    for i in range(Nsubplots):\n",
    "        axft.append(plt.subplot2grid((2+Nsubplots, 1), (2+i, 0),sharex=ax1))\n",
    "        \n",
    "    ax1.xaxis.set_major_locator(autodate)\n",
    "    ax1.xaxis.set_minor_locator(alldays)\n",
    "    ax1.xaxis.set_major_formatter(weekFormatter)\n",
    "    \n",
    "    \n",
    "\n",
    "    quotes=[tuple(x) for x in df[['datenum','Open','High','Low','Close','Volume']].to_records(index=False)]\n",
    "    ret=candlestick_ohlc(ax1, quotes, width=0.6)\n",
    "    \n",
    "    for prcl in pricecols:\n",
    "        ax1.plot(df['datenum'],df[prcl['colname'] ],*prcl['plotargs'],**prcl['plotkwargs'] )\n",
    "    \n",
    "    L=df['High'].max()-df['Low'].min()\n",
    "    mm=df['Volume'].max()\n",
    "    ax1.bar(df['datenum'],0.5*L*df['Volume']/mm,bottom=df['Low'].min()-1.5,color='y',alpha=0.5)\n",
    "#     volume_overlay3(ax[0], quotes, colorup='k', colordown='r', width=4, alpha=1.0)\n",
    "    \n",
    "    for qcl in  querycols:\n",
    "        dp=df[df[qcl['colname']]==True]\n",
    "        ax1.plot(dp['datenum'],dp['Close'],*qcl['plotargs'],**qcl['plotkwargs'])\n",
    "    \n",
    "    ax1.set_xlim(quotes[0][0],quotes[-1][0])\n",
    "    ax1.set_ylim(df['Low'].min()*0.9,df['High'].max()*1.1)\n",
    "    ax1.xaxis_date()\n",
    "    ax1.xaxis.tick_top()\n",
    "    ax1.autoscale_view()\n",
    "    ax1.legend()\n",
    "    ax1.grid()\n",
    "    \n",
    "    ax12 = ax1.twinx()\n",
    "    ax12.set_ylim(df['Low'].min()*0.9,df['High'].max()*1.1)\n",
    "    ax12.grid()\n",
    "    \n",
    "    plt.setp(ax1.get_xticklabels(), rotation=45, horizontalalignment='left')\n",
    "        \n",
    "    for i in range(len(featcols)):\n",
    "        autodate=AutoDateLocator()\n",
    "        mondays = WeekdayLocator(MONDAY)        # major ticks on the mondays\n",
    "        alldays = DayLocator()              # minor ticks on the days\n",
    "        weekFormatter = DateFormatter('%Y %b %d')  # e.g., Jan 12\n",
    "        dayFormatter = DateFormatter('%d')      # e.g., 12\n",
    "        \n",
    "        axft[i].xaxis.set_major_locator(autodate)\n",
    "        axft[i].xaxis.set_minor_locator(alldays)\n",
    "        axft[i].xaxis.set_major_formatter(weekFormatter)\n",
    "        \n",
    "        ftymax=-1000000\n",
    "        ftymin=100000\n",
    "        for ft in featcols[i]:\n",
    "            axft[i].plot(df['datenum'],df[ ft['colname'] ],*ft['plotargs'],**ft['plotkwargs'])\n",
    "            ftymin=min([ftymin,df[ft['colname']].min()])\n",
    "            ftymax=max([ftymax,df[ft['colname']].max()])\n",
    "\n",
    "#         axft[i].set_ylim(ftymin*0.8,ftymax*1.2)\n",
    "        axft[i].set_xlim(quotes[0][0],quotes[-1][0])\n",
    "        axft[i].autoscale_view()\n",
    "        axft[i].legend()\n",
    "        axft[i].grid()\n",
    "        \n",
    "        if i==len(featcols)-1:\n",
    "            axft[i].xaxis_date()\n",
    "            plt.setp(axft[i].get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "        else:\n",
    "            axft[i].get_xaxis().set_visible(False)\n",
    "            \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "pricecols=[{'colname':'sma20','plotargs':('g',),'plotkwargs':{'label':'sma20',}},\n",
    "          {'colname':'sma50','plotargs':('r',),'plotkwargs':{'label':'sma50',}}]\n",
    "querycols=[{'colname':'hasCherries','plotargs':('y',),'plotkwargs':{'label':'hasCherries','marker':'o','markersize':15,'linestyle':''}}]\n",
    "featcols=[ [{'colname':'cci5','plotargs':('r--',),'plotkwargs':{'label':'cci5',}}],\n",
    "           [ {'colname':'cci50','plotargs':('g',),'plotkwargs':{'label':'cci50',}}]\n",
    "         ]  \n",
    "T0=pd.datetime(2010,1,1).date()\n",
    "TF=pd.datetime(2011,1,1).date()\n",
    "MakeChart(df[T0:TF].copy(),pricecols,querycols,featcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import zlib\n",
    "msg={'featcols':['A'],'df':df.round(decimals=3)}\n",
    "msg2={'featcols':['A'],'df':pkl.dumps( df.round(decimals=3))   }\n",
    "p=pkl.dumps(msg)\n",
    "p2=pkl.dumps(msg2)\n",
    "z=zlib.compress(p)\n",
    "z2=zlib.compress(p2)\n",
    "print len(p),len(z),len(p2),len(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window=360\n",
    "T0=df.index[0]\n",
    "TF=(df.index[0]+pd.DateOffset(window) ).date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zmq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import zlib\n",
    "import pickle as pkl\n",
    "\n",
    "context = zmq.Context()\n",
    "\n",
    "#  Socket to talk to server\n",
    "print(\"Connecting to hello world server\")\n",
    "socket = context.socket(zmq.REQ)\n",
    "socket.connect(\"tcp://localhost:5557\")\n",
    "\n",
    "msg={'featcols':['SMA20bounce'],'df':df.round(decimals=3)}\n",
    "p=pkl.dumps(msg)\n",
    "z=zlib.compress(p)\n",
    "print len(p),len(z)\n",
    "\n",
    "socket.send(z)\n",
    "\n",
    "#  Get the reply.\n",
    "message = socket.recv()\n",
    "print message\n",
    "socket.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "class registerfeature(object):\n",
    "    registry=[]\n",
    "    def __init__(self,para,returntype=None,cache=True):\n",
    "        self.para=para\n",
    "        self.usecache=cache\n",
    "        self.cache={}\n",
    "    def __call__(self,func):\n",
    "        \n",
    "        self.name=func.__name__\n",
    "        self.doc=func.__doc__\n",
    "        self.registry.append([self.name,self.doc])\n",
    "        \n",
    "        def func2(*args,**kwargs):\n",
    "            key=json.dumps([args[1:],kwargs])\n",
    "            if self.usecache:\n",
    "                if key in self.cache:\n",
    "                    print \"return cached result\"\n",
    "                    return self.cache[key]\n",
    "                else:\n",
    "                    print \"computing and saving result\"\n",
    "                    self.cache[key]=func(*args,**kwargs)\n",
    "                    return self.cache[key]\n",
    "            else:\n",
    "                return func(*args,**kwargs)\n",
    "        func2.isfeature=True\n",
    "        \n",
    "        return func2\n",
    "\n",
    "class foo(object):\n",
    "\n",
    "    @registerfeature(3,returntype=float)\n",
    "    def SMA10(self):\n",
    "        \"\"\"\n",
    "        sma10\n",
    "        \"\"\"\n",
    "        return 32\n",
    "    \n",
    "    @classmethod\n",
    "    def getfeatures(cls):\n",
    "        print cls.__dict__.items()\n",
    "        return [x for x, y in cls.__dict__.items() if hasattr(cls.__dict__[x],'isfeature')]\n",
    "    def __getitem__(self,feat):\n",
    "        return getattr(self,feat)()\n",
    "\n",
    "\n",
    "foo.getfeatures()\n",
    "    \n",
    "    \n",
    "f=foo()\n",
    "f.getfeatures()\n",
    "# print f.SMA10()\n",
    "# print f.SMA10()\n",
    "# print f['SMA10']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dd=dtalibs.GetStockData(stk.id)\n",
    "dd.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Fromdate=pd.datetime(2008,1,1)\n",
    "Todate=pd.datetime.today()\n",
    "Trange=pd.date_range(Fromdate,Todate)\n",
    "Trange=[T.date() for T in Trange if T.weekday()<=4]\n",
    "\n",
    "pd.DataFrame(list(ftmd.FeaturesData.objects.filter(T__in=Trange).values_list('Featuredata',flat=True) ))\n",
    "# featdata.Featuredata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D={}\n",
    "D[(1,[1,2,3])]=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.16\n",
      "[1]\ttrain-error:0.14\n",
      "[2]\ttrain-error:0.1\n",
      "[3]\ttrain-error:0.02\n",
      "[4]\ttrain-error:0.02\n",
      "[5]\ttrain-error:0\n",
      "[6]\ttrain-error:0\n",
      "[7]\ttrain-error:0\n",
      "[8]\ttrain-error:0\n",
      "[9]\ttrain-error:0\n",
      "[0]\ttrain-error:0.16\n",
      "[1]\ttrain-error:0.14\n",
      "[2]\ttrain-error:0.1\n",
      "[3]\ttrain-error:0.02\n",
      "[4]\ttrain-error:0.02\n",
      "[5]\ttrain-error:0\n",
      "[6]\ttrain-error:0\n",
      "[7]\ttrain-error:0\n",
      "[8]\ttrain-error:0\n",
      "[9]\ttrain-error:0\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = np.random.rand(50,10) # 5 entities, each contains 10 features\n",
    "label = np.random.randint(2, size=50) # binary target\n",
    "dtrain = xgb.DMatrix( data, label=label)\n",
    "\n",
    "data2 = np.random.rand(7, 10)\n",
    "dtest = xgb.DMatrix(data2)\n",
    "\n",
    "param = {'max_depth':2, 'eta':1, 'silent':1, 'objective':'binary:logistic' }\n",
    "param['nthread'] = 2\n",
    "# param['eval_metric'] = 'logloss'\n",
    "# param['eval_metric'] = ['auc', 'ams@0']\n",
    "\n",
    "evallist  = [(dtrain,'train')]\n",
    "\n",
    "num_round = 10\n",
    "plst = param.items()\n",
    "bst = xgb.train( plst, dtrain, num_round, evallist )\n",
    "bst.save_model('0001.model')\n",
    "\n",
    "bst = xgb.Booster() #init model\n",
    "bst.load_model('0001.model') # load data\n",
    "\n",
    "bst = xgb.train( plst, dtrain, num_round, evallist )\n",
    "\n",
    "\n",
    "ypred = bst.predict(dtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Project' object has no attribute 'ProjectPath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6f999129e12c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtscmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMisc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Testing all code and models'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/venkat/Documents/repos/trade_analytics/trade_analytics/datascience/models.pyc\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;31m# save the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProjectPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Project' object has no attribute 'ProjectPath'"
     ]
    }
   ],
   "source": [
    "obj=dtscmd.Project(Name='test',Misc={'description':'Testing all code and models'})\n",
    "obj.save()\n",
    "obj.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "obj=dtscmd.Project.objects.all()[1]\n",
    "print obj\n",
    "obj.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "mean = [0, 0, 0, 0]\n",
    "cov = [[100, 0,0,0], [1, 100,50,1], [5, 5,50,5], [10, 10,50,1000]]  # diagonal covariance\n",
    "X1=np.random.multivariate_normal(mean, cov, 5000)\n",
    "Y1=np.array(['A']*5000).reshape(-1,1)\n",
    "\n",
    "mean = [20, 10, 5, 0]\n",
    "cov = np.array( [[100, 0,0,0], [1, 100,50,1], [5, 5,50,5], [10, 10,50,1000]] )/2 # diagonal covariance \n",
    "X2=np.random.multivariate_normal(mean, cov, 3000)\n",
    "Y2=np.array(['B']*3000).reshape(-1,1)\n",
    "\n",
    "\n",
    "mean = -1*np.array([20, 10, 5, 0])\n",
    "cov = np.array( [[100, 0,0,0], [1, 100,50,1], [5, 5,50,5], [10, 10,50,1000]])/3  # diagonal covariance\n",
    "X3=np.random.multivariate_normal(mean, cov, 100)\n",
    "Y3=np.array(['C']*100).reshape(-1,1)\n",
    "\n",
    "X=np.vstack((X1,X2,X3))\n",
    "Y=np.vstack((Y1,Y2,Y3))\n",
    "D={'X':X,'Y':Y}\n",
    "import joblib\n",
    "with open('class3set.joblib','w') as F:\n",
    "    joblib.dump(D,F,compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "coercing to Unicode: need string or buffer, tuple found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-344987ee49a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mRawData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRawData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewshardpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: coercing to Unicode: need string or buffer, tuple found"
     ]
    }
   ],
   "source": [
    "RawData= dtscmd.Data(Project=obj,GroupName='RawMultiClass',tag='',Modeltype='Classification',Datatype='Raw',Dataformat='joblib')\n",
    "RawData.save()\n",
    "RawData.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name,path=RawData.newshardpath()\n",
    "with open(path,'w') as F:\n",
    "    joblib.dump({},F,compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cloudpickle as cldpkl\n",
    "\n",
    "class foo(object):\n",
    "    x=1\n",
    "    def f1(self,a):\n",
    "        print \"f1\"\n",
    "        return a+self.x\n",
    "class goo(foo):\n",
    "    y=2\n",
    "    def f2(self,b):\n",
    "        print \"f2\"\n",
    "        return b+self.y+self.x\n",
    "with open(\"test.pkl\",'w') as F:\n",
    "    cldpkl.dump(goo,F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "�\u0002ccloudpickle.cloudpickle\n",
      "_fill_function\n",
      "q\u0000(ccloudpickle.cloudpickle\n",
      "_make_skel_func\n",
      "q\u0001ccloudpickle.cloudpickle\n",
      "_builtin_type\n",
      "q\u0002U\bCodeTypeq\u0003�q\u0004Rq\u0005(K\u0000K\u0000K\u0001KCU\u0004d\u0001\u0000Sq\u0006U\u0014\n",
      "    doc string\n",
      "    q\u0007K!�q\b))U\u001e",
      "<ipython-input-2-3de01950b81d>q\tU\u0005func1q\n",
      "K\u0003U\u0002\u0000\u0004q\u000b",
      "))tq\f",
      "Rq\r",
      "J����}q\u000e�q\u000fRq\u0010}q\u0011N}q\u0012NtR.\n"
     ]
    }
   ],
   "source": [
    "from dill.source import getsource\n",
    "\n",
    "def func1():\n",
    "    \"\"\"\n",
    "    doc string\n",
    "    \"\"\"\n",
    "    return 33\n",
    "    \n",
    "import datascience.libs as dtalibs\n",
    "# dtalibs.getsrccode(func1)\n",
    "dtalibs.getpkl(func1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_id=register_dataset(\n",
    "    project_Name=\"PredictReturn_TSLA\", \n",
    "    create_project_ifnotexists=True,\n",
    "    project_Info={'description': \"Testing the algorithms on TSLA to predict next 10 day return \\n\"+\n",
    "                                 \"Data taken on every Friday\"},\n",
    "    GroupName=\"Fullstocktime\",\n",
    "    tag=\"1\",\n",
    "    Datatype='RawProcessed',\n",
    "    data_format='npz',\n",
    "    Modeltype='Regression',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @register_func(overwrite_if_exists=False)\n",
    "def extractdataset(data_id,Symbol):\n",
    "    window=60\n",
    "    window_fut=30\n",
    "    Tfs=map(lambda x: ( (x.date()-pd.Dateoffset(window)).date(),x.date(), (x.date()+pd.Dateoffset(window_fut)).date() ),\n",
    "            pd.date_range(start=pd.datetime(2010,1,1),end=pd.datetime.today(),freq='W-MON') )\n",
    "    \n",
    "    N=len(Tfs)\n",
    "    dfinstants=pd.DataFrame({'T0':map(lambda x: x[0],Tfs),'TF':map(lambda x: x[1],Tfs),'Symbol':[Symbol]*N})\n",
    "    X,Meta=dtalibs.Getbatchdata(dfinstants)\n",
    "    \n",
    "    dfinstants=pd.DataFrame({'T0':map(lambda x: x[1],Tfs),'TF':map(lambda x: x[2],Tfs),'Symbol':[Symbol]*N})\n",
    "    Y,Meta=dtalibs.Getbatchdata(dfinstants)\n",
    "    \n",
    "    shardpath=dtscmd.Data.objects.get(id=data_id).newshardpath()\n",
    "    np.savez_compressed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
