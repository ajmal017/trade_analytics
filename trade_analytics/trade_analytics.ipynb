{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/cbook.py:136: MatplotlibDeprecationWarning: The finance module has been deprecated in mpl 2.0 and will be removed in mpl 2.2. Please use the module mpl_finance instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n",
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1401: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n",
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter, WeekdayLocator,\\\n",
    "    DayLocator, MONDAY,date2num,num2date,AutoDateLocator\n",
    "from matplotlib.finance import quotes_historical_yahoo_ohlc, candlestick_ohlc,candlestick2_ochl,volume_overlay3\n",
    "\n",
    "from stockapp import models as stkmd\n",
    "from dataapp import models as dtamd\n",
    "from dataapp import tasks as dtatks\n",
    "from dataapp import libs as dtalibs\n",
    "from featureapp import libs as ftlibs\n",
    "from featureapp import models as ftmd\n",
    "from stockapp import tasks as stktks\n",
    "from stockapp import libs as stklibs\n",
    "import featureapp.models as ftmd\n",
    "import featureapp.tasks as fttks\n",
    "import queryapp.models as qrymd\n",
    "import queryapp.tasks as qrytks\n",
    "\n",
    "import charts.chartservers.libs as chservlibs\n",
    "import charts.libs as chlibs\n",
    "\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "\n",
    "\n",
    "import featureapp as ftapp\n",
    "import utility as uty\n",
    "from utility import models as utymd\n",
    "import itertools as itt\n",
    "import multiprocessing as mp\n",
    "from django.db import connection,connections\n",
    "from django.db import reset_queries\n",
    "import time \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import inspect\n",
    "import imp\n",
    "import datetime\n",
    "from talib.abstract import *\n",
    "import utility.models as utmd\n",
    "import stockapp.libs as stklib\n",
    "from utility import codemanager as cdmng\n",
    "from utility import maintenance as mnt\n",
    "import os \n",
    "import json\n",
    "from django.contrib.auth.models import AnonymousUser\n",
    "import threading\n",
    "\n",
    "stk=stkmd.Stockmeta.objects.get(Symbol='TSLA')\n",
    "Fromdate=pd.datetime(2008,1,1)\n",
    "Todate=pd.datetime.today()\n",
    "Trange=pd.date_range(Fromdate,Todate)\n",
    "Trange=[T.date() for T in Trange if T.weekday()<=4]\n",
    "\n",
    "import json\n",
    "# fttks.computefeatuers(stk.id,Trange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entries=[\n",
    "    {'Symbol':'TSLA','TF':pd.datetime(2012,1,1).date(),'T0':pd.datetime(2011,1,1).date()  },\n",
    "    {'Symbol':'AAPL','TF':pd.datetime(2012,1,1).date(),'T0':pd.datetime(2011,1,1).date()  }\n",
    "]\n",
    "chservlibs.request_db_charts(entries,5003)\n",
    "# img=chlibs.CurrentByFutureChart_bydb(entries[0]['T0'],entries[0]['TF'],entries[0]['Symbol'],indicatorlist=(),pricecols=(),querycols=(),featcols=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Running Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import featureapp.models as ftmd\n",
    "import dataapp.datamanager as dtamng\n",
    "import stockapp.models as stkmd\n",
    "import pandas as pd\n",
    "symbolid=stkmd.Stockmeta.objects.get(Symbol='AAPL')\n",
    "DM=dtamng.DataManager()\n",
    "TT=DM.getTradingdates()\n",
    "Trange=filter(lambda x: x>pd.datetime(2010,1,1).date(),TT)\n",
    "\n",
    "featurecodes=ftmd.FeatureComputeCode.objects.all()\n",
    "computecode=featurecodes[0]\n",
    "computeclass=computecode.importcomputeclass()\n",
    "# CC=computeclass(symbolid.id,Trange)\n",
    "# CC.computeonly(featurelist=['SMA20','SMAstd20','FutPROFIT10days','FutLOSS10days','FutPROFIT30days','FutLOSS30days','FutPROFIT90days','FutLOSS90days','CCI5','CCI50'])\n",
    "# CC.saveall()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataapp.datamanager as dtamng\n",
    "import stockapp.models as stkmd\n",
    "import featureapp.tasks as fttks\n",
    "import pandas as pd\n",
    "DM=dtamng.DataManager()\n",
    "TT=DM.getTradingdates()\n",
    "Trange=filter(lambda x: x>pd.datetime(2010,1,1).date(),TT)\n",
    "symbolids=stkmd.Stockmeta.objects.all().values_list('id',flat=True)\n",
    "for stkid in symbolids:\n",
    "    fttks.computefeatuers.delay(stkid,Trange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMING : GetStockData time = 0.442144 on 2017-09-10 12:42:34 with args: ((<StockMetaQuerySet [1431, 1165, 1830]>,),{'standardize': True, 'Todate': datetime.date(2017, 9, 10), 'addcols': None, 'Fromdate': datetime.da) \n"
     ]
    }
   ],
   "source": [
    "import dataapp.datamanager as dtamng\n",
    "DM=dtamng.DataManager()\n",
    "DM.setTradingdates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1849"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TT=DM.getTradingdates()\n",
    "Trange=filter(lambda x: x>pd.datetime(2010,1,1).date(),TT)\n",
    "len(Trange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sync  AnonymousUser,  ... , analytics/featureapp/FeatureCodes/AnonymousUser.py  to db\n"
     ]
    }
   ],
   "source": [
    "import featureapp.models as ftmd\n",
    "ftmd.FeaturesData.objects.all().count()\n",
    "ftmd.FeatureComputeCode.initialize()\n",
    "ftmd.FeatureComputeCode.Sync_files2db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "featureapp.FeatureCodes.AnonymousUser\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__builtins__',\n",
       " '__file__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " 'division',\n",
       " 'featuremodel',\n",
       " 'features',\n",
       " 'filename',\n",
       " 'json',\n",
       " 'np',\n",
       " 'pd',\n",
       " 'registerfeature']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FC=ftmd.FeatureComputeCode.objects.all()[0]\n",
    "print FC.Code\n",
    "import importlib\n",
    "modpath=FC.getimportpath()\n",
    "print modpath\n",
    "module=importlib.import_module(modpath) \n",
    "module=reload(module)\n",
    "# compclass = getattr(module, name)\n",
    "dir(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name features",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c9a62f734479>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfeatureapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeatureCodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAnonymousUser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name features"
     ]
    }
   ],
   "source": [
    "from featureapp.FeatureCodes.AnonymousUser import features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CF=computeclass(stk.id,Trange)\n",
    "CF.computeall(skipdone=True)\n",
    "# CF.saveall()\n",
    "print CF.getfeaturelist()\n",
    "\n",
    "CF.df=CF.addindicators(CF.df,[\n",
    "        {'name':'SMAstd','timeperiod':20,'colname':'SMAstd20'},\n",
    "        {'name':'EMAstd','timeperiod':8,'colname':'EMAstd8'},\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "querycodes=qrymd.QueryComputeCode.objects.all()\n",
    "computecode=querycodes[0]\n",
    "computeclass=computecode.importcomputeclass()\n",
    "CQ=computeclass(stk.id,Trange)\n",
    "CQ.computeall(skipdone=True)\n",
    "CQ.saveall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CQ.OutcomeCharts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CQ.getquerylist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CQ.chartfeatures(addpricecols=(),ip=5562,\n",
    "addfeatcols=[\n",
    "    ['CCI5','CCI50'],\n",
    "    ['PastPROFIT10days','PastLOSS10days'],['FutPROFIT10days','FutLOSS10days']\n",
    "],\n",
    "addquerycols=[\n",
    "    'CCICHERRIES',\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datascience import libs as dtsclibs\n",
    "import pandas as pd\n",
    "\n",
    "@dtsclibs.register_compfunc(RequiredImports=['import pandas as pd','from dataapp import libs as dtalibs'],overwrite_if_exists=False)\n",
    "def extractdataset(data_id,Symbol):\n",
    "    \"\"\"\n",
    "    @funcName : test\n",
    "    @input x : an int\n",
    "    @output df : pd.DataFrame, some random 2 by 2 \n",
    "    @description : takes an int and then returns a dummy dataframe. This is just for testing purposes\n",
    "    @Source : \n",
    "    def extractdataset(data_id,Symbol):\n",
    "        window=60\n",
    "        window_fut=30\n",
    "        Tfs=map(lambda x: ( (x.date()-pd.Dateoffset(window)).date(),x.date(), (x.date()+pd.Dateoffset(window_fut)).date() ),\n",
    "                pd.date_range(start=pd.datetime(2010,1,1),end=pd.datetime.today(),freq='W-MON') )\n",
    "\n",
    "        N=len(Tfs)\n",
    "        dfinstants=pd.DataFrame({'T0':map(lambda x: x[0],Tfs),'TF':map(lambda x: x[1],Tfs),'Symbol':[Symbol]*N})\n",
    "        X,X_Meta=dtalibs.Getbatchdata(dfinstants)\n",
    "\n",
    "        dfinstants=pd.DataFrame({'T0':map(lambda x: x[1],Tfs),'TF':map(lambda x: x[2],Tfs),'Symbol':[Symbol]*N})\n",
    "        Y,Y_Meta=dtalibs.Getbatchdata(dfinstants)\n",
    "\n",
    "        shard=dtscmd.DataShard(Data__id=data_id)\n",
    "        shard.Info['X_Meta']=X_Meta\n",
    "        shard.Info['Y_Meta']=Y_Meta\n",
    "        shard.save()\n",
    "\n",
    "        np.savez_compressed(shard.shardpath(),X=X,Y=Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    window=60\n",
    "    window_fut=30\n",
    "    Tfs=map(lambda x: ( (x.date()-pd.Dateoffset(window)).date(),x.date(), (x.date()+pd.Dateoffset(window_fut)).date() ),\n",
    "            pd.date_range(start=pd.datetime(2010,1,1),end=pd.datetime.today(),freq='W-MON') )\n",
    "    \n",
    "    N=len(Tfs)\n",
    "    dfinstants=pd.DataFrame({'T0':map(lambda x: x[0],Tfs),'TF':map(lambda x: x[1],Tfs),'Symbol':[Symbol]*N})\n",
    "    X,X_Meta=dtalibs.Getbatchdata(dfinstants)\n",
    "    \n",
    "    dfinstants=pd.DataFrame({'T0':map(lambda x: x[1],Tfs),'TF':map(lambda x: x[2],Tfs),'Symbol':[Symbol]*N})\n",
    "    Y,Y_Meta=dtalibs.Getbatchdata(dfinstants)\n",
    "    \n",
    "    shard=dtscmd.DataShard(Data__id=data_id)\n",
    "    shard.Info['X_Meta']=X_Meta\n",
    "    shard.Info['Y_Meta']=Y_Meta\n",
    "    shard.save()\n",
    "    \n",
    "    np.savez_compressed(shard.shardpath(),X=X,Y=Y)\n",
    "\n",
    "extractdataset.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating initial Stock price dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "projid,dataid=dtsclibs.register_dataset(project_Name=\"PredictReturn\",project_Info={'description': \"Data taken on every Monday. 360 days back and 60 days forward\"},\n",
    "                                        Datatype='RawProcessed',GroupName=\"AllStocks\",tag=\"1\",\n",
    "                                        data_format='npz',Modeltype='Regression',\n",
    "                                        TransformedFromDataId=None,TransFuncId=None, use_project_ifexists=True)\n",
    "\n",
    "dtsctks.CreateStockData_2(360,60,dataid,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print dtscmd.Data.objects.filter(Project__id=7,GroupName=\"AllStocks\",tag=\"1\",Datatype='RawProcessed',\n",
    "                           Dataformat='npz',Modeltype='Regression')\n",
    "data = dtscmd.Data.objects.get(Project__id=7,GroupName=\"AllStocks\",tag=\"1\",Datatype='RawProcessed',\n",
    "                           Dataformat='npz',Modeltype='Regression')\n",
    "\n",
    "data.datapath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtscmd.DataShard.objects.filter(Data=data).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtscmd.DataShard.objects.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions on shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over writing previous function\n",
      "saving function :  CleanData\n",
      "function id =  20\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "@dtsclibs.register_compfunc(Group='Cleanup',overwrite_if_exists=True)\n",
    "def CleanData(shardId):\n",
    "    \"\"\"\n",
    "    @Description: remove shards with many Nan samples. Remove samples with many Nans\n",
    "    @Source\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import datascience.models as dtscmd\n",
    "    \n",
    "    \n",
    "    \n",
    "    shard=dtscmd.DataShard.objects.get(id=shardId)\n",
    "    X,Y,Meta=shard.getdata()\n",
    "    \n",
    "    # remove samples with lot of nans\n",
    "    Nsamples=X.shape[0]\n",
    "    Tsteps=X.shape[1]\n",
    "    Nfeat=X.shape[2]\n",
    "    volumecols=[4,11,12]\n",
    "    colsX=list( Meta['MetaX']['columns'] )\n",
    "    colsY=list( Meta['MetaY']['columns'] )\n",
    "    delsample=[]\n",
    "    for i in range(Nsamples):\n",
    "        dfX=pd.DataFrame(X[i,:,:],columns=colsX)\n",
    "        dfY=pd.DataFrame(X[i,:,:],columns=colsY)\n",
    "        if np.sum(pd.isnull(dfX['Close'].values).astype(float))/len(dfX)>=0.5:\n",
    "            delsample.append(i)\n",
    "    \n",
    "    X=np.delete(X,delsample,axis=0)\n",
    "    Y=np.delete(Y,delsample,axis=0)\n",
    "    \n",
    "    # if not more samples then delete the shard\n",
    "    if X.shape[0]/(Nsamples*1.0) <0.4:\n",
    "        print \"----------------------\"\n",
    "        print str(shardId)+\"   \"+ str(X.shape[0]/(Nsamples*1.0))\n",
    "        print \"----------------------\"\n",
    "        print \"delete shaord id = \"+str(shard.id)\n",
    "        shard.delete()\n",
    "    elif X.shape[0]!=Nsamples:\n",
    "        shard.savedata(X=X,Y=Y,Meta=Meta)\n",
    "    else:\n",
    "        pass\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import datascience.models as dtscmd\n",
    "    \n",
    "    \n",
    "    \n",
    "    shard=dtscmd.DataShard.objects.get(id=shardId)\n",
    "    X,Y,Meta=shard.getdata()\n",
    "    \n",
    "    # remove samples with lot of nans\n",
    "    Nsamples=X.shape[0]\n",
    "    Tsteps=X.shape[1]\n",
    "    Nfeat=X.shape[2]\n",
    "    volumecols=[4,11,12]\n",
    "    colsX=list( Meta['MetaX']['columns'] )\n",
    "    colsY=list( Meta['MetaY']['columns'] )\n",
    "    delsample=[]\n",
    "    for i in range(Nsamples):\n",
    "        dfX=pd.DataFrame(X[i,:,:],columns=colsX)\n",
    "        dfY=pd.DataFrame(X[i,:,:],columns=colsY)\n",
    "        if np.sum(pd.isnull(dfX['Close'].values).astype(float))/len(dfX)>=0.5:\n",
    "            delsample.append(i)\n",
    "    \n",
    "    X=np.delete(X,delsample,axis=0)\n",
    "    Y=np.delete(Y,delsample,axis=0)\n",
    "    \n",
    "    # if not more samples then delete the shard\n",
    "    if X.shape[0]/(Nsamples*1.0) <0.4:\n",
    "        print \"----------------------\"\n",
    "        print str(shardId)+\"   \"+ str(X.shape[0]/(Nsamples*1.0))\n",
    "        print \"----------------------\"\n",
    "        print \"delete shaord id = \"+str(shard.id)\n",
    "        shard.delete()\n",
    "    elif X.shape[0]!=Nsamples:\n",
    "        shard.savedata(X=X,Y=Y,Meta=Meta)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "# dtsctks.applyfunc2data(CleanData.id,6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6548"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtscmd.DataShard.objects.filter(Data__id=6).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Creating derived DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over writing previous function\n",
      "saving function :  StandardizeData_1\n",
      "function id =  21\n",
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "saving transfoermer function to this dataset\n",
      "('project id', 'data id')  :  (7, 9)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "\n",
    "@dtsclibs.register_compfunc(Group='Transformer',overwrite_if_exists=True)\n",
    "def StandardizeData_1(X,Y,Meta):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    \"\"\"\n",
    "    A transformer function has to take X,Y,Meta and return another modified X,Y,Meta\n",
    "    1. Normalize all data as \n",
    "        1. Volume to 0-1\n",
    "        2. Prices --> also 0-1\n",
    "    2. For output Y data:\n",
    "        1. Take only Close\n",
    "        2. Pull out best returns in first 5 days, 10 days, 30 days, 60 days, 90 days\n",
    "        3. Pull out worst loss in first 5 days, 10 days, 30 days, 60 days, 90 days\n",
    "    \n",
    "    @Source:\n",
    "    # next normalize the volume to 0-1\n",
    "    Nsamples=X.shape[0]\n",
    "    Tsteps=X.shape[1]\n",
    "    Nfeat=X.shape[2]\n",
    "    volumecols=['Volume','VolSMA10','VolSMA20']\n",
    "    pricecols=['Close','Open','High','Low','SMA10','SMA20','SMA50','SMA100','SMA200','EMA8','EMA20']\n",
    "    colsX=list( Meta['MetaX']['columns'] )\n",
    "    colsY=list( Meta['MetaY']['columns'] )\n",
    "    FinalXcols=pricecols+volumecols\n",
    "    FinalYcols=['FutProfit5days','FutProfit10days','FutProfit30days','FutProfit60days','FutProfit90days']+['FutLoss5days',\n",
    "                                    'FutLoss10days','FutLoss30days','FutLoss60days','FutLoss90days']\n",
    "    \n",
    "    Xn=None\n",
    "    Yn=None\n",
    "    Metan=None\n",
    "    for i in range(Nsamples):\n",
    "        dfX=pd.DataFrame(X[i,:,:],columns=colsX)\n",
    "        dfY=pd.DataFrame(Y[i,:,:],columns=colsY)\n",
    "        \n",
    "        \n",
    "        # clean up Y\n",
    "        dfY.drop('Symbol',axis=1,inplace=True)\n",
    "        \n",
    "        dfY.drop(volumecols,axis=1,inplace=True)\n",
    "        dfY.drop([cc for cc in pricecols if cc!='Close'],axis=1,inplace=True)\n",
    "        \n",
    "        Ydict={}\n",
    "        dfY['ZeroPerf']=0\n",
    "    \n",
    "#         dfY['FutProfit5days']=-100*self.df['Close'].diff(periods=-5)/self.df['Close']\n",
    "        dfY['Returns']=100*(dfY['Close']-dfY['Close'].iloc[0])/dfY['Close'].iloc[0]\n",
    "        \n",
    "        Ydict['FutProfit5days']=dfY[['Returns','ZeroPerf']].iloc[0:5].max(axis=1).round().max()\n",
    "        Ydict['FutProfit10days']=dfY[['Returns','ZeroPerf']].iloc[0:10].max(axis=1).round().max()\n",
    "        Ydict['FutProfit30days']=dfY[['Returns','ZeroPerf']].iloc[0:30].max(axis=1).round().max()\n",
    "        Ydict['FutProfit60days']=dfY[['Returns','ZeroPerf']].iloc[0:60].max(axis=1).round().max()\n",
    "        Ydict['FutProfit90days']=dfY[['Returns','ZeroPerf']].iloc[0:90].max(axis=1).round().max()\n",
    "        \n",
    "        Ydict['FutLoss5days']=dfY[['Returns','ZeroPerf']].iloc[0:5].min(axis=1).round().min()\n",
    "        Ydict['FutLoss10days']=dfY[['Returns','ZeroPerf']].iloc[0:10].min(axis=1).round().min()\n",
    "        Ydict['FutLoss30days']=dfY[['Returns','ZeroPerf']].iloc[0:30].min(axis=1).round().min()\n",
    "        Ydict['FutLoss60days']=dfY[['Returns','ZeroPerf']].iloc[0:60].min(axis=1).round().min()\n",
    "        Ydict['FutLoss90days']=dfY[['Returns','ZeroPerf']].iloc[0:90].min(axis=1).round().min()\n",
    "        \n",
    "        # clean up X\n",
    "        dfX.drop('Symbol',axis=1,inplace=True)\n",
    "        \n",
    "        mxvol=dfX['Volume'].max()\n",
    "        dfX['Volume']=dfX['Volume']/mxvol\n",
    "        dfX['VolSMA10']=dfX['VolSMA10']/mxvol\n",
    "        dfX['VolSMA20']=dfX['VolSMA20']/mxvol\n",
    "        \n",
    "        mxHigh=dfX['High'].max()\n",
    "        mnLow=dfX['Low'].min()\n",
    "        for cc in pricecols:\n",
    "            dfX[cc]=(dfX[cc]-mnLow)/mxHigh\n",
    "        \n",
    "        XX=np.expand_dims( dfX[FinalXcols].astype(float).values   ,axis=0     )\n",
    "        YY=np.expand_dims( np.array([int(Ydict[key]) for key in FinalYcols]),axis=0 )\n",
    "        if Xn is None:\n",
    "            Xn=XX\n",
    "            Yn=YY\n",
    "        else:\n",
    "            Xn=np.vstack((Xn,XX))\n",
    "            Yn=np.vstack((Yn,YY))\n",
    "    \n",
    "    Metan=Meta\n",
    "    Metan['MetaX']['columns']=FinalXcols\n",
    "    Metan['MetaY']['columns']=FinalYcols\n",
    "    \n",
    "    return Xn,Yn,Metan\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # next normalize the volume to 0-1\n",
    "    Nsamples=X.shape[0]\n",
    "    Tsteps=X.shape[1]\n",
    "    Nfeat=X.shape[2]\n",
    "    volumecols=['Volume','VolSMA10','VolSMA20']\n",
    "    pricecols=['Close','Open','High','Low','SMA10','SMA20','SMA50','SMA100','SMA200','EMA8','EMA20']\n",
    "    colsX=list( Meta['MetaX']['columns'] )\n",
    "    colsY=list( Meta['MetaY']['columns'] )\n",
    "    FinalXcols=pricecols+volumecols\n",
    "    FinalYcols=['FutProfit5days','FutProfit10days','FutProfit30days','FutProfit60days','FutProfit90days']+['FutLoss5days',\n",
    "                                    'FutLoss10days','FutLoss30days','FutLoss60days','FutLoss90days']\n",
    "    \n",
    "    Xn=None\n",
    "    Yn=None\n",
    "    Metan=None\n",
    "    for i in range(Nsamples):\n",
    "        dfX=pd.DataFrame(X[i,:,:],columns=colsX)\n",
    "        dfY=pd.DataFrame(Y[i,:,:],columns=colsY)\n",
    "        \n",
    "        \n",
    "        # clean up Y\n",
    "        dfY.drop('Symbol',axis=1,inplace=True)\n",
    "        \n",
    "        dfY.drop(volumecols,axis=1,inplace=True)\n",
    "        dfY.drop([cc for cc in pricecols if cc!='Close'],axis=1,inplace=True)\n",
    "        \n",
    "        Ydict={}\n",
    "        dfY['ZeroPerf']=0\n",
    "    \n",
    "#         dfY['FutProfit5days']=-100*self.df['Close'].diff(periods=-5)/self.df['Close']\n",
    "        dfY['Returns']=100*(dfY['Close']-dfY['Close'].iloc[0])/dfY['Close'].iloc[0]\n",
    "        \n",
    "        Ydict['FutProfit5days']=dfY[['Returns','ZeroPerf']].iloc[0:5].max(axis=1).round().max()\n",
    "        Ydict['FutProfit10days']=dfY[['Returns','ZeroPerf']].iloc[0:10].max(axis=1).round().max()\n",
    "        Ydict['FutProfit30days']=dfY[['Returns','ZeroPerf']].iloc[0:30].max(axis=1).round().max()\n",
    "        Ydict['FutProfit60days']=dfY[['Returns','ZeroPerf']].iloc[0:60].max(axis=1).round().max()\n",
    "        Ydict['FutProfit90days']=dfY[['Returns','ZeroPerf']].iloc[0:90].max(axis=1).round().max()\n",
    "        \n",
    "        Ydict['FutLoss5days']=dfY[['Returns','ZeroPerf']].iloc[0:5].min(axis=1).round().min()\n",
    "        Ydict['FutLoss10days']=dfY[['Returns','ZeroPerf']].iloc[0:10].min(axis=1).round().min()\n",
    "        Ydict['FutLoss30days']=dfY[['Returns','ZeroPerf']].iloc[0:30].min(axis=1).round().min()\n",
    "        Ydict['FutLoss60days']=dfY[['Returns','ZeroPerf']].iloc[0:60].min(axis=1).round().min()\n",
    "        Ydict['FutLoss90days']=dfY[['Returns','ZeroPerf']].iloc[0:90].min(axis=1).round().min()\n",
    "        \n",
    "        # clean up X\n",
    "        dfX.drop('Symbol',axis=1,inplace=True)\n",
    "        \n",
    "        mxvol=dfX['Volume'].max()\n",
    "        dfX['Volume']=dfX['Volume']/mxvol\n",
    "        dfX['VolSMA10']=dfX['VolSMA10']/mxvol\n",
    "        dfX['VolSMA20']=dfX['VolSMA20']/mxvol\n",
    "        \n",
    "        mxHigh=dfX['High'].max()\n",
    "        mnLow=dfX['Low'].min()\n",
    "        for cc in pricecols:\n",
    "            dfX[cc]=(dfX[cc]-mnLow)/mxHigh\n",
    "        \n",
    "        XX=np.expand_dims( dfX[FinalXcols].astype(float).values   ,axis=0     )\n",
    "        YY=np.expand_dims( np.array([int(Ydict[key]) for key in FinalYcols]),axis=0 )\n",
    "        if Xn is None:\n",
    "            Xn=XX\n",
    "            Yn=YY\n",
    "        else:\n",
    "            Xn=np.vstack((Xn,XX))\n",
    "            Yn=np.vstack((Yn,YY))\n",
    "    \n",
    "    Metan=Meta\n",
    "    Metan['MetaX']['columns']=FinalXcols\n",
    "    Metan['MetaY']['columns']=FinalYcols\n",
    "    \n",
    "    return Xn,Yn,Metan\n",
    "\n",
    "\n",
    "projectid,dataid=dtsclibs.register_dataset(tag='NormalizedDerivedFromId6',TransformedFromDataId=6,TransFuncId=StandardizeData_1.id )\n",
    "\n",
    "dtsctks.Perform_TransformData(dataid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over writing previous function\n",
      "saving function :  DataShardMeta_1\n",
      "function id =  29\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_1\n",
      "function id =  21\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_CloseReturnVolume01\n",
      "function id =  22\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_Close01Volume01\n",
      "function id =  23\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_CloseSMAVolSMA10\n",
      "function id =  24\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_Close01Volume01_X30_Y5prfbylss_interpcleaned\n",
      "function id =  27\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_Close01Volume01_X30_Y5prfbylss_interpcleaned_flatout\n",
      "function id =  28\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_HLmeanVolumeSMA01_X30_Y5return_interpcleaned_flatout\n",
      "function id =  30\n",
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "updating data info\n",
      "Deleting the existing shards for this data\n",
      "saving transfoermer function to this dataset\n",
      "('project id', 'data id')  :  (7, 58)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "import datascience.DataCodes.AnonymousUser as stktrfrmr\n",
    "\n",
    "func=stktrfrmr.StandardizeData_HLmeanVolumeSMA01_X30_Y5return_interpcleaned_flatout\n",
    "funcid=func.id\n",
    "DataInfo={'description':func.__doc__}\n",
    "projectid,dataid=dtsclibs.register_dataset(GroupName='AllStocksCloseVol',DataInfo=DataInfo,tag='NormalizedDerivedX23_Y5_HLSMAVolSMA_FromId6',TransformedFromDataId=6,TransFuncId=funcid ,DeleteShards=True )\n",
    "\n",
    "dtsctks.Perform_TransformData(dataid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtscmd.DataShard.objects.filter(Data__id=58).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(301, 184)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "updating data info\n",
      "Deleting the existing shards for this data\n",
      "saving transfoermer function to this dataset\n",
      "('project id', 'data id')  :  (7, 42)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "import datascience.DataCodes.AnonymousUser as stktrfrmr\n",
    "\n",
    "funcid=stktrfrmr.StandardizeData_Close01Volume01_X30_Y5prfbylss_interpcleaned_flatout.id\n",
    "DataInfo={'description':'FLATTENED!!!  All stocks data, close price is made into 0-1, Volume is made into 0-1, Y is profit/(profit+loss) for next 5 days'}\n",
    "projectid,dataid=dtsclibs.register_dataset(GroupName='AllStocksCloseVol_FLATTENED',DataInfo=DataInfo,tag='NormalizedDerived30days_5daysFromId6',TransformedFromDataId=6,TransFuncId=funcid,DeleteShards=True )\n",
    "\n",
    "dtsctks.Perform_TransformData(dataid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "\n",
    "data=dtscmd.Data.objects.get(id=43)\n",
    "X,Y,Meta=data.getdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1546208, 60)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Shard Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over writing previous function\n",
      "saving function :  DataShardMeta_1\n",
      "function id =  29\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_1\n",
      "function id =  21\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_CloseReturnVolume01\n",
      "function id =  22\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_Close01Volume01\n",
      "function id =  23\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_CloseSMAVolSMA10\n",
      "function id =  24\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_Close01Volume01_X30_Y5prfbylss_interpcleaned\n",
      "function id =  27\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_Close01Volume01_X30_Y5prfbylss_interpcleaned_flatout\n",
      "function id =  28\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "import datascience.DataCodes.AnonymousUser as stktrfrmr\n",
    "\n",
    "funcId=stktrfrmr.DataShardMeta_1.id\n",
    "dataId=42\n",
    "M=dtsctks.applyfunc2data(funcId,dataId,wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M.values()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 2315765, 2315765]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x,y:[x[0]+y[0],x[1]+y[1],x[2]+y[2][0],x[3]+y[3][0]],M.values(),[0,0,0,0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Overview of Projects and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over writing previous function\n",
      "saving function :  StandardizeData_1\n",
      "function id =  21\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_CloseReturnVolume01\n",
      "function id =  22\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_Close01Volume01\n",
      "function id =  23\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_CloseSMAVolSMA10\n",
      "function id =  24\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_Close01Volume01_X30_Y5\n",
      "function id =  25\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_Close01Volume01_X30_Y5_flatout\n",
      "function id =  26\n",
      "------------ Data ----------------------\n",
      "Project  ::  ProjectName= PredictReturn  ProjectId 7\n",
      "\tid= 6  ParentId= None\n",
      "\tGroupName= AllStocks  tag= 1\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= RawProcessed  Dataformat= npz\n",
      "\t#shards= 6548\n",
      "\t ------------------\n",
      "\tid= 42  ParentId= 6\n",
      "\tGroupName= AllStocksCloseVol_FLATTENED  tag= NormalizedDerived30days_5daysFromId6\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= RawProcessed  Dataformat= npz\n",
      "\t#shards= 6540\n",
      "\t ------------------\n",
      "\tid= 43  ParentId= 42\n",
      "\tGroupName= AllStocksCloseVol_FLATTENED  tag= NormalizedDerived30days_5daysFromId6_train_0\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Train  Dataformat= npz\n",
      "\t#shards= 6540\n",
      "\t ------------------\n",
      "\tid= 44  ParentId= 42\n",
      "\tGroupName= AllStocksCloseVol_FLATTENED  tag= NormalizedDerived30days_5daysFromId6_valid_0\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Validation  Dataformat= npz\n",
      "\t#shards= 6540\n",
      "\t ------------------\n",
      "\tid= 45  ParentId= 42\n",
      "\tGroupName= AllStocksCloseVol_FLATTENED  tag= NormalizedDerived30days_5daysFromId6_train_1\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Train  Dataformat= npz\n",
      "\t#shards= 6540\n",
      "\t ------------------\n",
      "\tid= 46  ParentId= 42\n",
      "\tGroupName= AllStocksCloseVol_FLATTENED  tag= NormalizedDerived30days_5daysFromId6_valid_1\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Validation  Dataformat= npz\n",
      "\t#shards= 6540\n",
      "\t ------------------\n",
      "\tid= 47  ParentId= 42\n",
      "\tGroupName= AllStocksCloseVol_FLATTENED  tag= NormalizedDerived30days_5daysFromId6_train_2\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Train  Dataformat= npz\n",
      "\t#shards= 6540\n",
      "\t ------------------\n",
      "\tid= 48  ParentId= 42\n",
      "\tGroupName= AllStocksCloseVol_FLATTENED  tag= NormalizedDerived30days_5daysFromId6_valid_2\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Validation  Dataformat= npz\n",
      "\t#shards= 6540\n",
      "\t ------------------\n",
      "\tid= 49  ParentId= 42\n",
      "\tGroupName= AllStocksCloseVol_FLATTENED  tag= NormalizedDerived30days_5daysFromId6_train_3\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Train  Dataformat= npz\n",
      "\t#shards= 6540\n",
      "\t ------------------\n",
      "\tid= 50  ParentId= 42\n",
      "\tGroupName= AllStocksCloseVol_FLATTENED  tag= NormalizedDerived30days_5daysFromId6_valid_3\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Validation  Dataformat= npz\n",
      "\t#shards= 6540\n",
      "\t ------------------\n",
      "\tid= 51  ParentId= 42\n",
      "\tGroupName= AllStocksCloseVol_FLATTENED  tag= NormalizedDerived30days_5daysFromId6_train_4\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Train  Dataformat= npz\n",
      "\t#shards= 6540\n",
      "\t ------------------\n",
      "\tid= 52  ParentId= 42\n",
      "\tGroupName= AllStocksCloseVol_FLATTENED  tag= NormalizedDerived30days_5daysFromId6_valid_4\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Validation  Dataformat= npz\n",
      "\t#shards= 6540\n",
      "\t ------------------\n",
      "\tid= 53  ParentId= 42\n",
      "\tGroupName= AllStocksCloseVol_FLATTENED  tag= NormalizedDerived30days_5daysFromId6_valid_5\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Validation  Dataformat= npz\n",
      "\t#shards= 6540\n",
      "\t ------------------\n",
      "\tid= 54  ParentId= 42\n",
      "\tGroupName= AllStocksCloseVol_FLATTENED  tag= NormalizedDerived30days_5daysFromId6_valid_6\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Validation  Dataformat= npz\n",
      "\t#shards= 6540\n",
      "\t ------------------\n",
      "\tid= 55  ParentId= 42\n",
      "\tGroupName= AllStocksCloseVol_FLATTENED  tag= NormalizedDerived30days_5daysFromId6_valid_7\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Validation  Dataformat= npz\n",
      "\t#shards= 6540\n",
      "\t ------------------\n",
      "\tid= 56  ParentId= 42\n",
      "\tGroupName= AllStocksCloseVol_FLATTENED  tag= NormalizedDerived30days_5daysFromId6_valid_8\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Validation  Dataformat= npz\n",
      "\t#shards= 6540\n",
      "\t ------------------\n",
      "\tid= 57  ParentId= 42\n",
      "\tGroupName= AllStocksCloseVol_FLATTENED  tag= NormalizedDerived30days_5daysFromId6_valid_9\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Validation  Dataformat= npz\n",
      "\t#shards= 6540\n",
      "\t ------------------\n",
      "\tid= 58  ParentId= 6\n",
      "\tGroupName= AllStocksCloseVol  tag= NormalizedDerivedX23_Y5_HLSMAVolSMA_FromId6\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= RawProcessed  Dataformat= npz\n",
      "\t#shards= 6511\n",
      "\t ------------------\n",
      "\tid= 59  ParentId= 58\n",
      "\tGroupName= AllStocksCloseVol  tag= NormalizedDerivedX23_Y5_HLSMAVolSMA_FromId6_train_0\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Train  Dataformat= npz\n",
      "\t#shards= 6508\n",
      "\t ------------------\n",
      "\tid= 60  ParentId= 58\n",
      "\tGroupName= AllStocksCloseVol  tag= NormalizedDerivedX23_Y5_HLSMAVolSMA_FromId6_valid_0\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Validation  Dataformat= npz\n",
      "\t#shards= 6508\n",
      "\t ------------------\n",
      "\tid= 61  ParentId= 58\n",
      "\tGroupName= AllStocksCloseVol  tag= NormalizedDerivedX23_Y5_HLSMAVolSMA_FromId6_train_1\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Train  Dataformat= npz\n",
      "\t#shards= 6508\n",
      "\t ------------------\n",
      "\tid= 62  ParentId= 58\n",
      "\tGroupName= AllStocksCloseVol  tag= NormalizedDerivedX23_Y5_HLSMAVolSMA_FromId6_valid_1\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Validation  Dataformat= npz\n",
      "\t#shards= 6508\n",
      "\t ------------------\n",
      "\tid= 63  ParentId= 58\n",
      "\tGroupName= AllStocksCloseVol  tag= NormalizedDerivedX23_Y5_HLSMAVolSMA_FromId6_train_2\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Train  Dataformat= npz\n",
      "\t#shards= 6508\n",
      "\t ------------------\n",
      "\tid= 64  ParentId= 58\n",
      "\tGroupName= AllStocksCloseVol  tag= NormalizedDerivedX23_Y5_HLSMAVolSMA_FromId6_valid_2\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Validation  Dataformat= npz\n",
      "\t#shards= 6508\n",
      "\t ------------------\n",
      "\tid= 65  ParentId= 58\n",
      "\tGroupName= AllStocksCloseVol  tag= NormalizedDerivedX23_Y5_HLSMAVolSMA_FromId6_train_3\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Train  Dataformat= npz\n",
      "\t#shards= 6508\n",
      "\t ------------------\n",
      "\tid= 66  ParentId= 58\n",
      "\tGroupName= AllStocksCloseVol  tag= NormalizedDerivedX23_Y5_HLSMAVolSMA_FromId6_valid_3\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Validation  Dataformat= npz\n",
      "\t#shards= 6508\n",
      "\t ------------------\n",
      "\tid= 67  ParentId= 58\n",
      "\tGroupName= AllStocksCloseVol  tag= NormalizedDerivedX23_Y5_HLSMAVolSMA_FromId6_train_4\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Train  Dataformat= npz\n",
      "\t#shards= 6508\n",
      "\t ------------------\n",
      "\tid= 68  ParentId= 58\n",
      "\tGroupName= AllStocksCloseVol  tag= NormalizedDerivedX23_Y5_HLSMAVolSMA_FromId6_valid_4\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Validation  Dataformat= npz\n",
      "\t#shards= 6508\n",
      "\t ------------------\n",
      "\tid= 69  ParentId= 58\n",
      "\tGroupName= AllStocksCloseVol  tag= NormalizedDerivedX23_Y5_HLSMAVolSMA_FromId6_valid_5\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Validation  Dataformat= npz\n",
      "\t#shards= 6508\n",
      "\t ------------------\n",
      "\tid= 70  ParentId= 58\n",
      "\tGroupName= AllStocksCloseVol  tag= NormalizedDerivedX23_Y5_HLSMAVolSMA_FromId6_valid_6\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Validation  Dataformat= npz\n",
      "\t#shards= 6508\n",
      "\t ------------------\n",
      "\tid= 71  ParentId= 58\n",
      "\tGroupName= AllStocksCloseVol  tag= NormalizedDerivedX23_Y5_HLSMAVolSMA_FromId6_valid_7\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Validation  Dataformat= npz\n",
      "\t#shards= 6508\n",
      "\t ------------------\n",
      "\tid= 72  ParentId= 58\n",
      "\tGroupName= AllStocksCloseVol  tag= NormalizedDerivedX23_Y5_HLSMAVolSMA_FromId6_valid_8\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Validation  Dataformat= npz\n",
      "\t#shards= 6508\n",
      "\t ------------------\n",
      "\tid= 73  ParentId= 58\n",
      "\tGroupName= AllStocksCloseVol  tag= NormalizedDerivedX23_Y5_HLSMAVolSMA_FromId6_valid_9\n",
      "\tModeltype= Regression  DataStructure= Channels\n",
      "\tDatatype= Validation  Dataformat= npz\n",
      "\t#shards= 6508\n",
      "\t ------------------\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "import datascience.stockdata_transformers as stktrfrmr\n",
    "\n",
    "\n",
    "    \n",
    "print \"------------ Data ----------------------\"\n",
    "for proj in dtscmd.Project.objects.all():\n",
    "    print \"Project  :: \",\"ProjectName=\",proj.Name,\" ProjectId\",proj.id\n",
    "    for data in dtscmd.Data.objects.filter(Project=proj).order_by(\"id\"): #\n",
    "        print \"\\t\",\"id=\",data.id,\" ParentId=\",data.ParentData.id if data.ParentData is not None else None \n",
    "        print \"\\t\",\"GroupName=\",data.GroupName,\" tag=\",data.tag\n",
    "        print \"\\t\",\"Modeltype=\",data.Modeltype,\" DataStructure=\",data.DataStructure\n",
    "        print \"\\t\",\"Datatype=\",data.Datatype,\" Dataformat=\",data.Dataformat\n",
    "        print \"\\t\",\"#shards=\",dtscmd.DataShard.objects.filter(Data=data).count()\n",
    "        print \"\\t ------------------\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create Train and Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project  PredictReturn  already exists\n",
      "('project id', 'data id')  :  (7, 59)\n",
      "Project  PredictReturn  already exists\n",
      "('project id', 'data id')  :  (7, 60)\n",
      "Project  PredictReturn  already exists\n",
      "('project id', 'data id')  :  (7, 61)\n",
      "Project  PredictReturn  already exists\n",
      "('project id', 'data id')  :  (7, 62)\n",
      "Project  PredictReturn  already exists\n",
      "('project id', 'data id')  :  (7, 63)\n",
      "Project  PredictReturn  already exists\n",
      "('project id', 'data id')  :  (7, 64)\n",
      "Project  PredictReturn  already exists\n",
      "('project id', 'data id')  :  (7, 65)\n",
      "Project  PredictReturn  already exists\n",
      "('project id', 'data id')  :  (7, 66)\n",
      "Project  PredictReturn  already exists\n",
      "('project id', 'data id')  :  (7, 67)\n",
      "Project  PredictReturn  already exists\n",
      "('project id', 'data id')  :  (7, 68)\n",
      "Project  PredictReturn  already exists\n",
      "('project id', 'data id')  :  (7, 69)\n",
      "Project  PredictReturn  already exists\n",
      "('project id', 'data id')  :  (7, 70)\n",
      "Project  PredictReturn  already exists\n",
      "('project id', 'data id')  :  (7, 71)\n",
      "Project  PredictReturn  already exists\n",
      "('project id', 'data id')  :  (7, 72)\n",
      "Project  PredictReturn  already exists\n",
      "('project id', 'data id')  :  (7, 73)\n"
     ]
    }
   ],
   "source": [
    "import datascience.ML.MLlibs as MLlibs\n",
    "MLlibs.get_train_test_from_RawProcessed(58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generate Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "dtscmd.MLmodels.objects.all().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "import datascience.ModelCodes.AnonymousUser as MLmod\n",
    "\n",
    "proj=dtscmd.Project.objects.get(id=7)\n",
    "traindata=dtscmd.Data.objects.get(id=59)\n",
    "M=MLmod.RandomForrrest_2()\n",
    "M.GenModels(proj,traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "import datascience.ModelCodes.AnonymousUser as MLmod\n",
    "\n",
    "proj=dtscmd.Project.objects.get(id=7)\n",
    "traindata=dtscmd.Data.objects.get(id=59)\n",
    "M=MLmod.SVC_1()\n",
    "M.GenModels(proj,traindata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "import datascience.ModelCodes.AnonymousUser as MLmod\n",
    "\n",
    "proj=dtscmd.Project.objects.get(id=7)\n",
    "traindata=dtscmd.Data.objects.get(id=59)\n",
    "M=MLmod.QDA_1()\n",
    "M.GenModels(proj,traindata)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184 2 50 100 0.02 0 sigmoid 368\n",
      "compile =  0.0696158409119\n",
      "save =  0.0511529445648\n",
      "2\n",
      "184 2 50 100 0.02 0 sigmoid 93\n",
      "compile =  0.170254230499\n",
      "save =  0.031613111496\n",
      "3\n",
      "184 2 50 100 0.02 0 relu 368\n",
      "compile =  0.0393362045288\n",
      "save =  0.0142769813538\n",
      "4\n",
      "184 2 50 100 0.02 0 relu 93\n",
      "compile =  0.0411579608917\n",
      "save =  0.0160109996796\n",
      "5\n",
      "184 2 50 100 0.02 0.2 sigmoid 368\n",
      "compile =  0.0467600822449\n",
      "save =  0.023090839386\n",
      "6\n",
      "184 2 50 100 0.02 0.2 sigmoid 93\n",
      "compile =  0.0429379940033\n",
      "save =  0.0286300182343\n",
      "7\n",
      "184 2 50 100 0.02 0.2 relu 368\n",
      "compile =  0.0590920448303\n",
      "save =  0.0323839187622\n",
      "8\n",
      "184 2 50 100 0.02 0.2 relu 93\n",
      "compile =  0.0511960983276\n",
      "save =  0.0345780849457\n",
      "9\n",
      "184 2 50 100 0.2 0 sigmoid 368\n",
      "compile =  0.0406560897827\n",
      "save =  0.0309629440308\n",
      "10\n",
      "184 2 50 100 0.2 0 sigmoid 93\n",
      "compile =  0.0365490913391\n",
      "save =  0.0312111377716\n",
      "11\n",
      "184 2 50 100 0.2 0 relu 368\n",
      "compile =  0.0373060703278\n",
      "save =  0.0314371585846\n",
      "12\n",
      "184 2 50 100 0.2 0 relu 93\n",
      "compile =  0.036966085434\n",
      "save =  0.0317130088806\n",
      "13\n",
      "184 2 50 100 0.2 0.2 sigmoid 368\n",
      "compile =  0.0458059310913\n",
      "save =  0.0390491485596\n",
      "14\n",
      "184 2 50 100 0.2 0.2 sigmoid 93\n",
      "compile =  0.0430128574371\n",
      "save =  0.0410420894623\n",
      "15\n",
      "184 2 50 100 0.2 0.2 relu 368\n",
      "compile =  0.0439691543579\n",
      "save =  0.0455269813538\n",
      "16\n",
      "184 2 50 100 0.2 0.2 relu 93\n",
      "compile =  0.0443270206451\n",
      "save =  0.0480229854584\n",
      "17\n",
      "184 2 50 250 0.02 0 sigmoid 368\n",
      "compile =  0.182718038559\n",
      "save =  0.0504789352417\n",
      "18\n",
      "184 2 50 250 0.02 0 sigmoid 93\n",
      "compile =  0.0371119976044\n",
      "save =  0.047101020813\n",
      "19\n",
      "184 2 50 250 0.02 0 relu 368\n",
      "compile =  0.0364089012146\n",
      "save =  0.0509209632874\n",
      "20\n",
      "184 2 50 250 0.02 0 relu 93\n",
      "compile =  0.043084859848\n",
      "save =  0.0520310401917\n",
      "21\n",
      "184 2 50 250 0.02 0.2 sigmoid 368\n",
      "compile =  0.0464828014374\n",
      "save =  0.0656430721283\n",
      "22\n",
      "184 2 50 250 0.02 0.2 sigmoid 93\n",
      "compile =  0.0471580028534\n",
      "save =  0.0698750019073\n",
      "23\n",
      "184 2 50 250 0.02 0.2 relu 368\n",
      "compile =  0.0474171638489\n",
      "save =  0.0760610103607\n",
      "24\n",
      "184 2 50 250 0.02 0.2 relu 93\n",
      "compile =  0.0433509349823\n",
      "save =  0.077565908432\n",
      "25\n",
      "184 2 50 250 0.2 0 sigmoid 368\n",
      "compile =  0.0385301113129\n",
      "save =  0.075010061264\n",
      "26\n",
      "184 2 50 250 0.2 0 sigmoid 93\n",
      "compile =  0.0398271083832\n",
      "save =  0.0780160427094\n",
      "27\n",
      "184 2 50 250 0.2 0 relu 368\n",
      "compile =  0.0468671321869\n",
      "save =  0.0800490379333\n",
      "28\n",
      "184 2 50 250 0.2 0 relu 93\n",
      "compile =  0.0474669933319\n",
      "save =  0.078850030899\n",
      "29\n",
      "184 2 50 250 0.2 0.2 sigmoid 368\n",
      "compile =  0.0471920967102\n",
      "save =  0.0820438861847\n",
      "30\n",
      "184 2 50 250 0.2 0.2 sigmoid 93\n",
      "compile =  0.0522198677063\n",
      "save =  0.0949950218201\n",
      "31\n",
      "184 2 50 250 0.2 0.2 relu 368\n",
      "compile =  0.0460720062256\n",
      "save =  0.0802898406982\n",
      "32\n",
      "184 2 50 250 0.2 0.2 relu 93\n",
      "compile =  0.209537982941\n",
      "save =  0.0853569507599\n",
      "33\n",
      "184 2 50 500 0.02 0 sigmoid 368\n",
      "compile =  0.039608001709\n",
      "save =  0.0990800857544\n",
      "34\n",
      "184 2 50 500 0.02 0 sigmoid 93\n",
      "compile =  0.0365948677063\n",
      "save =  0.0936100482941\n",
      "35\n",
      "184 2 50 500 0.02 0 relu 368\n",
      "compile =  0.0363609790802\n",
      "save =  0.0964419841766\n",
      "36\n",
      "184 2 50 500 0.02 0 relu 93\n",
      "compile =  0.0420260429382\n",
      "save =  0.0998950004578\n",
      "37\n",
      "184 2 50 500 0.02 0.2 sigmoid 368\n",
      "compile =  0.0439729690552\n",
      "save =  0.103365898132\n",
      "38\n",
      "184 2 50 500 0.02 0.2 sigmoid 93\n",
      "compile =  0.0447289943695\n",
      "save =  0.105880975723\n",
      "39\n",
      "184 2 50 500 0.02 0.2 relu 368\n",
      "compile =  0.0478761196136\n",
      "save =  0.114753007889\n",
      "40\n",
      "184 2 50 500 0.02 0.2 relu 93\n",
      "compile =  0.044261932373\n",
      "save =  0.113847017288\n",
      "41\n",
      "184 2 50 500 0.2 0 sigmoid 368\n",
      "compile =  0.0372359752655\n",
      "save =  0.11399102211\n",
      "42\n",
      "184 2 50 500 0.2 0 sigmoid 93\n",
      "compile =  0.0365879535675\n",
      "save =  0.115118026733\n",
      "43\n",
      "184 2 50 500 0.2 0 relu 368\n",
      "compile =  0.0392401218414\n",
      "save =  0.121211051941\n",
      "44\n",
      "184 2 50 500 0.2 0 relu 93\n",
      "compile =  0.03542304039\n",
      "save =  0.126188993454\n",
      "45\n",
      "184 2 50 500 0.2 0.2 sigmoid 368\n",
      "compile =  0.0447800159454\n",
      "save =  0.124121904373\n",
      "46\n",
      "184 2 50 500 0.2 0.2 sigmoid 93\n",
      "compile =  0.0461740493774\n",
      "save =  0.140013933182\n",
      "47\n",
      "184 2 50 500 0.2 0.2 relu 368\n",
      "compile =  0.0474541187286\n",
      "save =  0.146249055862\n",
      "48\n",
      "184 2 50 500 0.2 0.2 relu 93\n",
      "compile =  0.0536911487579\n",
      "save =  0.161403179169\n",
      "49\n",
      "184 2 50 750 0.02 0 sigmoid 368\n",
      "compile =  0.243506908417\n",
      "save =  0.138721942902\n",
      "50\n",
      "184 2 50 750 0.02 0 sigmoid 93\n",
      "compile =  0.0380480289459\n",
      "save =  0.144685983658\n",
      "51\n",
      "184 2 50 750 0.02 0 relu 368\n",
      "compile =  0.0349910259247\n",
      "save =  0.140532016754\n",
      "52\n",
      "184 2 50 750 0.02 0 relu 93\n",
      "compile =  0.0376479625702\n",
      "save =  0.15193104744\n",
      "53\n",
      "184 2 50 750 0.02 0.2 sigmoid 368\n",
      "compile =  0.0487589836121\n",
      "save =  0.145853996277\n",
      "54\n",
      "184 2 50 750 0.02 0.2 sigmoid 93\n",
      "compile =  0.0426259040833\n",
      "save =  0.149123907089\n",
      "55\n",
      "184 2 50 750 0.02 0.2 relu 368\n",
      "compile =  0.0413548946381\n",
      "save =  0.155567884445\n",
      "56\n",
      "184 2 50 750 0.02 0.2 relu 93\n",
      "compile =  0.0417730808258\n",
      "save =  0.165009975433\n",
      "57\n",
      "184 2 50 750 0.2 0 sigmoid 368\n",
      "compile =  0.0451009273529\n",
      "save =  0.190454959869\n",
      "58\n",
      "184 2 50 750 0.2 0 sigmoid 93\n",
      "compile =  0.0431859493256\n",
      "save =  0.19683599472\n",
      "59\n",
      "184 2 50 750 0.2 0 relu 368\n",
      "compile =  0.0417850017548\n",
      "save =  0.19813990593\n",
      "60\n",
      "184 2 50 750 0.2 0 relu 93\n",
      "compile =  0.0407090187073\n",
      "save =  0.181121826172\n",
      "61\n",
      "184 2 50 750 0.2 0.2 sigmoid 368\n",
      "compile =  0.0473589897156\n",
      "save =  0.197381973267\n",
      "62\n",
      "184 2 50 750 0.2 0.2 sigmoid 93\n",
      "compile =  0.0564141273499\n",
      "save =  0.191389083862\n",
      "63\n",
      "184 2 50 750 0.2 0.2 relu 368\n",
      "compile =  0.048134803772\n",
      "save =  0.199742078781\n",
      "64\n",
      "184 2 50 750 0.2 0.2 relu 93\n",
      "compile =  0.049329996109\n",
      "save =  0.185803174973\n",
      "65\n",
      "184 2 100 100 0.02 0 sigmoid 368\n",
      "compile =  0.0360288619995\n",
      "save =  0.203103065491\n",
      "66\n",
      "184 2 100 100 0.02 0 sigmoid 93\n",
      "compile =  0.0377540588379\n",
      "save =  0.190288066864\n",
      "67\n",
      "184 2 100 100 0.02 0 relu 368\n",
      "compile =  0.0360300540924\n",
      "save =  0.193114995956\n",
      "68\n",
      "184 2 100 100 0.02 0 relu 93\n",
      "compile =  0.0387699604034\n",
      "save =  0.212775945663\n",
      "69\n",
      "184 2 100 100 0.02 0.2 sigmoid 368\n",
      "compile =  0.294466972351\n",
      "save =  0.236806869507\n",
      "70\n",
      "184 2 100 100 0.02 0.2 sigmoid 93\n",
      "compile =  0.0539588928223\n",
      "save =  0.23268699646\n",
      "71\n",
      "184 2 100 100 0.02 0.2 relu 368\n",
      "compile =  0.0439732074738\n",
      "save =  0.232433080673\n",
      "72\n",
      "184 2 100 100 0.02 0.2 relu 93\n",
      "compile =  0.050302028656\n",
      "save =  0.234917879105\n",
      "73\n",
      "184 2 100 100 0.2 0 sigmoid 368\n",
      "compile =  0.0370070934296\n",
      "save =  0.214855909348\n",
      "74\n",
      "184 2 100 100 0.2 0 sigmoid 93\n",
      "compile =  0.0356919765472\n",
      "save =  0.222146987915\n",
      "75\n",
      "184 2 100 100 0.2 0 relu 368\n",
      "compile =  0.0368921756744\n",
      "save =  0.216181993484\n",
      "76\n",
      "184 2 100 100 0.2 0 relu 93\n",
      "compile =  0.0353310108185\n",
      "save =  0.216675043106\n",
      "77\n",
      "184 2 100 100 0.2 0.2 sigmoid 368\n",
      "compile =  0.046324968338\n",
      "save =  0.218425035477\n",
      "78\n",
      "184 2 100 100 0.2 0.2 sigmoid 93\n",
      "compile =  0.0443091392517\n",
      "save =  0.226727962494\n",
      "79\n",
      "184 2 100 100 0.2 0.2 relu 368\n",
      "compile =  0.0436029434204\n",
      "save =  0.24024605751\n",
      "80\n",
      "184 2 100 100 0.2 0.2 relu 93\n",
      "compile =  0.0451819896698\n",
      "save =  0.254920959473\n",
      "81\n",
      "184 2 100 250 0.02 0 sigmoid 368\n",
      "compile =  0.0373558998108\n",
      "save =  0.26598906517\n",
      "82\n",
      "184 2 100 250 0.02 0 sigmoid 93\n",
      "compile =  0.0395770072937\n",
      "save =  0.27904009819\n",
      "83\n",
      "184 2 100 250 0.02 0 relu 368\n",
      "compile =  0.0423829555511\n",
      "save =  0.27112197876\n",
      "84\n",
      "184 2 100 250 0.02 0 relu 93\n",
      "compile =  0.0455541610718\n",
      "save =  0.278342962265\n",
      "85\n",
      "184 2 100 250 0.02 0.2 sigmoid 368\n",
      "compile =  0.0432560443878\n",
      "save =  0.246042966843\n",
      "86\n",
      "184 2 100 250 0.02 0.2 sigmoid 93\n",
      "compile =  0.0504288673401\n",
      "save =  0.275393009186\n",
      "87\n",
      "184 2 100 250 0.02 0.2 relu 368\n",
      "compile =  0.0470480918884\n",
      "save =  0.277110815048\n",
      "88\n",
      "184 2 100 250 0.02 0.2 relu 93\n",
      "compile =  0.0486488342285\n",
      "save =  0.254137039185\n",
      "89\n",
      "184 2 100 250 0.2 0 sigmoid 368\n",
      "compile =  0.0351159572601\n",
      "save =  0.259036064148\n",
      "90\n",
      "184 2 100 250 0.2 0 sigmoid 93\n",
      "compile =  0.0345950126648\n",
      "save =  0.265733003616\n",
      "91\n",
      "184 2 100 250 0.2 0 relu 368\n",
      "compile =  0.0358810424805\n",
      "save =  0.283518075943\n",
      "92\n",
      "184 2 100 250 0.2 0 relu 93\n",
      "compile =  0.0387079715729\n",
      "save =  0.313092947006\n",
      "93\n",
      "184 2 100 250 0.2 0.2 sigmoid 368\n",
      "compile =  0.0509459972382\n",
      "save =  0.318542003632\n",
      "94\n",
      "184 2 100 250 0.2 0.2 sigmoid 93\n",
      "compile =  0.0540990829468\n",
      "save =  0.3280210495\n",
      "95\n",
      "184 2 100 250 0.2 0.2 relu 368\n",
      "compile =  0.337244033813\n",
      "save =  0.29899597168\n",
      "96\n",
      "184 2 100 250 0.2 0.2 relu 93\n",
      "compile =  0.0479350090027\n",
      "save =  0.307784795761\n",
      "97\n",
      "184 2 100 500 0.02 0 sigmoid 368\n",
      "compile =  0.0347781181335\n",
      "save =  0.284952878952\n",
      "98\n",
      "184 2 100 500 0.02 0 sigmoid 93\n",
      "compile =  0.0338449478149\n",
      "save =  0.308459997177\n",
      "99\n",
      "184 2 100 500 0.02 0 relu 368\n",
      "compile =  0.0458970069885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save =  0.329963922501\n",
      "100\n",
      "184 2 100 500 0.02 0 relu 93\n",
      "compile =  0.0404269695282\n",
      "save =  0.334634065628\n",
      "101\n",
      "184 2 100 500 0.02 0.2 sigmoid 368\n",
      "compile =  0.0466890335083\n",
      "save =  0.338186979294\n",
      "102\n",
      "184 2 100 500 0.02 0.2 sigmoid 93\n",
      "compile =  0.0461449623108\n",
      "save =  0.334941864014\n",
      "103\n",
      "184 2 100 500 0.02 0.2 relu 368\n",
      "compile =  0.0430729389191\n",
      "save =  0.306221008301\n",
      "104\n",
      "184 2 100 500 0.02 0.2 relu 93\n",
      "compile =  0.0466709136963\n",
      "save =  0.328294038773\n",
      "105\n",
      "184 2 100 500 0.2 0 sigmoid 368\n",
      "compile =  0.0418920516968\n",
      "save =  0.33665895462\n",
      "106\n",
      "184 2 100 500 0.2 0 sigmoid 93\n",
      "compile =  0.0389869213104\n",
      "save =  0.333751916885\n",
      "107\n",
      "184 2 100 500 0.2 0 relu 368\n",
      "compile =  0.0441961288452\n",
      "save =  0.343854904175\n",
      "108\n",
      "184 2 100 500 0.2 0 relu 93\n",
      "compile =  0.0364170074463\n",
      "save =  0.356847047806\n",
      "109\n",
      "184 2 100 500 0.2 0.2 sigmoid 368\n",
      "compile =  0.0475640296936\n",
      "save =  0.364627122879\n",
      "110\n",
      "184 2 100 500 0.2 0.2 sigmoid 93\n",
      "compile =  0.04833984375\n",
      "save =  0.330704927444\n",
      "111\n",
      "184 2 100 500 0.2 0.2 relu 368\n",
      "compile =  0.0431480407715\n",
      "save =  0.349571943283\n",
      "112\n",
      "184 2 100 500 0.2 0.2 relu 93\n",
      "compile =  0.043967962265\n",
      "save =  0.33064699173\n",
      "113\n",
      "184 2 100 750 0.02 0 sigmoid 368\n",
      "compile =  0.034521818161\n",
      "save =  0.338439941406\n",
      "114\n",
      "184 2 100 750 0.02 0 sigmoid 93\n",
      "compile =  0.0408010482788\n",
      "save =  0.383442878723\n",
      "115\n",
      "184 2 100 750 0.02 0 relu 368\n",
      "compile =  0.044361114502\n",
      "save =  0.400754928589\n",
      "116\n",
      "184 2 100 750 0.02 0 relu 93\n",
      "compile =  0.0492370128632\n",
      "save =  0.384479045868\n",
      "117\n",
      "184 2 100 750 0.02 0.2 sigmoid 368\n",
      "compile =  0.0490601062775\n",
      "save =  0.397446155548\n",
      "118\n",
      "184 2 100 750 0.02 0.2 sigmoid 93\n",
      "compile =  0.0460231304169\n",
      "save =  0.380758047104\n",
      "119\n",
      "184 2 100 750 0.02 0.2 relu 368\n",
      "compile =  0.0446858406067\n",
      "save =  0.358121871948\n",
      "120\n",
      "184 2 100 750 0.02 0.2 relu 93\n",
      "compile =  0.0426330566406\n",
      "save =  0.364431858063\n",
      "121\n",
      "184 2 100 750 0.2 0 sigmoid 368\n",
      "compile =  0.0377089977264\n",
      "save =  0.384919166565\n",
      "122\n",
      "184 2 100 750 0.2 0 sigmoid 93\n",
      "compile =  0.0381109714508\n",
      "save =  0.433292150497\n",
      "123\n",
      "184 2 100 750 0.2 0 relu 368\n",
      "compile =  0.0429430007935\n",
      "save =  0.441876173019\n",
      "124\n",
      "184 2 100 750 0.2 0 relu 93\n",
      "compile =  0.0459640026093\n",
      "save =  0.445610046387\n",
      "125\n",
      "184 2 100 750 0.2 0.2 sigmoid 368\n",
      "compile =  0.0444939136505\n",
      "save =  0.378087043762\n",
      "126\n",
      "184 2 100 750 0.2 0.2 sigmoid 93\n",
      "compile =  0.0455300807953\n",
      "save =  0.382990837097\n",
      "127\n",
      "184 2 100 750 0.2 0.2 relu 368\n",
      "compile =  0.380559921265\n",
      "save =  0.434366226196\n",
      "128\n",
      "184 2 100 750 0.2 0.2 relu 93\n",
      "compile =  0.048663854599\n",
      "save =  0.44392490387\n",
      "129\n",
      "184 2 5000 100 0.02 0 sigmoid 368\n",
      "compile =  0.0428819656372\n",
      "save =  0.39453792572\n",
      "130\n",
      "184 2 5000 100 0.02 0 sigmoid 93\n",
      "compile =  0.0376670360565\n",
      "save =  0.396819829941\n",
      "131\n",
      "184 2 5000 100 0.02 0 relu 368\n",
      "compile =  0.0396499633789\n",
      "save =  0.418253183365\n",
      "132\n",
      "184 2 5000 100 0.02 0 relu 93\n",
      "compile =  0.0396659374237\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "import datascience.ModelCodes.AnonymousUser as MLmod\n",
    "\n",
    "proj=dtscmd.Project.objects.get(id=7)\n",
    "traindata=dtscmd.Data.objects.get(id=59)\n",
    "M=MLmod.NN_1()\n",
    "M.GenModels(proj,traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "dtscmd.MLmodels.objects.all().count()\n",
    "# NN_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "import datascience.ModelCodes.AnonymousUser as MLmod\n",
    "\n",
    "model=dtscmd.MLmodels.objects.get(id=4)\n",
    "M=MLmod.RandomForrrest_1()\n",
    "M.loadmodel(model)\n",
    "M.loaddata()\n",
    "X,Y=M.pre_processing_train()\n",
    "M.train()\n",
    "M.Run_validation_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(490, {u'datascience.MLmodels': 490, u'datascience.ModelMetrics': 0})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datascience import models as dtscmd\n",
    "dtscmd.MLmodels.objects.all().delete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- NN_1 -------------- \n",
      "All       :  192\n",
      "Untrained :  180\n",
      "Trained   :  11\n"
     ]
    }
   ],
   "source": [
    "from datascience import models as dtscmd\n",
    "modelnames=dtscmd.MLmodels.objects.filter(Data__id=59).values_list('Name',flat=True).distinct()\n",
    "for name in modelnames:\n",
    "    print \"-------------- \"+name+\" -------------- \"\n",
    "    print 'All       : ',dtscmd.MLmodels.objects.filter(Data__id=59,Name=name).count()\n",
    "    print 'Untrained : ',dtscmd.MLmodels.objects.filter(Data__id=59,Name=name,Status='UnTrained').count()\n",
    "    print 'Trained   : ',dtscmd.MLmodels.objects.filter(Data__id=59,Name=name,Status='Trained').count()\n",
    "\n",
    "# print model.created_at\n",
    "# print model.updated_at\n",
    "# model.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244, {u'datascience.MLmodels': 244, u'datascience.ModelMetrics': 0})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datascience import models as dtscmd\n",
    "dtscmd.MLmodels.objects.all().delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "\n",
    "SS=dtscmd.ModelCode.objects.all()[0]\n",
    "SS.Username='AnonymousUser'\n",
    "SS.save()\n",
    "\n",
    "# print dtscmd.ModelCode.objects.all()[0].Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "datascience/ML/MLmodels.py:179: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.clf.fit(X,Y)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "\n",
    "modelid=2642\n",
    "model=dtscmd.MLmodels.objects.get(id=modelid)\n",
    "model.Status='UnTrained'\n",
    "model.save()\n",
    "MCode=dtscmd.ModelCode.objects.get(Username=model.Userfilename)\n",
    "Mclass=MCode.importobject(model.Name)\n",
    "M=Mclass()\n",
    "M.loadmodel(model)\n",
    "M.loaddata()\n",
    "M.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.isdir('/home/venkat/GoogleDrive/repos/trade_analytics/trade_analytics/bigdata/datascience/Projects/PredictReturn/Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "\n",
    "dtsctks.TrainProject(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'acc': 0.5873301132989499,\n",
       "  u'avgprec': 0.7500245475466635,\n",
       "  u'logloss': 14.253347959156393,\n",
       "  u'precisionscore': 0.5873301132989499,\n",
       "  u'recallscore': 0.5873301132989499},\n",
       " {u'acc': 0.5778389385061795,\n",
       "  u'avgprec': 0.7391744823896076,\n",
       "  u'logloss': 14.58117097308135,\n",
       "  u'precisionscore': 0.5778389385061795,\n",
       "  u'recallscore': 0.5778389385061795},\n",
       " {u'acc': 0.5755662023735734,\n",
       "  u'avgprec': 0.7360772170628063,\n",
       "  u'logloss': 14.659673633086008,\n",
       "  u'precisionscore': 0.5755662023735734,\n",
       "  u'recallscore': 0.5755662023735734},\n",
       " {u'acc': 0.5804014517443152,\n",
       "  u'avgprec': 0.7419373765432435,\n",
       "  u'logloss': 14.492664477413697,\n",
       "  u'precisionscore': 0.5804014517443152,\n",
       "  u'recallscore': 0.5804014517443152},\n",
       " {u'acc': 0.5781638007321095,\n",
       "  u'avgprec': 0.7417555815507899,\n",
       "  u'logloss': 14.569947829098775,\n",
       "  u'precisionscore': 0.5781638007321095,\n",
       "  u'recallscore': 0.5781638007321095},\n",
       " {u'acc': 0.5707517441852911,\n",
       "  u'avgprec': 0.7286107886195881,\n",
       "  u'logloss': 14.825961350411745,\n",
       "  u'precisionscore': 0.5707517441852911,\n",
       "  u'recallscore': 0.5707517441852911},\n",
       " {u'acc': 0.5690104826543063,\n",
       "  u'avgprec': 0.738368769977961,\n",
       "  u'logloss': 14.886097900283808,\n",
       "  u'precisionscore': 0.5690104826543063,\n",
       "  u'recallscore': 0.5690104826543063},\n",
       " {u'acc': 0.583877477561766,\n",
       "  u'avgprec': 0.7426596510316843,\n",
       "  u'logloss': 14.372605739145103,\n",
       "  u'precisionscore': 0.583877477561766,\n",
       "  u'recallscore': 0.583877477561766},\n",
       " {u'acc': 0.588819281742613,\n",
       "  u'avgprec': 0.7510331984994645,\n",
       "  u'logloss': 14.201912662657366,\n",
       "  u'precisionscore': 0.588819281742613,\n",
       "  u'recallscore': 0.588819281742613},\n",
       " {u'acc': 0.5796997493363065,\n",
       "  u'avgprec': 0.7424736429248361,\n",
       "  u'logloss': 14.516901979572715,\n",
       "  u'precisionscore': 0.5796997493363065,\n",
       "  u'recallscore': 0.5796997493363065}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dtscmd.ModelMetrics.objects.all().values_list('Metrics',flat=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       ..., \n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "import datascience.ModelCodes.AnonymousUser as MLmod\n",
    "\n",
    "proj=dtscmd.Project.objects.get(id=7)\n",
    "traindata=dtscmd.Data.objects.get(id=59)\n",
    "X,YY,Meta=traindata.getdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y=YY.copy()\n",
    "Y[Y<5]=0\n",
    "Y[Y>=5]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       ..., \n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.71067795,  0.71067795,  0.72538924, ...,  0.39808993,\n",
       "         0.47631846,  0.51671738],\n",
       "       [ 0.19613575,  0.18811493,  0.37081135, ...,  0.38218681,\n",
       "         0.35160145,  0.36272483],\n",
       "       [ 0.75250164,  0.75250164,  0.68242133, ...,  0.42132235,\n",
       "         0.43131029,  0.43402331],\n",
       "       ..., \n",
       "       [ 0.35034309,  0.37933701,  0.36242389, ...,  0.31574803,\n",
       "         0.32472441,  0.35212598],\n",
       "       [ 0.63918621,  0.50406134,  0.36362256, ...,  0.54581288,\n",
       "         0.51055252,  0.53806791],\n",
       "       [ 0.57985668,  0.65347993,  0.66624129, ...,  0.4548038 ,\n",
       "         0.40539037,  0.3950445 ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[(Y==1).reshape(1,-1)[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X0=X[(Y==0).reshape(1,-1)[0],:]\n",
    "X1=X[(Y==1).reshape(1,-1)[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1238585, 184)\n",
      "(158724, 184)\n"
     ]
    }
   ],
   "source": [
    "print X0.shape\n",
    "print X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "j=0\n",
    "while i<1000:\n",
    "    fig,ax=plt.subplots(2,2,figsize=(15,10))\n",
    "#     XX=X0[i,:].reshape(23,8,order='F')\n",
    "#     ax[0].plot(XX[:,0:3])\n",
    "#     ax[0].plot(XX[:,3],linestyle='-.')\n",
    "#     ax[0].plot(XX[:,4:6],linestyle='--')\n",
    "    XX=X1[i,:].reshape(23,8,order='F')\n",
    "    ax[0][0].plot(XX[:,0:3])\n",
    "    ax[0][0].plot(XX[:,3],linestyle='-.')\n",
    "    ax[0][0].plot(XX[:,4:6],linestyle='--')\n",
    "\n",
    "    i=i+1\n",
    "    \n",
    "    XX=X1[i,:].reshape(23,8,order='F')\n",
    "    ax[0][1].plot(XX[:,0:3])\n",
    "    ax[0][1].plot(XX[:,3],linestyle='-.')\n",
    "    ax[0][1].plot(XX[:,4:6],linestyle='--')\n",
    "    \n",
    "    i=i+1\n",
    "    \n",
    "    XX=X1[i,:].reshape(23,8,order='F')\n",
    "    ax[1][0].plot(XX[:,0:3])\n",
    "    ax[1][0].plot(XX[:,3],linestyle='-.')\n",
    "    ax[1][0].plot(XX[:,4:6],linestyle='--')\n",
    "    \n",
    "    i=i+1\n",
    "    \n",
    "    XX=X1[i,:].reshape(23,8,order='F')\n",
    "    ax[1][1].plot(XX[:,0:3])\n",
    "    ax[1][1].plot(XX[:,3],linestyle='-.')\n",
    "    ax[1][1].plot(XX[:,4:6],linestyle='--')\n",
    "    \n",
    "    i=i+1\n",
    "    \n",
    "\n",
    "    plt.savefig('plot_'+str(j),bbox_inches='tight')\n",
    "    plt.close()\n",
    "    j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss=''\n",
    "for k in range(j):\n",
    "    ss=ss+'<img src=\"plot_'+str(k)+'.png\">'\n",
    "with open('Goodchart.html','w') as F:\n",
    "    F.write(ss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HLmean', 'SMA10', 'SMA20', 'SMA50', 'SMA100', 'SMA200', 'Volume', 'VolSMA10']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Meta['MetaX']['columns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y[Y<5]=0\n",
    "Y[Y>=5]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       ..., \n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
