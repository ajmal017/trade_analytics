{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/cbook.py:136: MatplotlibDeprecationWarning: The finance module has been deprecated in mpl 2.0 and will be removed in mpl 2.2. Please use the module mpl_finance instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n",
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1401: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter, WeekdayLocator,\\\n",
    "    DayLocator, MONDAY,date2num,num2date,AutoDateLocator\n",
    "from matplotlib.finance import quotes_historical_yahoo_ohlc, candlestick_ohlc,candlestick2_ochl,volume_overlay3\n",
    "\n",
    "from stockapp import models as stkmd\n",
    "from dataapp import models as dtamd\n",
    "from dataapp import tasks as dtatks\n",
    "from dataapp import libs as dtalibs\n",
    "from featureapp import libs as ftlibs\n",
    "from featureapp import models as ftmd\n",
    "from stockapp import tasks as stktks\n",
    "from stockapp import libs as stklibs\n",
    "import featureapp.models as ftmd\n",
    "import featureapp.tasks as fttks\n",
    "import queryapp.models as qrymd\n",
    "import queryapp.tasks as qrytks\n",
    "\n",
    "import charts.chartservers.libs as chservlibs\n",
    "import charts.libs as chlibs\n",
    "\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "\n",
    "\n",
    "import featureapp as ftapp\n",
    "import utility as uty\n",
    "from utility import models as utymd\n",
    "import itertools as itt\n",
    "import multiprocessing as mp\n",
    "from django.db import connection,connections\n",
    "from django.db import reset_queries\n",
    "import time \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import inspect\n",
    "import imp\n",
    "import datetime\n",
    "from talib.abstract import *\n",
    "import utility.models as utmd\n",
    "import stockapp.libs as stklib\n",
    "from utility import codemanager as cdmng\n",
    "from utility import maintenance as mnt\n",
    "import os \n",
    "import json\n",
    "from django.contrib.auth.models import AnonymousUser\n",
    "import threading\n",
    "\n",
    "stk=stkmd.Stockmeta.objects.get(Symbol='TSLA')\n",
    "Fromdate=pd.datetime(2008,1,1)\n",
    "Todate=pd.datetime.today()\n",
    "Trange=pd.date_range(Fromdate,Todate)\n",
    "Trange=[T.date() for T in Trange if T.weekday()<=4]\n",
    "\n",
    "import json\n",
    "# fttks.computefeatuers(stk.id,Trange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entries=[\n",
    "    {'Symbol':'TSLA','TF':pd.datetime(2012,1,1).date(),'T0':pd.datetime(2011,1,1).date()  },\n",
    "    {'Symbol':'AAPL','TF':pd.datetime(2012,1,1).date(),'T0':pd.datetime(2011,1,1).date()  }\n",
    "]\n",
    "chservlibs.request_db_charts(entries,5003)\n",
    "# img=chlibs.CurrentByFutureChart_bydb(entries[0]['T0'],entries[0]['TF'],entries[0]['Symbol'],indicatorlist=(),pricecols=(),querycols=(),featcols=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Running Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"---------------- features-------------------\"\n",
    "featurecodes=ftmd.FeatureComputeCode.objects.all()\n",
    "computecode=featurecodes[0]\n",
    "computeclass=computecode.importcomputeclass()\n",
    "CF=computeclass(stk.id,Trange)\n",
    "CF.computeall(skipdone=True)\n",
    "# CF.saveall()\n",
    "print CF.getfeaturelist()\n",
    "\n",
    "CF.df=CF.addindicators(CF.df,[\n",
    "        {'name':'SMAstd','timeperiod':20,'colname':'SMAstd20'},\n",
    "        {'name':'EMAstd','timeperiod':8,'colname':'EMAstd8'},\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "querycodes=qrymd.QueryComputeCode.objects.all()\n",
    "computecode=querycodes[0]\n",
    "computeclass=computecode.importcomputeclass()\n",
    "CQ=computeclass(stk.id,Trange)\n",
    "CQ.computeall(skipdone=True)\n",
    "CQ.saveall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CQ.OutcomeCharts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CQ.getquerylist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CQ.chartfeatures(addpricecols=(),ip=5562,\n",
    "addfeatcols=[\n",
    "    ['CCI5','CCI50'],\n",
    "    ['PastPROFIT10days','PastLOSS10days'],['FutPROFIT10days','FutLOSS10days']\n",
    "],\n",
    "addquerycols=[\n",
    "    'CCICHERRIES',\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datascience import libs as dtsclibs\n",
    "import pandas as pd\n",
    "\n",
    "@dtsclibs.register_compfunc(RequiredImports=['import pandas as pd','from dataapp import libs as dtalibs'],overwrite_if_exists=False)\n",
    "def extractdataset(data_id,Symbol):\n",
    "    \"\"\"\n",
    "    @funcName : test\n",
    "    @input x : an int\n",
    "    @output df : pd.DataFrame, some random 2 by 2 \n",
    "    @description : takes an int and then returns a dummy dataframe. This is just for testing purposes\n",
    "    @Source : \n",
    "    def extractdataset(data_id,Symbol):\n",
    "        window=60\n",
    "        window_fut=30\n",
    "        Tfs=map(lambda x: ( (x.date()-pd.Dateoffset(window)).date(),x.date(), (x.date()+pd.Dateoffset(window_fut)).date() ),\n",
    "                pd.date_range(start=pd.datetime(2010,1,1),end=pd.datetime.today(),freq='W-MON') )\n",
    "\n",
    "        N=len(Tfs)\n",
    "        dfinstants=pd.DataFrame({'T0':map(lambda x: x[0],Tfs),'TF':map(lambda x: x[1],Tfs),'Symbol':[Symbol]*N})\n",
    "        X,X_Meta=dtalibs.Getbatchdata(dfinstants)\n",
    "\n",
    "        dfinstants=pd.DataFrame({'T0':map(lambda x: x[1],Tfs),'TF':map(lambda x: x[2],Tfs),'Symbol':[Symbol]*N})\n",
    "        Y,Y_Meta=dtalibs.Getbatchdata(dfinstants)\n",
    "\n",
    "        shard=dtscmd.DataShard(Data__id=data_id)\n",
    "        shard.Info['X_Meta']=X_Meta\n",
    "        shard.Info['Y_Meta']=Y_Meta\n",
    "        shard.save()\n",
    "\n",
    "        np.savez_compressed(shard.shardpath(),X=X,Y=Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    window=60\n",
    "    window_fut=30\n",
    "    Tfs=map(lambda x: ( (x.date()-pd.Dateoffset(window)).date(),x.date(), (x.date()+pd.Dateoffset(window_fut)).date() ),\n",
    "            pd.date_range(start=pd.datetime(2010,1,1),end=pd.datetime.today(),freq='W-MON') )\n",
    "    \n",
    "    N=len(Tfs)\n",
    "    dfinstants=pd.DataFrame({'T0':map(lambda x: x[0],Tfs),'TF':map(lambda x: x[1],Tfs),'Symbol':[Symbol]*N})\n",
    "    X,X_Meta=dtalibs.Getbatchdata(dfinstants)\n",
    "    \n",
    "    dfinstants=pd.DataFrame({'T0':map(lambda x: x[1],Tfs),'TF':map(lambda x: x[2],Tfs),'Symbol':[Symbol]*N})\n",
    "    Y,Y_Meta=dtalibs.Getbatchdata(dfinstants)\n",
    "    \n",
    "    shard=dtscmd.DataShard(Data__id=data_id)\n",
    "    shard.Info['X_Meta']=X_Meta\n",
    "    shard.Info['Y_Meta']=Y_Meta\n",
    "    shard.save()\n",
    "    \n",
    "    np.savez_compressed(shard.shardpath(),X=X,Y=Y)\n",
    "\n",
    "extractdataset.id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating initial Stock price dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "projid,dataid=dtsclibs.register_dataset(project_Name=\"PredictReturn\",project_Info={'description': \"Data taken on every Monday. 360 days back and 60 days forward\"},\n",
    "                                        Datatype='RawProcessed',GroupName=\"AllStocks\",tag=\"1\",\n",
    "                                        data_format='npz',Modeltype='Regression',\n",
    "                                        TransformedFromDataId=None,TransFuncId=None, use_project_ifexists=True)\n",
    "\n",
    "dtsctks.CreateStockData_2(360,60,dataid,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print dtscmd.Data.objects.filter(Project__id=7,GroupName=\"AllStocks\",tag=\"1\",Datatype='RawProcessed',\n",
    "                           Dataformat='npz',Modeltype='Regression')\n",
    "data = dtscmd.Data.objects.get(Project__id=7,GroupName=\"AllStocks\",tag=\"1\",Datatype='RawProcessed',\n",
    "                           Dataformat='npz',Modeltype='Regression')\n",
    "\n",
    "data.datapath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtscmd.DataShard.objects.filter(Data=data).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtscmd.DataShard.objects.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions on shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over writing previous function\n",
      "saving function :  CleanData\n",
      "function id =  20\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "@dtsclibs.register_compfunc(Group='Cleanup',overwrite_if_exists=True)\n",
    "def CleanData(shardId):\n",
    "    \"\"\"\n",
    "    @Description: remove shards with many Nan samples. Remove samples with many Nans\n",
    "    @Source\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import datascience.models as dtscmd\n",
    "    \n",
    "    \n",
    "    \n",
    "    shard=dtscmd.DataShard.objects.get(id=shardId)\n",
    "    X,Y,Meta=shard.getdata()\n",
    "    \n",
    "    # remove samples with lot of nans\n",
    "    Nsamples=X.shape[0]\n",
    "    Tsteps=X.shape[1]\n",
    "    Nfeat=X.shape[2]\n",
    "    volumecols=[4,11,12]\n",
    "    colsX=list( Meta['MetaX']['columns'] )\n",
    "    colsY=list( Meta['MetaY']['columns'] )\n",
    "    delsample=[]\n",
    "    for i in range(Nsamples):\n",
    "        dfX=pd.DataFrame(X[i,:,:],columns=colsX)\n",
    "        dfY=pd.DataFrame(X[i,:,:],columns=colsY)\n",
    "        if np.sum(pd.isnull(dfX['Close'].values).astype(float))/len(dfX)>=0.5:\n",
    "            delsample.append(i)\n",
    "    \n",
    "    X=np.delete(X,delsample,axis=0)\n",
    "    Y=np.delete(Y,delsample,axis=0)\n",
    "    \n",
    "    # if not more samples then delete the shard\n",
    "    if X.shape[0]/(Nsamples*1.0) <0.4:\n",
    "        print \"----------------------\"\n",
    "        print str(shardId)+\"   \"+ str(X.shape[0]/(Nsamples*1.0))\n",
    "        print \"----------------------\"\n",
    "        print \"delete shaord id = \"+str(shard.id)\n",
    "        shard.delete()\n",
    "    elif X.shape[0]!=Nsamples:\n",
    "        shard.savedata(X=X,Y=Y,Meta=Meta)\n",
    "    else:\n",
    "        pass\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import datascience.models as dtscmd\n",
    "    \n",
    "    \n",
    "    \n",
    "    shard=dtscmd.DataShard.objects.get(id=shardId)\n",
    "    X,Y,Meta=shard.getdata()\n",
    "    \n",
    "    # remove samples with lot of nans\n",
    "    Nsamples=X.shape[0]\n",
    "    Tsteps=X.shape[1]\n",
    "    Nfeat=X.shape[2]\n",
    "    volumecols=[4,11,12]\n",
    "    colsX=list( Meta['MetaX']['columns'] )\n",
    "    colsY=list( Meta['MetaY']['columns'] )\n",
    "    delsample=[]\n",
    "    for i in range(Nsamples):\n",
    "        dfX=pd.DataFrame(X[i,:,:],columns=colsX)\n",
    "        dfY=pd.DataFrame(X[i,:,:],columns=colsY)\n",
    "        if np.sum(pd.isnull(dfX['Close'].values).astype(float))/len(dfX)>=0.5:\n",
    "            delsample.append(i)\n",
    "    \n",
    "    X=np.delete(X,delsample,axis=0)\n",
    "    Y=np.delete(Y,delsample,axis=0)\n",
    "    \n",
    "    # if not more samples then delete the shard\n",
    "    if X.shape[0]/(Nsamples*1.0) <0.4:\n",
    "        print \"----------------------\"\n",
    "        print str(shardId)+\"   \"+ str(X.shape[0]/(Nsamples*1.0))\n",
    "        print \"----------------------\"\n",
    "        print \"delete shaord id = \"+str(shard.id)\n",
    "        shard.delete()\n",
    "    elif X.shape[0]!=Nsamples:\n",
    "        shard.savedata(X=X,Y=Y,Meta=Meta)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "# dtsctks.applyfunc2data(CleanData.id,6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6548"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtscmd.DataShard.objects.filter(Data__id=6).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Creating derived DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over writing previous function\n",
      "saving function :  StandardizeData_1\n",
      "function id =  21\n",
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "saving transfoermer function to this dataset\n",
      "('project id', 'data id')  :  (7, 9)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "\n",
    "@dtsclibs.register_compfunc(Group='Transformer',overwrite_if_exists=True)\n",
    "def StandardizeData_1(X,Y,Meta):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    \"\"\"\n",
    "    A transformer function has to take X,Y,Meta and return another modified X,Y,Meta\n",
    "    1. Normalize all data as \n",
    "        1. Volume to 0-1\n",
    "        2. Prices --> also 0-1\n",
    "    2. For output Y data:\n",
    "        1. Take only Close\n",
    "        2. Pull out best returns in first 5 days, 10 days, 30 days, 60 days, 90 days\n",
    "        3. Pull out worst loss in first 5 days, 10 days, 30 days, 60 days, 90 days\n",
    "    \n",
    "    @Source:\n",
    "    # next normalize the volume to 0-1\n",
    "    Nsamples=X.shape[0]\n",
    "    Tsteps=X.shape[1]\n",
    "    Nfeat=X.shape[2]\n",
    "    volumecols=['Volume','VolSMA10','VolSMA20']\n",
    "    pricecols=['Close','Open','High','Low','SMA10','SMA20','SMA50','SMA100','SMA200','EMA8','EMA20']\n",
    "    colsX=list( Meta['MetaX']['columns'] )\n",
    "    colsY=list( Meta['MetaY']['columns'] )\n",
    "    FinalXcols=pricecols+volumecols\n",
    "    FinalYcols=['FutProfit5days','FutProfit10days','FutProfit30days','FutProfit60days','FutProfit90days']+['FutLoss5days',\n",
    "                                    'FutLoss10days','FutLoss30days','FutLoss60days','FutLoss90days']\n",
    "    \n",
    "    Xn=None\n",
    "    Yn=None\n",
    "    Metan=None\n",
    "    for i in range(Nsamples):\n",
    "        dfX=pd.DataFrame(X[i,:,:],columns=colsX)\n",
    "        dfY=pd.DataFrame(Y[i,:,:],columns=colsY)\n",
    "        \n",
    "        \n",
    "        # clean up Y\n",
    "        dfY.drop('Symbol',axis=1,inplace=True)\n",
    "        \n",
    "        dfY.drop(volumecols,axis=1,inplace=True)\n",
    "        dfY.drop([cc for cc in pricecols if cc!='Close'],axis=1,inplace=True)\n",
    "        \n",
    "        Ydict={}\n",
    "        dfY['ZeroPerf']=0\n",
    "    \n",
    "#         dfY['FutProfit5days']=-100*self.df['Close'].diff(periods=-5)/self.df['Close']\n",
    "        dfY['Returns']=100*(dfY['Close']-dfY['Close'].iloc[0])/dfY['Close'].iloc[0]\n",
    "        \n",
    "        Ydict['FutProfit5days']=dfY[['Returns','ZeroPerf']].iloc[0:5].max(axis=1).round().max()\n",
    "        Ydict['FutProfit10days']=dfY[['Returns','ZeroPerf']].iloc[0:10].max(axis=1).round().max()\n",
    "        Ydict['FutProfit30days']=dfY[['Returns','ZeroPerf']].iloc[0:30].max(axis=1).round().max()\n",
    "        Ydict['FutProfit60days']=dfY[['Returns','ZeroPerf']].iloc[0:60].max(axis=1).round().max()\n",
    "        Ydict['FutProfit90days']=dfY[['Returns','ZeroPerf']].iloc[0:90].max(axis=1).round().max()\n",
    "        \n",
    "        Ydict['FutLoss5days']=dfY[['Returns','ZeroPerf']].iloc[0:5].min(axis=1).round().min()\n",
    "        Ydict['FutLoss10days']=dfY[['Returns','ZeroPerf']].iloc[0:10].min(axis=1).round().min()\n",
    "        Ydict['FutLoss30days']=dfY[['Returns','ZeroPerf']].iloc[0:30].min(axis=1).round().min()\n",
    "        Ydict['FutLoss60days']=dfY[['Returns','ZeroPerf']].iloc[0:60].min(axis=1).round().min()\n",
    "        Ydict['FutLoss90days']=dfY[['Returns','ZeroPerf']].iloc[0:90].min(axis=1).round().min()\n",
    "        \n",
    "        # clean up X\n",
    "        dfX.drop('Symbol',axis=1,inplace=True)\n",
    "        \n",
    "        mxvol=dfX['Volume'].max()\n",
    "        dfX['Volume']=dfX['Volume']/mxvol\n",
    "        dfX['VolSMA10']=dfX['VolSMA10']/mxvol\n",
    "        dfX['VolSMA20']=dfX['VolSMA20']/mxvol\n",
    "        \n",
    "        mxHigh=dfX['High'].max()\n",
    "        mnLow=dfX['Low'].min()\n",
    "        for cc in pricecols:\n",
    "            dfX[cc]=(dfX[cc]-mnLow)/mxHigh\n",
    "        \n",
    "        XX=np.expand_dims( dfX[FinalXcols].astype(float).values   ,axis=0     )\n",
    "        YY=np.expand_dims( np.array([int(Ydict[key]) for key in FinalYcols]),axis=0 )\n",
    "        if Xn is None:\n",
    "            Xn=XX\n",
    "            Yn=YY\n",
    "        else:\n",
    "            Xn=np.vstack((Xn,XX))\n",
    "            Yn=np.vstack((Yn,YY))\n",
    "    \n",
    "    Metan=Meta\n",
    "    Metan['MetaX']['columns']=FinalXcols\n",
    "    Metan['MetaY']['columns']=FinalYcols\n",
    "    \n",
    "    return Xn,Yn,Metan\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # next normalize the volume to 0-1\n",
    "    Nsamples=X.shape[0]\n",
    "    Tsteps=X.shape[1]\n",
    "    Nfeat=X.shape[2]\n",
    "    volumecols=['Volume','VolSMA10','VolSMA20']\n",
    "    pricecols=['Close','Open','High','Low','SMA10','SMA20','SMA50','SMA100','SMA200','EMA8','EMA20']\n",
    "    colsX=list( Meta['MetaX']['columns'] )\n",
    "    colsY=list( Meta['MetaY']['columns'] )\n",
    "    FinalXcols=pricecols+volumecols\n",
    "    FinalYcols=['FutProfit5days','FutProfit10days','FutProfit30days','FutProfit60days','FutProfit90days']+['FutLoss5days',\n",
    "                                    'FutLoss10days','FutLoss30days','FutLoss60days','FutLoss90days']\n",
    "    \n",
    "    Xn=None\n",
    "    Yn=None\n",
    "    Metan=None\n",
    "    for i in range(Nsamples):\n",
    "        dfX=pd.DataFrame(X[i,:,:],columns=colsX)\n",
    "        dfY=pd.DataFrame(Y[i,:,:],columns=colsY)\n",
    "        \n",
    "        \n",
    "        # clean up Y\n",
    "        dfY.drop('Symbol',axis=1,inplace=True)\n",
    "        \n",
    "        dfY.drop(volumecols,axis=1,inplace=True)\n",
    "        dfY.drop([cc for cc in pricecols if cc!='Close'],axis=1,inplace=True)\n",
    "        \n",
    "        Ydict={}\n",
    "        dfY['ZeroPerf']=0\n",
    "    \n",
    "#         dfY['FutProfit5days']=-100*self.df['Close'].diff(periods=-5)/self.df['Close']\n",
    "        dfY['Returns']=100*(dfY['Close']-dfY['Close'].iloc[0])/dfY['Close'].iloc[0]\n",
    "        \n",
    "        Ydict['FutProfit5days']=dfY[['Returns','ZeroPerf']].iloc[0:5].max(axis=1).round().max()\n",
    "        Ydict['FutProfit10days']=dfY[['Returns','ZeroPerf']].iloc[0:10].max(axis=1).round().max()\n",
    "        Ydict['FutProfit30days']=dfY[['Returns','ZeroPerf']].iloc[0:30].max(axis=1).round().max()\n",
    "        Ydict['FutProfit60days']=dfY[['Returns','ZeroPerf']].iloc[0:60].max(axis=1).round().max()\n",
    "        Ydict['FutProfit90days']=dfY[['Returns','ZeroPerf']].iloc[0:90].max(axis=1).round().max()\n",
    "        \n",
    "        Ydict['FutLoss5days']=dfY[['Returns','ZeroPerf']].iloc[0:5].min(axis=1).round().min()\n",
    "        Ydict['FutLoss10days']=dfY[['Returns','ZeroPerf']].iloc[0:10].min(axis=1).round().min()\n",
    "        Ydict['FutLoss30days']=dfY[['Returns','ZeroPerf']].iloc[0:30].min(axis=1).round().min()\n",
    "        Ydict['FutLoss60days']=dfY[['Returns','ZeroPerf']].iloc[0:60].min(axis=1).round().min()\n",
    "        Ydict['FutLoss90days']=dfY[['Returns','ZeroPerf']].iloc[0:90].min(axis=1).round().min()\n",
    "        \n",
    "        # clean up X\n",
    "        dfX.drop('Symbol',axis=1,inplace=True)\n",
    "        \n",
    "        mxvol=dfX['Volume'].max()\n",
    "        dfX['Volume']=dfX['Volume']/mxvol\n",
    "        dfX['VolSMA10']=dfX['VolSMA10']/mxvol\n",
    "        dfX['VolSMA20']=dfX['VolSMA20']/mxvol\n",
    "        \n",
    "        mxHigh=dfX['High'].max()\n",
    "        mnLow=dfX['Low'].min()\n",
    "        for cc in pricecols:\n",
    "            dfX[cc]=(dfX[cc]-mnLow)/mxHigh\n",
    "        \n",
    "        XX=np.expand_dims( dfX[FinalXcols].astype(float).values   ,axis=0     )\n",
    "        YY=np.expand_dims( np.array([int(Ydict[key]) for key in FinalYcols]),axis=0 )\n",
    "        if Xn is None:\n",
    "            Xn=XX\n",
    "            Yn=YY\n",
    "        else:\n",
    "            Xn=np.vstack((Xn,XX))\n",
    "            Yn=np.vstack((Yn,YY))\n",
    "    \n",
    "    Metan=Meta\n",
    "    Metan['MetaX']['columns']=FinalXcols\n",
    "    Metan['MetaY']['columns']=FinalYcols\n",
    "    \n",
    "    return Xn,Yn,Metan\n",
    "\n",
    "\n",
    "projectid,dataid=dtsclibs.register_dataset(tag='NormalizedDerivedFromId6',TransformedFromDataId=6,TransFuncId=StandardizeData_1.id )\n",
    "\n",
    "dtsctks.Perform_TransformData(dataid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over writing previous function\n",
      "saving function :  StandardizeData_1\n",
      "function id =  21\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_CloseReturnVolume01\n",
      "function id =  22\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_Close01Volume01\n",
      "function id =  23\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_CloseSMAVolSMA10\n",
      "function id =  24\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_Close01Volume01_X30_Y5\n",
      "function id =  25\n",
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "updating data info\n",
      "saving transfoermer function to this dataset\n",
      "('project id', 'data id')  :  (7, 10)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "import dataapp.stockdata_transformers as stktrfrmr\n",
    "\n",
    "funcid=stktrfrmr.StandardizeData_Close01Volume01_X30_Y5.id\n",
    "DataInfo={'description':'All stocks data, close price is made into 0-1, Volume is made into 0-1, Y is profit/(profit+loss) for next 5 days'}\n",
    "projectid,dataid=dtsclibs.register_dataset(GroupName='AllStocksCloseVol',DataInfo=DataInfo,tag='NormalizedDerived30days_5daysFromId6',TransformedFromDataId=6,TransFuncId=funcid )\n",
    "\n",
    "dtsctks.Perform_TransformData(dataid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over writing previous function\n",
      "saving function :  StandardizeData_1\n",
      "function id =  21\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_CloseReturnVolume01\n",
      "function id =  22\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_Close01Volume01\n",
      "function id =  23\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_CloseSMAVolSMA10\n",
      "function id =  24\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_Close01Volume01_X30_Y5\n",
      "function id =  25\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_Close01Volume01_X30_Y5_flatout\n",
      "function id =  26\n",
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "updating data info\n",
      "saving transfoermer function to this dataset\n",
      "('project id', 'data id')  :  (7, 11)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "import dataapp.stockdata_transformers as stktrfrmr\n",
    "\n",
    "funcid=stktrfrmr.StandardizeData_Close01Volume01_X30_Y5_flatout.id\n",
    "DataInfo={'description':'FLATTENED!!!  All stocks data, close price is made into 0-1, Volume is made into 0-1, Y is profit/(profit+loss) for next 5 days'}\n",
    "projectid,dataid=dtsclibs.register_dataset(GroupName='AllStocksCloseVol_FLATTENED',DataInfo=DataInfo,tag='NormalizedDerived30days_5daysFromId6',TransformedFromDataId=6,TransFuncId=funcid )\n",
    "\n",
    "dtsctks.Perform_TransformData(dataid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over writing previous function\n",
      "saving function :  StandardizeData_1\n",
      "function id =  21\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_CloseReturnVolume01\n",
      "function id =  22\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_Close01Volume01\n",
      "function id =  23\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_CloseSMAVolSMA10\n",
      "function id =  24\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_Close01Volume01_X30_Y5\n",
      "function id =  25\n",
      "over writing previous function\n",
      "saving function :  StandardizeData_Close01Volume01_X30_Y5_flatout\n",
      "function id =  26\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from datascience import models as dtscmd\n",
    "from datascience import tasks as dtsctks\n",
    "from datascience import libs as dtsclibs\n",
    "from celery import group\n",
    "import time\n",
    "import pickle as pkl\n",
    "import dataapp.stockdata_transformers as stktrfrmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "dataId1=7\n",
    "data1=dtscmd.Data.objects.get(id=dataId1)\n",
    "data0=dtscmd.Data.objects.get(id=data1.ParentData.id)\n",
    "print data0.id\n",
    "# shard0Ids=data0.objects.all().values_list('id',flat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6432"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtscmd.DataShard.objects.filter(Data__id=9).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6548"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtscmd.DataShard.objects.filter(Data__id=10).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6548"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtscmd.DataShard.objects.filter(Data__id=11).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112179"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtscmd.DataShard.objects.filter(Data__id__gte=12).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shard=dtscmd.DataShard.objects.filter(id=308321).last()\n",
    "X,Y,Meta=shard.getdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6a599c6e10>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6a5f27ffd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X[2,0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shards=dtscmd.DataShard.objects.filter(Data__id=27)\n",
    "X=None\n",
    "for shard in shards:\n",
    "    if X is None:\n",
    "        X,Y,_=shard.getdata()\n",
    "    else:\n",
    "        XX,YY,_=shard.getdata()\n",
    "        X=np.vstack((X,XX))\n",
    "        Y=np.vstack((Y,YY))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       ..., \n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from sklearn import linear_model,decomposition\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score,r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "import scipy\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression,SGDClassifier\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f99cc6b93655>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mclf_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \"\"\"\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=25)\n",
    "clf.fit(X, Y)\n",
    "clf_probs = clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create Train and Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "('project id', 'data id')  :  (7, 27)\n",
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "('project id', 'data id')  :  (7, 28)\n",
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "('project id', 'data id')  :  (7, 29)\n",
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "('project id', 'data id')  :  (7, 30)\n",
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "('project id', 'data id')  :  (7, 31)\n",
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "('project id', 'data id')  :  (7, 32)\n",
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "('project id', 'data id')  :  (7, 33)\n",
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "('project id', 'data id')  :  (7, 34)\n",
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "('project id', 'data id')  :  (7, 35)\n",
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "('project id', 'data id')  :  (7, 36)\n",
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "Deleting the existing shards for this data\n",
      "('project id', 'data id')  :  (7, 37)\n",
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "('project id', 'data id')  :  (7, 38)\n",
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "('project id', 'data id')  :  (7, 39)\n",
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "('project id', 'data id')  :  (7, 40)\n",
      "Project  PredictReturn  already exists\n",
      "The dataset already exists\n",
      "('project id', 'data id')  :  (7, 41)\n"
     ]
    }
   ],
   "source": [
    "import datascience.ML.MLlibs as MLlibs\n",
    "MLlibs.get_train_test_from_RawProcessed(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
